---
title: "Microsoft as a Factor?"
author: "Frank"
date: "June 16, 2015"
output: html_document
---

## Preliminaries
First I'll load my (in-progress) package for Bayesian analysis of factor asset pricing models:
```{r}
library(zoofactr)
```
If you haven't already installed ``zoofactr``, you can do so using Hadley Wickham's ``devtools`` package as follows: ``devtools::install_github("fditraglia/zoofactr")``.)

The Fama-French five factors are available in ``zoofactr``
```{r}
head(ff5)
```
as are the ten industry portfolios, both value-weighted:
```{r}
head(industry10$value)
```
and equal-weighted:
```{r}
head(industry10$equal)
```

## Constructing the "Microsoft Factor"

We'll pull in Microsoft returns form Yahoo finance using the ``quandl`` package
```{r,cache=TRUE}
msft <- Quandl::Quandl("YAHOO/MSFT", type = "raw", transformation = "rdiff",
                       collapse = "monthly", sort = "asc")
head(msft)
```
and then create a ``month`` column to match our other dataframes:
```{r}
msft$month <- substr(msft$Date, 1, 7)
msft$month <- gsub("-", "", msft$month)
```
before removing the columns we don't need:
```{r}
msft <- data.frame(month = msft$month, MSFT = 100 * msft$`Adjusted Close`)
head(msft)
```
Notice that we multiply Microsoft returns by 100 so they will be in the same units as the columns of ``ff5`` which are in *percentage points* rather than decimal percents.

Now we can merge everything that we'll use to run the SUR regressions:
```{r}
SURdata <- merge(ff5, msft, by = "month")
SURdata <- merge(SURdata, industry10$value, by = "month")
head(SURdata)
```
and then create the "Microsoft Factor"
```{r}
SURdata$MSFT.Rf <- with(SURdata, MSFT - RF)
rm(msft)
```

## Training Sample Prior
Now we'll use the first 100 rows of ``SURdata`` as a "training sample" to form a data-based prior for marginal likelihood comparisons in the subsequent sample:
```{r}
train <- SURdata[1:100,]
fit <- SURdata[-c(1:100),] 
rm(SURdata)
```
Now we construct the "Y"-variables for the regressions by converting the industry portfolio returns to excess returns
```{r}
industry_names <- setdiff(names(industry10$value), "month")
Y_train <- as.matrix(train[,industry_names])
Y_fit <- as.matrix(fit[,industry_names])
rm(industry_names)
Y_train <- Y_train - train$RF
Y_fit <- Y_fit - fit$RF
```
To start we'll consider the following models:
```{r}
models <- list(formula("~ Mkt.RF - 1"),
               formula("~ Mkt.RF"),
               formula("~ Mkt.RF + SMB + HML - 1"),
               formula("~ Mkt.RF + SMB + HML"),
               formula("~ Mkt.RF + MSFT.Rf - 1"),
               formula("~ Mkt.RF + MSFT.Rf"),
               formula("~ Mkt.RF + SMB + HML + MSFT.Rf - 1"),
               formula("~ Mkt.RF + SMB + HML + MSFT.Rf"))
```
For the training estimation I use vague but proper priors: independent $N(0,100)$ for the regression coefficients and $W(12, I_{10}/100)$ for the precision matrix which corresponds to prior expectation of $100 \times I_{10}$ for the associated *covariance matrix*.
Setting the degrees of freedom equal to 12 gives the most diffuse prior possible subject to the constraint that the expected value of the covariance matrix is finite.
Results are based on 5000 simulated posterior draws after a burn-in of 1000 draws.

Let's take a look at the results of fitting the training sample for the CAPM without an intercept:
```{r}
train_results <- defaultSUR(model.matrix(models[[1]], train), Y_train)
```
The posterior means for the industry portfolio betas look reasonable:
```{r}
rowMeans(train_results$g_draws)
```
But the standard deviations are probably too small to use as a prior:
```{r}
sqrt(diag(var(t(train_results$g_draws))))
```
The posterior mean for the precision matrix is as follows:
```{r}
Omega_inv_mean <- devech(rowMeans(train_results$Omega_inv_draws), ncol(Y_train))
Omega_inv_mean
```
which corresponds to the following covariance matrix:
```{r}
solve(Omega_inv_mean)
```
with diagonal elements:
```{r}
diag(solve(Omega_inv_mean))
rm(g_var, Omega_inv_mean)
```
It's difficult to interpret the off-diagonal elements here but the magnitudes of the diagonal elements seem reasonable given that the portfolio excess returns are expressed in percentage points, i.e. 3.4 means a 3.4% return.

## Marginal Likelihood Comparisons
For the marginal likelihood comparisons I'll center the priors around the results from the training samples.
Based on the results for the CAPM in the preceding section, however, if seems a bit too dogmatic to take the 100 observations literally as a dummy observation prior so I'll make things a bit more diffuse.
This is a bit arbitrary, but for the moment I'll give the Wishart prior 35 degrees of freedom, equivalent to augmenting the prior for the training sample with 20 dummy observations rather than the 100 we actually used.
I'll likewise scale up the posterior standard deviations from the training sample by a factor of five. For simplicity I'll use independent priors for the regression coefficients.

```{r}
get_logML <- function(model_formula){
  train_results <- defaultSUR(model.matrix(model_formula, train), Y_train)
  g0 <- rowMeans(train_results$g_draws)
  g_sd <- sqrt(diag(var(t(train_results$g_draws))))
  G0 <- diag((g_sd * 5)^2)
  d <- ncol(Y_train)
  r0 <- d + 2 + 20
  Omega_inv_mean <- devech(rowMeans(train_results$Omega_inv_draws), d)
  R0 <- Omega_inv_mean / r0
  logML_SUR(model.matrix(model_formula, fit), Y_fit, G0, g0, R0, r0)
}
```
Here are the results:
```{r}
set.seed(1234)
results <- lapply(seq_along(models), function(i) get_logML(models[[i]]))
results <- data.frame(model = as.character(models), logML = unlist(results))
results[order(results$logML, decreasing = TRUE),]
```












