%!TEX root = main.tex
\section{The Model}
Consider a linear $K$-factor model for $D$ assets of the form
\begin{equation*}
y_{dt}=\alpha _{d} + \mathbf{f}_{t}' \boldsymbol{\beta}_d + \varepsilon_{it}
\end{equation*}
where $y_{dt}$ denotes the excess return for test portfolio $d$ in period $t$ for $d = 1, \hdots, D$ and $t = 1, \hdots, T$ and $\mathbf{f}_{t}'=\left(f_{t1}, \hdots,f_{tK}\right)$ vector of factor returns.
This is a special case of the seemingly unrelated regression (SUR) model in which the regressors are \emph{identical} across equations.
Note that the factors included into the regression $\mathbf{f}$, the number of factors $K$ as well as the presence of the intercept depend on the model under consideration.  
In order to simplify the notation, we do not introduce a model index, however, it is important to keep in mind that the techniques discussed below are applied to each possible combination of factors.

Stacking observations for a given time period across assets, define $\mathbf{y}_t' = (y_{1t}, \hdots, y_{Dt})$ and analogously $\boldsymbol{\varepsilon}_t' = \left( \varepsilon_{t1}, \hdots \varepsilon_{tD} \right)$. 
Both $\mathbf{y}_t$ and $\boldsymbol{\varepsilon}_t$ are $D \times 1$ vectors.
Now let $\mathbf{x}_t' = (1, \mathbf{f}_t')$ be a $1 \times (K+1)$ vector and $\boldsymbol{\gamma}_d' = (\alpha_d, \boldsymbol{\beta}_d')$ to be a $1 \times (K+1)$ so we have
\begin{equation*}
\mathbf{y}_t = X_t \boldsymbol{\gamma} + \boldsymbol{\varepsilon}_t
\end{equation*}
where $X_t = I_D \otimes \mathbf{x}_t'$, a $D \times D(K+1)$ matrix, and $\boldsymbol{\gamma}'= \left( \boldsymbol{\gamma}_1', \hdots, \boldsymbol{\gamma}_D' \right)$, a $1 \times (K+1)D$ vector. 
If the model does not include a constant we simply remove the column of ones from the definition of $\mathbf{x}_t$.
Let $Y_T$ denote the full data sample, i.e.\ $\left\{ \mathbf{y}_t, \mathbf{x}_t \right\}_{t=1}^T$.

In order to apply Bayesian inference, we need to specify the distribution of the errors. 
Because asset returns are typically thought to be heavy-tailed our models will be based on a multivariate Student-t distribution in which we will allow the data to choose the degrees of freedom.\footnote{Our model selection exercise below will also consider a Gaussian error distribution although this specification is soundly rejected by the data.}
We have:
\begin{equation*}
\boldsymbol{\varepsilon}_{t}\sim t_{D,\nu }\left( 0,\Omega \right)
\end{equation*}
where $\nu$ denotes the degrees of freedom of the distribution, the location parameter is zero and the scale matrix is $\Omega$.
If $\nu >1$ then $E(\boldsymbol{\varepsilon}) = 0$. 
If $\nu>2$ then $Var\left( \boldsymbol{\varepsilon} \right) = \nu \Omega/(\nu-2)$. 
The SUR model lets us explicitly explore the cross-sectional dependence of errors by jointly fitting the model for all assets. 
The possible variance linkages between different assets are captured by the scale matrix $\Omega$. 
While acknowledging the importance of these linkages and the possible influence they may have on the model selection, we avoid placing \emph{a priori} restrictions on this matrix, such as assuming that it is diagonal, and instead infer the covariance structure of errors from the data.

To permit use of Gibbs sampler draws, we use representation of the Student-t distribution as a scale mixture of normal distributions, as in \cite{chib1995hierarchical}.
In particular,
\begin{eqnarray*}
\boldsymbol{\varepsilon}_{t}|\lambda _{t} &\sim N\left( 0,\lambda _{t}^{-1}\Omega \right)
\\
\lambda _{t} &\sim G\left( \frac{\nu }{2},\frac{\nu }{2}\right)
\end{eqnarray*}
where $G(\alpha,\beta)$ denotes the Gamma distribution with shape parameter $\alpha$ and rate parameter $\beta$. 
We parameterize this problem in terms of the $D\times D$ \emph{precision} matrix $\Omega^{-1}$ and the $p\times1$ vector of regression coefficients $\boldsymbol{\gamma}$, where $p = D(K+1)$. 
In any inference that we carry out, as well as in the calculation of the marginal likelihood, we will marginalize over $\boldsymbol{\lambda}$ by simply ignoring these draws.
We will place a normal prior on $\boldsymbol{\gamma}$ and a Wishart prior on $\Omega^{-1}$. 

\subsection{The Gibbs Sampler}
In order to approximate the posterior distribution we implement Gibbs sample following \cite{chib1996markov} and \cite{carlin1991inference}. The sampler proceeds by fixing the degrees of freedom parameter $\nu$.
If $\nu$ is to be chosen from the data, this can be accomplished using the marginal likelihood, as described below.
Holding $\nu$ fixed, the full set of conditional posteriors is as follows:

\paragraph{Regression Coefficients:} 
$\boldsymbol{\gamma}|\Omega^{-1},Y_T \sim \mathcal{N}_p\left( \bar{\boldsymbol{\gamma}}_{\lambda},G_{T,\lambda} \right)$
\begin{eqnarray*}
	G_{T,\lambda} &=& \left[ G_0^{-1} + \sum_{t=1}^T \lambda_t X_t' \Omega^{-1} X_t \right]^{-1}\\
	\bar{\boldsymbol{\gamma}}_{\lambda} &=& G_{T,\lambda} \left[ G_0^{-1}\boldsymbol{\gamma}_0 + \sum_{t=1}^{T}\lambda_t X_t'\Omega^{-1}\mathbf{y}_t \right]
\end{eqnarray*}
\paragraph{Inverse Scale Matrix:}
$\Omega^{-1}|Y_T \sim \mathcal{W}_D\left(\rho_0 + T, R_{T,\lambda}\right)$
\begin{equation*}
R_{T,\lambda} = \left[ R_0^{-1} + \sum_{t=1}^{T} \lambda_t\left( \mathbf{y}_t - X_t \boldsymbol{\gamma} \right)\left( \mathbf{y}_t - X_t \boldsymbol{\gamma} \right)' \right]^{-1}
\end{equation*}

\paragraph{Auxiliary Parameter:} $\lambda_t|\boldsymbol{\gamma}, \nu, Y_T \sim G\left(\displaystyle \frac{\nu + D}{2}, \frac{\nu + \boldsymbol{\varepsilon}_t' \boldsymbol{\varepsilon}_t}{2} \right)$, $\boldsymbol{\varepsilon}_t = \mathbf{y}_t - X_t \boldsymbol{\gamma}$

To implement the Gibbs sampler, we simply need to draw sequentially from these distributions, in the order given above. 
We will require, however, starting values for both $\Omega^{-1}$ and each of the $\lambda_t$ parameters. 
A reasonable starting value for $\lambda_t$ is one.

\subsection{Marginal Likelihood}
We calculate the marginal likelihood using the method of \cite{chib1995marginal}.
Consider the re-arranged Bayes rule:
\begin{equation*}
f(y) = \frac{f(y|\boldsymbol{\gamma},\Omega^{-1})\pi\left(\boldsymbol{\gamma}, \Omega^{-1}\right)}{\pi\left(\boldsymbol{\gamma}, \Omega^{-1}|y \right)}
\end{equation*}
which holds for any specified values $(\boldsymbol{\gamma},\Omega^{-1})$ of the parameters.
In particular this holds at the \emph{posterior mean} $(\boldsymbol{\gamma}^*,\Omega^{-1*})$ which is where we will evaluate the expression.
Hence, the \emph{log} marginal likelihood is given by
\begin{equation*}
\log{f(y)} =  \log \left(\boldsymbol{\gamma}^*, \Omega^{-1*}\right) + \log{f(y|\boldsymbol{\gamma}^*,\Omega^{-1*})} - \pi\left(\boldsymbol{\gamma}^*, \Omega^{-1*}|y \right)
\end{equation*}
Since our priors over $\boldsymbol{\gamma}$ and $\Omega^{-1}$ are independent, we can re-write the equation above as follows:
\begin{equation*}
\log f(Y_T) = \log \pi(\boldsymbol{\gamma}^*) + \log \pi\left( \Omega^{-1*} \right) + \log f\left( Y_T|\boldsymbol{\gamma}^*, \Omega^{-1*} \right) - \log \pi\left( \boldsymbol{\gamma}^*, \Omega^{-1*}|Y_T \right)
\end{equation*}
We can evaluate the marginal likelihood using the \cite{chib1995marginal} method that approximates $\log f(Y_T)$ by evaluating each of the terms on the right-hand-side of the preceding expression using the output of the Gibbs sampler.

\subsection{Priors}
In order for the marginal likelihood comparison to be valid, proper priors should be constructed for each candidate model. 
In particular, they should differ depending on the set of included factors.
For example, consider the market beta for non-durable consumption goods. 
We cannot set the same prior for all models that contain the market factor: the priors should be adjusted depending on which other factors are included. 
In order to solve this problem we employ a small training sample and use the posterior means to construct  model-specific prior.

For each model we use normal prior for the coefficient vector $\boldsymbol{\gamma}$ which is centered around the posterior mean obtained from the training sample.
In order to reflect the uncertainty, we inflate the covariance matrix of training sample draws by a factor of 4.

We place a Wishart prior with 25 degrees of freedom on the precision matrix $\Omega^{-1}$.
The prior mean is equal to the mean based on the training sample. 

This trick allows us to automatically set an individual prior for each model in an objective way. 
For example, for a simple CAPM with a single market factor  the prior for market beta of non-durable consumption goods is centered around 0.97 with the variance of 0.011.
However, when estimating Fama-French 3 factor model the prior is adjusted to reflect the presence of two other factors with the prior mean for the market beta being shifted to 1.00 with the variance of 0.015.
