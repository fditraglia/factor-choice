%!TEX root = main.tex
\section{The Model}
Consider a linear $K$-factor model for $D$ assets of the form
\begin{equation*}
y_{dt}=\alpha _{d} + \mathbf{f}_{t}' \boldsymbol{\beta}_d + \varepsilon_{it}
\end{equation*}
where $y_{dt}$ denotes the excess return for test portfolio $d$ in period $t$ for $d = 1, \hdots, D$ and $t = 1, \hdots, T$ and $\mathbf{f}_{t}'=\left(f_{t1}, \hdots,f_{tK}\right)$ vector of factor returns.
This is a special case of the seemingly unrelated regression (SUR) model in which the regressors are \emph{identical} across equations.
Note that the factors included into the regression $\mathbf{f}$, the number of factors $K$ as well as the presence of the intercept depend on the model under consideration.  
In order to simplify the notation, we do not introduce a model index, however, it is important to keep in mind that the techniques discussed below are applied to each possible combination of factors.

Stacking observations for a given time period across assets, define $\mathbf{y}_t' = (y_{1t}, \hdots, y_{Dt})$ and analogously $\boldsymbol{\varepsilon}_t' = \left( \varepsilon_{t1}, \hdots \varepsilon_{tD} \right)$. 
Both $\mathbf{y}_t$ and $\boldsymbol{\varepsilon}_t$ are $D \times 1$ vectors.
Now let $\mathbf{x}_t' = (1, \mathbf{f}_t')$ be a $1 \times (K+1)$ vector and $\boldsymbol{\gamma}_d' = (\alpha_d, \boldsymbol{\beta}_d')$ to be a $1 \times (K+1)$ so we have
\begin{equation*}
\mathbf{y}_t = X_t \boldsymbol{\gamma} + \boldsymbol{\varepsilon}_t
\end{equation*}
where $X_t = I_D \otimes \mathbf{x}_t'$, a $D \times D(K+1)$ matrix, and $\boldsymbol{\gamma}'= \left( \boldsymbol{\gamma}_1', \hdots, \boldsymbol{\gamma}_D' \right)$, a $1 \times (K+1)D$ vector. 
If the model does not include a constant we simply remove the column of ones from the definition of $\mathbf{x}_t$.
Let $Y_T$ denote the full data sample, i.e.\ $\left\{ \mathbf{y}_t, \mathbf{x}_t \right\}_{t=1}^T$.

In order to apply Bayesian inference, we need to specify the distribution of the errors. 
Because asset returns are typically thought to be heavy-tailed our models will be based on a multivariate Student-t distribution in which we will allow the data to choose the degrees of freedom.\footnote{Our model selection exercise below will also consider a Gaussian error distribution although this specification is soundly rejected by the data.}
We have:
\begin{equation*}
\boldsymbol{\varepsilon}_{t}\sim t_{D,\nu }\left( 0,\Omega \right)
\end{equation*}
where $\nu$ denotes the degrees of freedom of the distribution, the location parameter is zero and the scale matrix is $\Omega$.
If $\nu >1$ then $E(\boldsymbol{\varepsilon}) = 0$. 
If $\nu>2$ then $Var\left( \boldsymbol{\varepsilon} \right) = \nu \Omega/(\nu-2)$. 
The SUR model lets us explicitly explore the cross-sectional dependence of errors by jointly fitting the model for all assets. 
The possible variance linkages between different assets are captured by the scale matrix $\Omega$. 
While acknowledging the importance of these linkages and the possible influence they may have on the model selection, we avoid placing \emph{a priori} restrictions on this matrix, such as assuming that it is diagonal, and instead infer the covariance structure of errors from the data.

To permit use of Gibbs sampler draws, we use representation of the Student-t distribution as a scale mixture of normal distributions, as in \cite{chib1995hierarchical}.
In particular,
\begin{eqnarray*}
\boldsymbol{\varepsilon}_{t}|\lambda _{t} &\sim N\left( 0,\lambda _{t}^{-1}\Omega \right)
\\
\lambda _{t} &\sim G\left( \frac{\nu }{2},\frac{\nu }{2}\right)
\end{eqnarray*}
where $G(\alpha,\beta)$ denotes the Gamma distribution with shape parameter $\alpha$ and rate parameter $\beta$. 
We parameterize this problem in terms of the $D\times D$ \emph{precision} matrix $\Omega^{-1}$ and the $p\times1$ vector of regression coefficients $\boldsymbol{\gamma}$, where $p = D(K+1)$. 
In any inference that we carry out, as well as in the calculation of the marginal likelihood, we will marginalize over $\boldsymbol{\lambda}$ by simply ignoring these draws.
We will place a normal prior on $\boldsymbol{\gamma}$ and a Wishart prior on $\Omega^{-1}$. 

\subsection{The Gibbs Sampler}
In order to approximate the posterior distribution we implement Gibbs sample following \cite{chib1996markov} and \cite{carlin1991inference}. The sampler proceeds by fixing the degrees of freedom parameter $\nu$.
If $\nu$ is to be chosen from the data, this can be accomplished using the marginal likelihood, as described below.
Holding $\nu$ fixed, the full set of conditional posteriors is as follows:

\paragraph{Regression Coefficients:} 
$\boldsymbol{\gamma}|\Omega^{-1},Y_T \sim \mathcal{N}_p\left( \bar{\boldsymbol{\gamma}}_{\lambda},G_{T,\lambda} \right)$
\begin{eqnarray*}
	G_{T,\lambda} &=& \left[ G_0^{-1} + \sum_{t=1}^T \lambda_t X_t' \Omega^{-1} X_t \right]^{-1}\\
	\bar{\boldsymbol{\gamma}}_{\lambda} &=& G_{T,\lambda} \left[ G_0^{-1}\boldsymbol{\gamma}_0 + \sum_{t=1}^{T}\lambda_t X_t'\Omega^{-1}\mathbf{y}_t \right]
\end{eqnarray*}
\paragraph{Inverse Scale Matrix:}
$\Omega^{-1}|Y_T \sim \mathcal{W}_D\left(\rho_0 + T, R_{T,\lambda}\right)$
\begin{equation*}
R_{T,\lambda} = \left[ R_0^{-1} + \sum_{t=1}^{T} \lambda_t\left( \mathbf{y}_t - X_t \boldsymbol{\gamma} \right)\left( \mathbf{y}_t - X_t \boldsymbol{\gamma} \right)' \right]^{-1}
\end{equation*}

\paragraph{Auxiliary Parameter:} $\lambda_t|\boldsymbol{\gamma}, \nu, Y_T \sim G\left(\displaystyle \frac{\nu + D}{2}, \frac{\nu + \boldsymbol{\varepsilon}_t' \boldsymbol{\varepsilon}_t}{2} \right)$, $\boldsymbol{\varepsilon}_t = \mathbf{y}_t - X_t \boldsymbol{\gamma}$

To implement the Gibbs sampler, we simply need to draw sequentially from these distributions, in the order given above. 
We will require, however, starting values for both $\Omega^{-1}$ and each of the $\lambda_t$ parameters. 
A reasonable starting value for $\lambda_t$ is one.

\subsection{Marginal Likelihood}
We calculate the marginal likelihood using the method of \cite{chib1995marginal}.
Consider the re-arranged Bayes rule:
\begin{equation*}
f(y) = \frac{f(y|\boldsymbol{\gamma},\Omega^{-1})\pi\left(\boldsymbol{\gamma}, \Omega^{-1}\right)}{\pi\left(\boldsymbol{\gamma}, \Omega^{-1}|y \right)}
\end{equation*}
which holds for any specified values $(\boldsymbol{\gamma},\Omega^{-1})$ of the parameters.
In particular this holds at the \emph{posterior mean} $(\boldsymbol{\gamma}^*,\Omega^{-1*})$ which is where we will evaluate the expression.
Hence, the \emph{log} marginal likelihood is given by
\begin{equation*}
\log{f(y)} =  \log \left(\boldsymbol{\gamma}^*, \Omega^{-1*}\right) + \log{f(y|\boldsymbol{\gamma}^*,\Omega^{-1*})} - \pi\left(\boldsymbol{\gamma}^*, \Omega^{-1*}|y \right)
\end{equation*}
Since our priors over $\boldsymbol{\gamma}$ and $\Omega^{-1}$ are independent, we can re-write the equation above as follows:
\begin{equation*}
\log f(Y_T) = \log \pi(\boldsymbol{\gamma}^*) + \log \pi\left( \Omega^{-1*} \right) + \log f\left( Y_T|\boldsymbol{\gamma}^*, \Omega^{-1*} \right) - \log \pi\left( \boldsymbol{\gamma}^*, \Omega^{-1*}|Y_T \right)
\end{equation*}
We can evaluate the marginal likelihood using the \cite{chib1995marginal} method that approximates $\log f(Y_T)$ by evaluating each of the terms on the right-hand-side of the preceding expression using the output of the Gibbs sampler.

\subsection{Priors}
In order for the marginal likelihood comparison to be valid, proper priors should be specified. 

Given that the factor coefficients should be determined jointly and that factors differ for each model, an individual level prior should be set for every model.
The same hold for the error precision matrix: its prior should be specified differently depending on the factors.   
In order to place model-specific priors, we suggest to use a training sample.
We fit each model to the training sample data and use the model-specific posterior means obtained from the Gibbs sampler as a benchmark to specify the prior for the estimation sample. 
The training sample is employed solely to construct priors and is not a part of the final estimation procedure. 

We place two prior distributions: normal on the coefficients and Wishart on the  precision matrix. 
Parameters for the two distributions are specified differently for each stage.


\subsubsection{Stage 1: Training Sample}
In order to apply Bayesian inference to the training sample, we should specify priors for this estimation step as well. \\
At this stage diffuse priors are employed in order to reflect the parameter uncertainty.
\paragraph{Regression Coefficients:} 
\begin{eqnarray*}
	\boldsymbol{\gamma} &\sim& \mathcal{N}_{p}\left( \boldsymbol{\gamma}_{0},G_{0}\right) \\
	\boldsymbol{\gamma}_{0} &=& 0 \\ 
	G_{0} &=& C_{1}^{2}I_{p} 
\end{eqnarray*}
The prior distribution of the coefficient vector $\boldsymbol{\gamma}$ is centered around zero. 
The covariance matrix of the coefficients is assumed to be diagonal.
The tightness of the prior is controlled by the constant $C_{1}$: the larger $C_{1}$ is, the wider is the prior.
\paragraph{Inverse Scale Matrix:}
\begin{eqnarray*}
	\Omega^{-1} &\sim& \mathcal{W}_D\left(\rho_0, R_{0}\right) \\
	\rho_{0} &=& d + C_{2} \\ 
	R_{0} &=& \frac{1}{C_{3}^{2} (\rho_{0}-d-1)}I_{d}
\end{eqnarray*} 
Using the properties of the inverse Wishart distribution, it can be seen that both the mean  of the scale matrix $\Omega$ and the precision matrix $\Omega^{-1}$ implied by the prior are diagonal.
The tightness of the prior is governed by the number of degrees of freedom $\rho_{0}$. One way of widening the prior would be to decrease number of degrees of freedom by adjusting the value of $C_2$. 
Constant $C_{3}$ is used to set the magnitude of the elements of the covariance matrix. \\

For the rest of the paper we set $C_{1} = 2$, $C_{2} = 6$ and $C_{3} = 0.05$.\\
The posterior draws obtained from the Gibbs sampler when fitting a model to the training sample are later used to form model-specific priors used for the estimation sample.

\subsubsection{Stage 2: Estimation Sample}
The first stage draws serve as a basis to construct proper priors for the estimation.
\paragraph{Regression Coefficients:} 
\begin{eqnarray*}
	\boldsymbol{\gamma} &\sim& \mathcal{N}_{p}\left( \boldsymbol{\gamma}_{0},G_{0}\right) \\
	\boldsymbol{\gamma}_{0} &=& \overline{\gamma} \\ 
	G_{0} &=& C_{4}^{2} \widehat{G}
\end{eqnarray*}\\
Denote first stage posterior means of the coefficient vector $\boldsymbol{\gamma}$ as $\overline{\boldsymbol{\gamma}}$ and the sample covariance matrix of these draws as $\widehat{G}$.
The prior of factor loadings is thus centered around the posterior mean of the first stage draws. 
The standard deviation is based on the sample standard deviation of the first stage draws adjusted by the factor of $C_{4}$ to reflect uncertainty.
\paragraph{Inverse Scale Matrix:}
\begin{eqnarray*}
	\Omega^{-1} &\sim& \mathcal{W}_D\left(\rho_0, R_{0}\right) \\
	\rho_{0} &=& d + C_{5} \\ 
	R_{0} &=& \frac{1}{\rho} \overline{\Omega}^{-1},
\end{eqnarray*} 
where $\overline{\Omega}^{-1}$ is the posterior mean of the first stage draws of $\Omega^{-1}$.
Note that our algorithm provides us draws of the precision matrix $\Omega^{-1}$, not of the covariance matrix $\Omega$. 
For this reason we form the prior in terms of the posterior mean of ${\Omega}^{-1}$. 
The prior is constructed to set the mean of the precision matrix $\Omega^{-1}$ equal to the posterior mean based on the training sample. 
Again, the tightness of the prior is controlled by degrees of freedom parameter $\rho_{0}$. 

For all the applications in this paper we use $C_{4} = 2$ and $C_5 = 15$.

\subsection{Training Sample Prior: An Example}
Here we illustrate the process of setting model specific priors and show how priors vary by the model depending on the included factors.

For simplicity consider a hypothetical example with the cross-section of returns consisting only of  two assets: industry portfolios for durable(Durbl) and non-durable consumer goods (NoDur).
We fit two models.
First model is a usual CAPM that includes an intercept and a market portfolio(Mkt.RF). 
A second model is an extended version of the first one that includes a value factor(HML) in addition to the constant and the market factor. 

First, we consider how the factor betas get updated over time:
\begin{eqnarray*}
\underset{\text{Stage 1 Prior}}
{\begin{blockarray}{ccc}
	& constant & Mkt.RF  \\ 
	\begin{block}{c(cc)}
	NoDur & 0.000 & 0.000 \\ 
	Durbl & 0.000 & 0.000 \\ 
	\end{block}
\end{blockarray} }
\rightarrow
\underset{\text{Stage 2 Prior}}
{\begin{blockarray}{ccc}
	& constant & Mkt.RF \\ 
	\begin{block}{c(cc)}
	NoDur & 0.000 & 0.992 \\ 
	Durbl & -0.001 & 1.046 \\ 
	\end{block}
\end{blockarray} }
\rightarrow \\
\rightarrow
\underset{\text{Posterior}}
{\begin{blockarray}{ccc}
	& constant & Mkt.RF \\ 
	\begin{block}{c(cc)}
	NoDur & 0.004 & 0.793 \\ 
	Durbl & -0.002 & 1.164 \\ 
	\end{block}
\end{blockarray}}
\end{eqnarray*}
This equation demonstrates the update of beliefs about the CAPM coefficients.
The first stage prior is centered around zero for all coefficients. 
The second stage prior based on the training sample reflects the peculiarities of the data: e.g. the market beta for non-durable goods got updated from 0 to 0.992. 
Finally, the posterior is an updated mean based on the estimation sample. 
E.g. the market beta for non-durable goods was corrected from 0.992 to 0.793 to reflect the lower market risk associated with this asset that can be observed in the estimation sample.
\begin{eqnarray*}
\underset{\text{Stage 1 Prior}}
{\begin{blockarray}{cccc}
	& constant & Mkt.RF & HML \\ 
	\begin{block}{c(ccc )}
	NoDur & 0.000 & 0.000 & 0.000 \\ 
	Durbl & 0.000 & 0.000 & 0.000 \\ 
	\end{block}
\end{blockarray} }
\rightarrow
\underset{\text{Stage 2 Prior}}
{\begin{blockarray}{cccc}
	& constant & Mkt.RF & HML\\ 
	\begin{block}{c(ccc)}
NoDur & 0.001 & 0.981 & -0.078 \\ 
Durbl & -0.003 & 1.080 & 0.296 \\ 
	\end{block}
\end{blockarray} }
\rightarrow \\
\rightarrow
\underset{\text{Posterior}}
{\begin{blockarray}{cccc}
	& constant & Mkt.RF & HML \\ 
	\begin{block}{c(ccc)}
	NoDur & 0.003 & 0.807 & 0.136 \\ 
	Durbl & -0.004 & 1.240 & 0.479 \\ 
	\end{block}
\end{blockarray} }
\end{eqnarray*}
This equation shows the evolution of prior for an extended model. 
We can see that the second stage prior is different from the one we place on a CAPM model. 
Even though the market factor is included into both models, the prior mean of the market beta is now different for both portfolios than it was before because of an added factor.  
For instance, the market beta for non-durable goods was set to be zero in both models in the first stage prior.
The second stage prior values are model-specific: 0.992 for the CAPM and 0.981 for the extended model.
This results into a different albeit close posterior mean of the beta:  0.807 vs.  0.792.
This example shows that priors of the coefficient vectors vary by the model even the two models include the same factors.

Similarly, we can take look at how the precision matrix $\Omega^{-1}$ is updated. First, consider CAPM:
\begin{eqnarray*}
\underset{\text{Stage 1 Prior}}
{\begin{blockarray}{ccc}
	& NoDur & Durbl  \\ 
	\begin{block}{c(cc)}
	NoDur & 640 & 0 \\ 
	Durbl & 0 & 640 \\ 
	\end{block}
\end{blockarray} }
\rightarrow
\underset{\text{Stage 2 Prior}}
{\begin{blockarray}{ccc}
& NoDur & Durbl \\ 
	\begin{block}{c(cc)}
	NoDur & 3795 & -277 \\ 
	Durbl & -277 & 1907 \\ 
	\end{block}
\end{blockarray} }
\rightarrow \\
\rightarrow
\underset{\text{Posterior}}
{\begin{blockarray}{ccc}
	& NoDur & Durbl \\ 
	\begin{block}{c(cc)}
	NoDur & 2283 & 155 \\ 
	Durbl & 155 & 1013 \\ 
	\end{block}
\end{blockarray}}
\end{eqnarray*}
The following equation shows the changes in the inverse scale matrix for an extended model that includes the value portfolio:

\begin{eqnarray*}
\underset{\text{Stage 1 Prior}}
{\begin{blockarray}{ccc}
& NoDur & Durbl  \\ 
	\begin{block}{c(cc)}
	NoDur & 0.000 & 0.000 \\ 
	Durbl & 0.000 & 0.000 \\ 
	\end{block}
\end{blockarray} }
\rightarrow
\underset{\text{Stage 2 Prior}}
{\begin{blockarray}{ccc}
& NoDur & Durbl \\ 
	\begin{block}{c(cc)}
	NoDur & 3864 & -397 \\ 
	Durbl & -397 & 2045 \\
	\end{block}
\end{blockarray} }
\rightarrow \\
\rightarrow
\underset{\text{Posterior}}
{\begin{blockarray}{ccc}
& NoDur & Durbl \\ 
	\begin{block}{c(cc)}
	NoDur & 2270 & 224 \\ 
	Durbl & 224 & 1117 \\ 
	\end{block}
\end{blockarray}}
\end{eqnarray*}
As can be seen the prior for the precision matrix varies with the model as well. 

This example show that the proposed procedure allows to automatically specify proper priors for each model.

