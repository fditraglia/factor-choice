\documentclass[12pt]{article}
\usepackage{enumerate}
\usepackage{amsmath, amssymb}
\linespread{1.2}



\begin{document}


\section{Model and Likelihood}

Consider a linear $K$-factor model for $D$ assets of the form
\begin{equation*}
  y_{it}=\alpha _{d} + \mathbf{f}_{t}' \boldsymbol{\beta}_d + \varepsilon_{it}
\end{equation*}
where $d = 1, \hdots, D$ and $t = 1, \hdots, T$ and $\mathbf{f}_{t}'=\left(f_{t1}, \hdots,f_{tK}\right)$ is a $K\times1$ vector.
This is a special case of the seemingly unrelated regression (SUR) model in which the regressors are \emph{identical} across equations.
Stacking observations for a given time period across assets, define $\mathbf{y}_t' = (y_{1t}, \hdots, y_{Dt})$ and analogously $\boldsymbol{\varepsilon}_t' = \left( \varepsilon_{t1}, \hdots \varepsilon_{tD} \right)$.
Now let $\mathbf{x}_t' = (1, \mathbf{f}_t')$ and $\boldsymbol{\gamma}_d' = (\alpha_d, \boldsymbol{\beta}_d')$ so we have
\begin{equation*}
  \mathbf{y}_t = X_t \boldsymbol{\gamma} + \boldsymbol{\varepsilon}_t
\end{equation*}
where $X_t = I_D \otimes \mathbf{x}_t'$ and $\boldsymbol{\gamma}'= \left( \boldsymbol{\gamma}_1', \hdots, \boldsymbol{\gamma}_D' \right)$. 
Now, suppose that 
\begin{equation*}
  \boldsymbol{\varepsilon}_t|\mathbf{x}_t \sim \mbox{ iid } \mathcal{N}_D(0, \Omega^{-1})
\end{equation*}
Let $Y_T$ denote the full data sample, i.e.\ $\left\{ \mathbf{y}_t, \mathbf{x}_t \right\}_{t=1}^T$.
Then the likelihood is 
\begin{equation*}
  \pi(\boldsymbol{\gamma},\Omega^{-1}|Y_T) \propto |\Omega^{-1}|^{T/2} \exp\left[ -\frac{1}{2} \sum_{t=1}^T \left(\mathbf{y}_t - X_t \boldsymbol{\gamma} \right)' \Omega^{-1} \left(\mathbf{y}_t - X_t \boldsymbol{\gamma} \right)\right]
\end{equation*}
where we parameterize this problem in terms of the $D\times D$ \emph{precision} matrix $\Omega^{-1}$ and the $p\times1$ vector of regression coefficients $\boldsymbol{\gamma}$, where $p = D(K+1)$.

\section{Prior and Posterior Distribution}
To complete the model we specify the following prior distribution
\begin{equation*}
  \pi (\boldsymbol{\gamma},\Omega^{-1})=\mathcal{N}_{p}\left( \boldsymbol{\gamma} |\boldsymbol{\gamma_{0}},G_{0}\right)
\mathcal{W}_{D}\left(\Omega ^{-1}|\rho_{0}, R_{0}\right)
\end{equation*}
This prior is conditionally conjugate with the normal likelihood.
In particular, we have 
$\boldsymbol{\gamma}|\Omega^{-1},Y_T \sim \mathcal{N}_p\left( \bar{\boldsymbol{\gamma}},G_T \right)$
where
\begin{eqnarray*}
  G_T &=& \left[ G_0^{-1} + \sum_{t=1}^T X_t' \Omega^{-1} X_t \right]^{-1}\\
  \bar{\boldsymbol{\gamma}} &=& G_T \left[ G_0^{-1}\boldsymbol{\gamma}_0 + \sum_{t=1}^{T} X_t'\Omega^{-1}\mathbf{y}_t \right]
\end{eqnarray*}
and
$\Omega^{-1}|Y_T \sim \mathcal{W}_D\left(\rho_0 + T, R_T \right)$
where
\begin{equation*}
  R_T = \left[ R_0^{-1} + \sum_{t=1}^{T} \left( \mathbf{y}_t - X_t \boldsymbol{\gamma} \right)\left( \mathbf{y}_t - X_t \boldsymbol{\gamma} \right)' \right]^{-1}
\end{equation*}

\section{MCMC}
Using the full set of conditional posteriors, given in the preceding section, we can simulate from the joint posterior for this model using a Gibbs sampler:
\begin{enumerate}
  \item Select a starting value $\Omega^{-1(0)}$ for the precision matrix.
  \item Draw $\boldsymbol{\gamma}^{(1)} \sim \mathcal{N}\left( \bar{\boldsymbol{\gamma}}^{(1)}, G_T^{(1)}\right)$ where
    \begin{eqnarray*}
      G_T^{(1)} &=& \left[ G_0^{-1} + \sum_{t=1}^T X_t' \Omega^{-1(0)} X_t \right]^{-1}\\
      \bar{\boldsymbol{\gamma}}^{(1)} &=& G_T^{(1)} \left[ G_0^{-1}\boldsymbol{\gamma}_0 + \sum_{t=1}^{T} X_t'\Omega^{-1(0)}\mathbf{y}_t \right]
    \end{eqnarray*}
  \item Draw $\Omega^{-1(1)} \sim \mathcal{W}_D\left(\rho_T, R_T^{(1)}\right)$ where
    \begin{equation*}
      R_T^{(1)} = \left[ R_0^{-1} + \sum_{t=1}^{T} \left( \mathbf{y}_t - X_t \boldsymbol{\gamma}^{(1)} \right)\left( \mathbf{y}_t - X_t \boldsymbol{\gamma}^{(1)} \right)' \right]^{-1}
    \end{equation*}
  \item Repeat the preceding two steps a total of $G$ times.
    In the $g$th iteration:
    \begin{enumerate}[(i)]
       \item Draw $\boldsymbol{\gamma}^{(g)} \sim \mathcal{N}\left( \bar{\boldsymbol{\gamma}}^{(g)}, G_T^{(g)}\right)$ where
    \begin{eqnarray*}
      G_T^{(g)} &=& \left[ G_0^{-1} + \sum_{t=1}^T X_t' \Omega^{-1(g-1)} X_t \right]^{-1}\\
      \bar{\boldsymbol{\gamma}}^{(g)} &=& G_T^{(g)} \left[ G_0^{-1}\boldsymbol{\gamma}_0 + \sum_{t=1}^{T} X_t'\Omega^{-1(g-1)}\mathbf{y}_t \right]
    \end{eqnarray*}
       \item Draw $\Omega^{-1(g)} \sim \mathcal{W}_D\left(\rho_T, R_T^{(g)}\right)$ where
    \begin{equation*}
      R_T^{(g)} = \left[ R_0^{-1} + \sum_{t=1}^{T} \left( \mathbf{y}_t - X_t \boldsymbol{\gamma}^{(g)} \right)\left( \mathbf{y}_t - X_t \boldsymbol{\gamma}^{(g)} \right)' \right]^{-1}
    \end{equation*}
    \end{enumerate}
  \item Discard the first $B$ draws.
\end{enumerate}
Note that in iteration $g$, $G_T^{(g)}$ and $\bar{\boldsymbol{\gamma}}^{(g)}$ are calculated using $\Omega^{-1(g-1)}$ while $R_T^{(g)}$ is calculated using $\boldsymbol{\gamma}^{(0)}$.
This is because we choose to initialize the sample with a starting value $\Omega^{-1(0)}$ for the precision matrix rather than for the vector of regression coefficients.

\section{Numerical Details for the Gibbs Sampler}


\subsection{Drawing from a Normal Distribution}
Parameterize in terms of the precision matrix.

\subsection{Drawing from a Wishart Distribution}
Use the Bartlett Decomposition.

\subsection{Efficient Calculation of $R_T$}
In the second step of each iteration we compute 
$\left( R_0^{-1} + \sum_{t=1}^{T} \hat{\boldsymbol{\varepsilon}}_t\hat{\boldsymbol{\varepsilon}}_t'\right)^{-1}$ 
where 
$\hat{\boldsymbol{\varepsilon}}_t=\mathbf{y}_t - X_t \boldsymbol{\gamma}$.
Since $R_0$ is simply the prior scale matrix for $\Omega^{-1}$ and hence remains unchanged during the iterations, we can pre-compute it and store the result before starting the sampler.
Since $X_t$ is a sparse matrix, there is a much more efficient and compact way to compute the sum of outer products of residuals.
Define:
\begin{equation*}
  \widetilde{Y} = \left[
  \begin{array}{c}
    \mathbf{y}_1'\\
    \vdots \\
    \mathbf{y}_T'
  \end{array}
\right], \quad \widetilde{X} = \left[
\begin{array}{c}
  \mathbf{x}_{1}' \\
  \vdots \\
  \mathbf{x}_{T}' 
\end{array}
\right], \quad \Gamma = \left[
\begin{array}{ccc}
  \boldsymbol{\gamma}_1 & \cdots & \boldsymbol{\gamma}_D
\end{array}
\right], \quad \hat{\boldsymbol{\varepsilon}} = \left[
\begin{array}{c}
  \hat{\boldsymbol{\varepsilon}}_1' \\
  \vdots \\
  \hat{\boldsymbol{\varepsilon}}_T'
\end{array}
\right]
\end{equation*}
so that $\hat{\boldsymbol{\varepsilon}} = \widetilde{Y} - \widetilde{X} \Gamma$.
Note that the vector of regression coefficients $\boldsymbol{\gamma}$ is the vec of the \emph{matrix} of regression coefficients $\Gamma$. 
Thus, expressed in terms of dense matrix operations
\begin{equation*}
  R_T^{-1} =  R_0^{-1} + \left( \widetilde{Y} - \widetilde{X} \Gamma \right)'\left( \widetilde{Y} - \widetilde{X} \Gamma \right)
\end{equation*}
The final step is to invert this sum (which is positive definite) to calculate $R_T$.
Note that the Matrix Inversion Lemma (Sherman-Morrison-Woodbury Formula) does \emph{not} simplify this calculation unless $D > T$.

\subsection{Efficient Calculation of $G_T$}
Because we parameterize our multivariate normal sampler in terms of the \emph{precision} matrix rather than the covariance matrix, we work with the \emph{inverse} of $G_T$, namely
\begin{equation*}
  G_T^{-1} =  G_0^{-1} + \sum_{t=1}^T X_t' \Omega^{-1} X_t 
\end{equation*}
Since it is simply the prior precision matrix for the vector $\boldsymbol{\gamma}$ of regression coefficients we can pre-compute $G_0$ (assuming that we elicit a prior in terms of the covariance matrix).
Now, the sum over $X_t' \Omega X_t$ can in fact be simplified using the properties of the Kronecker product.\footnote{See, e.g., Horn and Johnson (1994) Chapter 4.2.}
Recall that $X_t = I_D \otimes \mathbf{x}_t$.
Since $\left( A\otimes B \right)' = A' \otimes B'$, 
\begin{equation*}
  X_t' \Omega^{-1} X_t = \left( I_D \otimes \mathbf{x}_t \right) \Omega^{-1}X_t
\end{equation*}
Since $\Omega^{-1}X_t = \left( \Omega^{-1}X_t \right) \otimes 1$, $\Omega^{-1} = \Omega^{-1}\otimes 1$, and $(A \otimes B)(C \otimes D) = AC \otimes BD$, provided that everything is conformable, we have
\begin{eqnarray*}
  \left( I_D \otimes \mathbf{x}_t \right) \Omega^{-1}X_t &=&  \left( I_D \otimes \mathbf{x}_t \right) (\Omega^{-1}X_t \otimes 1)\\
  &=&  \Omega^{-1} X_t \otimes \mathbf{x}_t = \left[\Omega^{-1} (I_D \otimes \mathbf{x}_t') \right]\otimes \mathbf{x}_t \\
  &=& \left[(\Omega^{-1} \otimes 1)(I_D \otimes \mathbf{x}_t') \right]\otimes \mathbf{x}_t \\
  &=& \Omega^{-1} \otimes \mathbf{x}_t' \otimes \mathbf{x}_t
\end{eqnarray*}
Finally, since  $A \otimes (B + C) = A\otimes B + A \otimes C$,
\begin{equation*}
  \sum_{t=1}^{T} X_t' \Omega^{-1} X_t = \sum_{t=1}^{T} \Omega^{-1} \otimes \mathbf{x}_t' \otimes \mathbf{x}_t = \Omega^{-1} \otimes \left(\sum_{t=1}^T  \mathbf{x}_t' \otimes \mathbf{x}_t \right)
\end{equation*}
This is an extremely useful simplification: because $\sum_{t=1}^T  \mathbf{x}_t' \otimes \mathbf{x}_t$ involves neither $\Omega^{-1}$ nor $\boldsymbol{\gamma}$, only the data, we can pre-compute this quantity.
In fact, there is one final simplification that makes this quantity even simpler. 
By writing out the definition of the Kronecker Product, we see that $\mathbf{x}_t' \otimes \mathbf{x}_t = \mathbf{x}_t \mathbf{x}_t'$
and hence
\begin{equation*}
  \sum_{t=1}^{T} X_t' \Omega^{-1} X_t = \Omega^{-1} \otimes \left( \sum_{t=1}^T \mathbf{x}_t \mathbf{x}_t' \right) = \Omega^{-1} \otimes \widetilde{X}' \widetilde{X}
\end{equation*}
where $\widetilde{X}' = \left[
\begin{array}{ccc}
  \mathbf{x}_1 & \cdots & \mathbf{x}_T
\end{array}
\right]$.
Thus, we have $G_T^{-1} = G_{0}^{-1} + \Omega^{-1} \otimes \widetilde{X}'\widetilde{X}$.
\subsection{Efficient Calculation of $\bar{\gamma}$}


\section{Calculating the Marginal likelihood}

The marginal likelihood is available by the method of Chib (1995). From the
Chib (1995) identity, we have
\begin{equation*}
\log m\left( \left\{ y_{t}\right\} \right) =\log \pi \left( \gamma ^{\ast
}\right) +\log \pi \left( \Omega ^{-1\ast }\right) +\sum_{t=1}^{n}\log
p\left( y_{t}|X_{t}\gamma ^{\ast },\Omega ^{\ast }\right) -\log \pi \left(
\gamma ^{\ast },\Omega ^{-1\ast }|Y_{n}\right)
\end{equation*}%
where the last term is calculated as%
\begin{equation*}
\pi \left( \Omega ^{-1\ast }|Y_{n}\right) \times \pi \left( \gamma ^{\ast
}|Y_{n},\Omega ^{-1\ast }\right)
\end{equation*}%
in which the first term is estimated by averaging the full-conditional
Wishart density over the draws $\left\{ \gamma ^{(g)}\right\} _{g=1}^{G}$
from the main MCMC run
\begin{equation*}
\pi \left( \Omega ^{-1\ast }|Y_{n}\right) =\frac{1}{G}\sum_{g=1}^{G}\mathcal{%
W}_{d}\left( \Omega ^{-1\ast }|\rho _{0}+n,\left(
R_{0}^{-1}+\sum_{t=1}^{n}\left( y_{t}-X_{t}\gamma ^{(g)}\right) \left(
y_{t}-X_{t}\gamma ^{(g)}\right) ^{\prime }\right) ^{-1}\right)
\end{equation*}%
and the second term $\pi \left( \gamma ^{\ast }|Y_{n},\Omega ^{-1\ast
}\right) $ is available directly as%
\begin{equation*}
\pi (\gamma ^{\ast }|Y_{n},\Omega ^{-1\ast })=\mathcal{N}_{d+p}\left( \gamma
^{\ast }|\hat{\gamma}^{\ast },G_{n}^{\ast }\right)
\end{equation*}%
where%
\begin{align*}
\hat{\gamma}^{\ast }& =G_{n}^{\ast }\left( G_{0}^{-1}\gamma
_{0}+\sum_{t=1}^{n}X_{t}^{\prime }\Omega ^{-1\ast }y_{t}\right)  \\
G_{n}^{\ast }& =\left( G_{0}^{-1}+\sum_{t=1}^{n}X_{t}^{\prime }\Omega
^{-1\ast }X_{t}\right) ^{-1}
\end{align*}

\section{Prediction}

Suppose we are interested in predicting the cross-section of returns $y_{n+1}
$ at time $(n+1)$. The Bayes prediction density of these returns,
conditioned on the data $Y_{n+1}$ and the factors $f_{n+1}$, is given by%
\begin{equation*}
p(y_{n+1}|Y_{n},f_{n+1})=\int_{\gamma ,\Omega ^{-1}}\mathcal{N}_{d}\left(
y_{n+1}|X_{n+1}\gamma ,\Omega \right) d\pi \left( \gamma ,\Omega
^{-1}|Y_{n}\right)
\end{equation*}%
which is estimated by the ergodic Monte Carlo average
\begin{equation*}
p(y_{n+1}|Y_{n},f_{n+1})=\frac{1}{G}\sum_{g=1}^{G}\mathcal{N}_{d}\left(
y_{n+1}|X_{n+1}\gamma ^{(g)},\Omega ^{(g)}\right)
\end{equation*}%
with the MCMC draws $\left\{ \gamma ^{(g)},\Omega ^{(g)}\right\} $ from the
posterior distribution.


\section{Student-t errors}

Suppose now that the errors are distributed as multivariate-t%
\begin{equation*}
\varepsilon _{t}\sim t_{d,\nu }\left( 0,\Omega \right)
\end{equation*}%
so that%
\begin{equation*}
E(\varepsilon _{t})=0\,,\;\nu >1
\end{equation*}%
\begin{equation*}
Var\left( \varepsilon _{t}\right) =\frac{\nu }{\nu -2}\Omega ,\;\nu >2
\end{equation*}%
The analysis of this model utilizes the hierarchical reprentation
\begin{align*}
\varepsilon _{t}|\lambda _{t} &\sim N\left( 0,\lambda _{t}^{-1}\Omega \right)
\\
\lambda _{t} &\sim G\left( \frac{\nu }{2},\frac{\nu }{2}\right)
\end{align*}%
which means that conditioned on $\left( \nu ,\left\{ \lambda _{t}\right\}
\right) $, the results presented for the Gaussian model can be applied with
minor modifications. The MCMC\ sampling is completed with the sampling of $%
\left( \nu ,\left\{ \lambda _{t}\right\} \right) $.

Following Albert and Chib (1993), let us assume that the support of $\nu $
is the set of values $\left\{ \nu _{j}\right\} _{j=1}^{J}$, for example, $%
\left\{ 4,6,8,10,12,14,16\right\} $ and that a priori
\begin{equation*}
\Pr \left( \nu =\nu _{j}\right) =q_{j}
\end{equation*}%
Then, simple calculations show that
\begin{equation*}
\gamma |Y_{n},\Omega ^{-1},\nu ,\left\{ \lambda _{t}\right\} \sim \mathcal{N}%
_{d+p}\left( \hat{\gamma}_{\lambda },G_{n,\lambda }\right)
\end{equation*}%
where
\begin{align*}
\hat{\gamma}_{\lambda }& =G_{n,\lambda }\left( G_{0}^{-1}\gamma
_{0}+\sum_{t=1}^{n}\lambda _{t}X_{t}^{\prime }\Omega ^{-1}y_{t}\right)  \\
G_{n,\lambda }& =\left( G_{0}^{-1}+\sum_{t=1}^{n}\lambda _{t}X_{t}^{\prime
}\Omega ^{-1}X_{t}\right) ^{-1}
\end{align*}%
and
\begin{equation*}
\Omega ^{-1}|Y_{n},\gamma ,\nu ,\left\{ \lambda _{t}\right\} \sim \mathcal{W}%
_{d}\left( \rho _{0}+n,\left( R_{0}^{-1}+\sum_{t=1}^{n}\lambda _{t}\left(
y_{t}-X_{t}\gamma \right) \left( y_{t}-X_{t}\gamma \right) ^{\prime }\right)
^{-1}\right)
\end{equation*}%
Moreover,%
\begin{equation*}
\Pr \left( \nu =\nu _{j}|Y_{n},\gamma ,\Omega ^{-1}\right) \propto
q_{j}\prod\limits_{t=1}^{n}t_{d,\nu _{j}}\left( y_{t}|X_{t}\gamma ,\Omega
\right)
\end{equation*}%
and%
\begin{equation*}
\lambda _{t}|Y_{n},\gamma ,\nu \sim G\left( \frac{\nu +d}{2},\frac{\nu
+\left( y_{t}-X_{t}\gamma \right) ^{\prime }\left( y_{t}-X_{t}\gamma \right)
}{2}\right)
\end{equation*}%
One sweep of the MCMC\ sampling is completed by sampling these four
distributions in this order.

\subsection{Marginal likelihood}

The Chib (1995) method can again be applied to find the log marginal
likelihood as
\begin{equation*}
\log \Pr \left( \nu ^{\ast }\right) +\log \pi \left( \gamma ^{\ast }\right)
+\log \pi \left( \Omega ^{-1\ast }\right) +\sum_{t=1}^{n}\log t_{d,\nu
^{\ast }}\left( y_{t}|X_{t}\gamma ^{\ast },\Omega ^{\ast }\right) -\log \pi
\left( \nu ^{\ast },\gamma ^{\ast },\Omega ^{-1\ast }|Y_{n}\right)
\end{equation*}%
where $\nu ^{\ast }$ is the posterior mode (which is easily computed from
the sampled values), the last term is calculated as%
\begin{equation*}
\Pr \left( \nu ^{\ast }|Y_{n}\right) \times \pi \left( \Omega ^{-1\ast
}|Y_{n},\nu ^{\ast }\right) \times \pi \left( \gamma ^{\ast }|Y_{n},\Omega
^{-1\ast },\nu ^{\ast }\right)
\end{equation*}%
in which the first term is obtained from the posterior frequency
distribution of $\nu $, the second term is obtained from a reduced run in
which $\nu $ is fixed at $\nu ^{\ast }$ and the remaining three
distributions are sampled and the draws
\begin{equation*}
\left\{ \gamma ^{(g)},\lambda _{t}^{(g)}\right\} _{g=1}^{G}
\end{equation*}%
from this reduced MCMC run are used to calculate $\pi \left( \Omega ^{-1\ast
}|Y_{n},\nu ^{\ast }\right) $ as
\begin{equation*}
\frac{1}{G}\sum_{g=1}^{G}\mathcal{W}_{d}\left( \Omega ^{-1\ast }|\rho
_{0}+n,\left( R_{0}^{-1}+\sum_{t=1}^{n}\lambda _{t}^{(g)}\left(
y_{t}-X_{t}\gamma ^{(g)}\right) \left( y_{t}-X_{t}\gamma ^{(g)}\right)
^{\prime }\right) ^{-1}\right)
\end{equation*}%
and the final term $\pi \left( \gamma ^{\ast }|Y_{n},\Omega ^{-1\ast
}\right) $ is obtained from a second reduced run in which $\nu $ is fixed at
$\nu ^{\ast }$ and $\Omega ^{-1}$ is fixed at $\Omega ^{-1\ast }$ and the
draws
\begin{equation*}
\left\{ \lambda _{t}^{(g)}\right\}
\end{equation*}%
from this reduced run are used to give
\begin{equation*}
\pi (\gamma ^{\ast }|Y_{n},\Omega ^{-1\ast },\nu ^{\ast })=\frac{1}{G}%
\sum_{g=1}^{G}\mathcal{N}_{d+p}\left( \gamma ^{\ast }|\hat{\gamma}_{\lambda
^{(g)}}^{\ast },G_{n,\lambda ^{(g)}}^{\ast }\right)
\end{equation*}%
where%
\begin{align*}
\hat{\gamma}_{\lambda ^{(g)}}^{\ast }& =G_{n,\lambda ^{(g)}}^{\ast }\left(
G_{0}^{-1}\gamma _{0}+\sum_{t=1}^{n}\lambda _{t}^{(g)}X_{t}^{\prime }\Omega
^{-1\ast }y_{t}\right)  \\
G_{n,\lambda ^{(g)}}^{\ast }& =\left( G_{0}^{-1}+\sum_{t=1}^{n}\lambda
_{t}^{(g)}X_{t}^{\prime }\Omega ^{-1\ast }X_{t}\right) ^{-1}
\end{align*}

\end{document}
