\documentclass[12pt]{article}
\usepackage{enumerate, hyperref}
\usepackage{amsmath, amssymb}
\linespread{1.2}



\begin{document}


\section{Model and Likelihood}

Consider a linear $K$-factor model for $D$ assets of the form
\begin{equation*}
  y_{it}=\alpha _{d} + \mathbf{f}_{t}' \boldsymbol{\beta}_d + \varepsilon_{it}
\end{equation*}
where $d = 1, \hdots, D$ and $t = 1, \hdots, T$ and $\mathbf{f}_{t}'=\left(f_{t1}, \hdots,f_{tK}\right)$ is a $K\times1$ vector.
This is a special case of the seemingly unrelated regression (SUR) model in which the regressors are \emph{identical} across equations.
Stacking observations for a given time period across assets, define $\mathbf{y}_t' = (y_{1t}, \hdots, y_{Dt})$ and analogously $\boldsymbol{\varepsilon}_t' = \left( \varepsilon_{t1}, \hdots \varepsilon_{tD} \right)$.
Now let $\mathbf{x}_t' = (1, \mathbf{f}_t')$ and $\boldsymbol{\gamma}_d' = (\alpha_d, \boldsymbol{\beta}_d')$ so we have
\begin{equation*}
  \mathbf{y}_t = X_t \boldsymbol{\gamma} + \boldsymbol{\varepsilon}_t
\end{equation*}
where $X_t = I_D \otimes \mathbf{x}_t'$ and $\boldsymbol{\gamma}'= \left( \boldsymbol{\gamma}_1', \hdots, \boldsymbol{\gamma}_D' \right)$. 
Now, suppose that 
\begin{equation*}
  \boldsymbol{\varepsilon}_t|\mathbf{x}_t \sim \mbox{ iid } \mathcal{N}_D(0, \Omega)
\end{equation*}
Let $Y_T$ denote the full data sample, i.e.\ $\left\{ \mathbf{y}_t, \mathbf{x}_t \right\}_{t=1}^T$.
Then the likelihood is 
\begin{equation*}
  \pi(Y_T|\boldsymbol{\gamma},\Omega^{-1}) \propto |\Omega^{-1}|^{T/2} \exp\left[ -\frac{1}{2} \sum_{t=1}^T \left(\mathbf{y}_t - X_t \boldsymbol{\gamma} \right)' \Omega^{-1} \left(\mathbf{y}_t - X_t \boldsymbol{\gamma} \right)\right]
\end{equation*}
where we parameterize this problem in terms of the $D\times D$ \emph{precision} matrix $\Omega^{-1}$ and the $p\times1$ vector of regression coefficients $\boldsymbol{\gamma}$, where $p = D(K+1)$.

\section{Prior and Posterior Distribution}
To complete the model we specify the following prior distribution
\begin{equation*}
  \pi (\boldsymbol{\gamma},\Omega^{-1})=\mathcal{N}_{p}\left( \boldsymbol{\gamma} |\boldsymbol{\gamma_{0}},G_{0}\right)
\mathcal{W}_{D}\left(\Omega ^{-1}|\rho_{0}, R_{0}\right)
\end{equation*}
This prior is conditionally conjugate with the normal likelihood.
In particular, we have 
$\boldsymbol{\gamma}|\Omega^{-1},Y_T \sim \mathcal{N}_p\left( \bar{\boldsymbol{\gamma}},G_T \right)$
where
\begin{eqnarray*}
  G_T &=& \left[ G_0^{-1} + \sum_{t=1}^T X_t' \Omega^{-1} X_t \right]^{-1}\\
  \bar{\boldsymbol{\gamma}} &=& G_T \left[ G_0^{-1}\boldsymbol{\gamma}_0 + \sum_{t=1}^{T} X_t'\Omega^{-1}\mathbf{y}_t \right]
\end{eqnarray*}
and
$\Omega^{-1}|Y_T \sim \mathcal{W}_D\left(\rho_0 + T, R_T \right)$
where
\begin{equation*}
  R_T = \left[ R_0^{-1} + \sum_{t=1}^{T} \left( \mathbf{y}_t - X_t \boldsymbol{\gamma} \right)\left( \mathbf{y}_t - X_t \boldsymbol{\gamma} \right)' \right]^{-1}
\end{equation*}

\section{MCMC}
Using the full set of conditional posteriors, given in the preceding section, we can simulate from the joint posterior for this model using a Gibbs sampler:
\begin{enumerate}
  \item Select a starting value $\Omega^{-1(0)}$ for the precision matrix.
  \item Draw $\boldsymbol{\gamma}^{(1)} \sim \mathcal{N}\left( \bar{\boldsymbol{\gamma}}^{(1)}, G_T^{(1)}\right)$ where
    \begin{eqnarray*}
      G_T^{(1)} &=& \left[ G_0^{-1} + \sum_{t=1}^T X_t' \Omega^{-1(0)} X_t \right]^{-1}\\
      \bar{\boldsymbol{\gamma}}^{(1)} &=& G_T^{(1)} \left[ G_0^{-1}\boldsymbol{\gamma}_0 + \sum_{t=1}^{T} X_t'\Omega^{-1(0)}\mathbf{y}_t \right]
    \end{eqnarray*}
  \item Draw $\Omega^{-1(1)} \sim \mathcal{W}_D\left(\rho_T, R_T^{(1)}\right)$ where
    \begin{equation*}
      R_T^{(1)} = \left[ R_0^{-1} + \sum_{t=1}^{T} \left( \mathbf{y}_t - X_t \boldsymbol{\gamma}^{(1)} \right)\left( \mathbf{y}_t - X_t \boldsymbol{\gamma}^{(1)} \right)' \right]^{-1}
    \end{equation*}
  \item Repeat the preceding two steps a total of $G$ times.
    In the $g$th iteration:
    \begin{enumerate}[(i)]
       \item Draw $\boldsymbol{\gamma}^{(g)} \sim \mathcal{N}\left( \bar{\boldsymbol{\gamma}}^{(g)}, G_T^{(g)}\right)$ where
    \begin{eqnarray*}
      G_T^{(g)} &=& \left[ G_0^{-1} + \sum_{t=1}^T X_t' \Omega^{-1(g-1)} X_t \right]^{-1}\\
      \bar{\boldsymbol{\gamma}}^{(g)} &=& G_T^{(g)} \left[ G_0^{-1}\boldsymbol{\gamma}_0 + \sum_{t=1}^{T} X_t'\Omega^{-1(g-1)}\mathbf{y}_t \right]
    \end{eqnarray*}
       \item Draw $\Omega^{-1(g)} \sim \mathcal{W}_D\left(\rho_T, R_T^{(g)}\right)$ where
    \begin{equation*}
      R_T^{(g)} = \left[ R_0^{-1} + \sum_{t=1}^{T} \left( \mathbf{y}_t - X_t \boldsymbol{\gamma}^{(g)} \right)\left( \mathbf{y}_t - X_t \boldsymbol{\gamma}^{(g)} \right)' \right]^{-1}
    \end{equation*}
    \end{enumerate}
  \item Discard the first $B$ draws.
\end{enumerate}
Note that in iteration $g$, $G_T^{(g)}$ and $\bar{\boldsymbol{\gamma}}^{(g)}$ are calculated using $\Omega^{-1(g-1)}$ while $R_T^{(g)}$ is calculated using $\boldsymbol{\gamma}^{(0)}$.
This is because we choose to initialize the sample with a starting value $\Omega^{-1(0)}$ for the precision matrix rather than for the vector of regression coefficients.

\section{Computational Details}

\subsection{Evaluating the MV Normal Density}
As one of the steps in the calculation of the marginal likelihood (see below) we will need to repeatedly evaluate the log of a multivariate normal density at a fixed set of parameter values.
Let $Z$ be a $p\times n$ matrix, each of whose columns is a point $\mathbf{z}$ at which we wish to evaluate 
$\log {\mathcal{N}_p\left(\mathbf{z}|\mu, \Sigma \right)}$
where $\mu$ is the mean vector and $\Sigma$ the covariance matrix of a multivariate normal.
Because our problem is parameterized in terms of the \emph{precision} matrix rather than the covariance matrix, the calculations given here assume that we are given $\Sigma^{-1}$ rather than $\Sigma$.
In terms of the precision matrix, the log of the MV normal density is given by
\begin{equation*}
  \log {\mathcal{N}_p\left(\mathbf{z}|\mu, \Sigma^{-1} \right)} = -\frac{p}{2}\log\left( 2\pi \right) + \frac{1}{2} \log \left| \Sigma^{-1} \right| - \frac{1}{2} \left(\mathbf{z} - \mu \right)' \Sigma^{-1} \left( \mathbf{z} - \mu \right)
\end{equation*}
Now let $R$ be the Cholesky factor of $\Sigma^{-1}$ so that $\Sigma^{-1} = R'R$ and define $\tilde{\mathbf{z}} = \mathbf{z} - \mu$ and $\mathbf{v} = R\tilde{\mathbf{z}}$.
Using these definitions,
\begin{equation*}
  \left( \mathbf{z} - \mu \right)' \Sigma^{-1} \left( \mathbf{z} - \mu \right) = (R\tilde{\mathbf{z}})'(R\tilde{\mathbf{z}}) = \mathbf{v}' \mathbf{v}
\end{equation*}
and, letting $R_{ii}$ denote the $i$th diagonal element of $R$,
\begin{eqnarray*}
  \frac{1}{2}\log |\Sigma^{-1}| &=&  \frac{1}{2}\log |R'R| = \frac{1}{2}\log\left(|R'| \cdot |R|\right) = \frac{1}{2}\left(\log |R'| + \log |R| \right)\\
  &=&\frac{1}{2}\left( 2 \log|R|\right)=  \sum_{i=1}^{p} \log R_{ii}
\end{eqnarray*} 
since $|A| = |A'|$, $|AB| = |A| \cdot |B|$ and the determinant of a triangular matrix equals the product of its diagonal elements.
Thus, we have
\begin{equation*}
  \log {\mathcal{N}_p\left(\mathbf{z}|\mu, \Sigma^{-1} \right)} = -\frac{p}{2}\log\left( 2\pi \right) + \mbox{trace}\left[\log \left(\mbox{diag}\left\{R  \right\}  \right)\right] - \frac{1}{2} \mathbf{v}'\mathbf{v}
\end{equation*}
The only term in the preceding expression that depends on $\mathbf{z}$ is $\mathbf{v}'\mathbf{v}$.
We can calculate this term simultaneously for all columns of $Z$ as follows.
First let $\widetilde{Z}$ denote the result subtracting of subtracting the vector $\mu$ from each column of $Z$, i.e.\ $\widetilde{Z} = Z - \mu \mathbf{1}_n'$.
To calculate $\mathbf{v}'\mathbf{v}$ for each column of $Z$ we simply square the elements of $R\widetilde{Z}$ and take the column sums of the resulting matrix.


\subsection{Efficient Calculation of $R_T$}
In the second step of each iteration we compute 
$\left( R_0^{-1} + \sum_{t=1}^{T} \hat{\boldsymbol{\varepsilon}}_t\hat{\boldsymbol{\varepsilon}}_t'\right)^{-1}$ 
where 
$\hat{\boldsymbol{\varepsilon}}_t=\mathbf{y}_t - X_t \boldsymbol{\gamma}$.
Since $R_0$ is simply the prior scale matrix for $\Omega^{-1}$ and hence remains unchanged during the iterations, we can pre-compute it and store the result before starting the sampler.
Since $X_t$ is a sparse matrix, there is a much more efficient and compact way to compute the sum of outer products of residuals.
Define:
\begin{equation*}
  \widetilde{Y} = \left[
  \begin{array}{c}
    \mathbf{y}_1'\\
    \vdots \\
    \mathbf{y}_T'
  \end{array}
\right], \quad \widetilde{X} = \left[
\begin{array}{c}
  \mathbf{x}_{1}' \\
  \vdots \\
  \mathbf{x}_{T}' 
\end{array}
\right], \quad \Gamma = \left[
\begin{array}{ccc}
  \boldsymbol{\gamma}_1 & \cdots & \boldsymbol{\gamma}_D
\end{array}
\right], \quad \hat{\boldsymbol{\varepsilon}} = \left[
\begin{array}{c}
  \hat{\boldsymbol{\varepsilon}}_1' \\
  \vdots \\
  \hat{\boldsymbol{\varepsilon}}_T'
\end{array}
\right]
\end{equation*}
so that $\hat{\boldsymbol{\varepsilon}} = \widetilde{Y} - \widetilde{X} \Gamma$.
Note that the vector of regression coefficients $\boldsymbol{\gamma}$ is the vec of the \emph{matrix} of regression coefficients $\Gamma$. 
Thus, expressed in terms of dense matrix operations
\begin{equation*}
  R_T^{-1} =  R_0^{-1} + \left( \widetilde{Y} - \widetilde{X} \Gamma \right)'\left( \widetilde{Y} - \widetilde{X} \Gamma \right)
\end{equation*}
The final step is to invert this sum (which is positive definite) to calculate $R_T$.
Note that the Matrix Inversion Lemma (Sherman-Morrison-Woodbury Formula) does \emph{not} simplify this calculation unless $D > T$.

\subsection{Efficient Calculation of $G_T$}
Because we parameterize our multivariate normal sampler in terms of the \emph{precision} matrix rather than the covariance matrix, we work with the \emph{inverse} of $G_T$, namely
\begin{equation*}
  G_T^{-1} =  G_0^{-1} + \sum_{t=1}^T X_t' \Omega^{-1} X_t 
\end{equation*}
Since it is simply the prior precision matrix for the vector $\boldsymbol{\gamma}$ of regression coefficients we can pre-compute $G_0^{-1}$ (assuming that we elicit a prior in terms of the covariance matrix).
Now, the sum over $X_t' \Omega X_t$ can in fact be simplified using the properties of the Kronecker product.\footnote{See, e.g., Horn and Johnson (1994) Chapter 4.2.}
Recall that $X_t = I_D \otimes \mathbf{x}_t'$.
Since $\left( A\otimes B \right)' = A' \otimes B'$, 
\begin{equation*}
  X_t' \Omega^{-1} X_t = \left( I_D \otimes \mathbf{x}_t \right) \Omega^{-1}X_t
\end{equation*}
Since $\Omega^{-1}X_t = \left( \Omega^{-1}X_t \right) \otimes 1$, $\Omega^{-1} = \Omega^{-1}\otimes 1$, and $(A \otimes B)(C \otimes D) = AC \otimes BD$, provided that everything is conformable, we have
\begin{eqnarray*}
  \left( I_D \otimes \mathbf{x}_t \right) \Omega^{-1}X_t &=&  \left( I_D \otimes \mathbf{x}_t \right) (\Omega^{-1}X_t \otimes 1)\\
  &=&  \Omega^{-1} X_t \otimes \mathbf{x}_t = \left[\Omega^{-1} (I_D \otimes \mathbf{x}_t') \right]\otimes \mathbf{x}_t \\
  &=& \left[(\Omega^{-1} \otimes 1)(I_D \otimes \mathbf{x}_t') \right]\otimes \mathbf{x}_t \\
  &=& \Omega^{-1} \otimes \mathbf{x}_t' \otimes \mathbf{x}_t
\end{eqnarray*}
Finally, since  $A \otimes (B + C) = A\otimes B + A \otimes C$,
\begin{equation*}
  \sum_{t=1}^{T} X_t' \Omega^{-1} X_t = \sum_{t=1}^{T} \Omega^{-1} \otimes \mathbf{x}_t' \otimes \mathbf{x}_t = \Omega^{-1} \otimes \left(\sum_{t=1}^T  \mathbf{x}_t' \otimes \mathbf{x}_t \right)
\end{equation*}
This is an extremely useful simplification: because $\sum_{t=1}^T  \mathbf{x}_t' \otimes \mathbf{x}_t$ involves neither $\Omega^{-1}$ nor $\boldsymbol{\gamma}$, only the data, we can pre-compute this quantity.
In fact, there is one final simplification that makes this quantity even simpler. 
By writing out the definition of the Kronecker Product, we see that $\mathbf{x}_t' \otimes \mathbf{x}_t = \mathbf{x}_t \mathbf{x}_t'$
and hence
\begin{equation*}
  \sum_{t=1}^{T} X_t' \Omega^{-1} X_t = \Omega^{-1} \otimes \left( \sum_{t=1}^T \mathbf{x}_t \mathbf{x}_t' \right) = \Omega^{-1} \otimes \widetilde{X}' \widetilde{X}
\end{equation*}
where $\widetilde{X}' = \left[
\begin{array}{ccc}
  \mathbf{x}_1 & \cdots & \mathbf{x}_T
\end{array}
\right]$.
Thus, we have $G_T^{-1} = G_{0}^{-1} + \Omega^{-1} \otimes \widetilde{X}'\widetilde{X}$.

\subsection{Efficient Calculation of $\bar{\gamma}$}
The vector $\bar{\boldsymbol{\gamma}}$ is constructed from several pieces.
The first is $G_0^{-1} \gamma_0$, the solution to the linear system $G_0
\mathbf{v} = \gamma_0$.
Since this piece depends only on the prior, we can pre-compute it.
The next piece is the sum $\sum_{t=1}^{T} X_t' \Omega^{-1} \mathbf{y}_t$.
We we noted above, $X_t$ is sparse so there is a more efficient way to compute this quantity.
Indeed, while this is far from obvious at first glance, it is possible to \emph{factor} $\Omega^{-1}$ outside of the sum using some clever matrix operations, allowing us to drastically reduce the computational complexity of the sampler.
To accomplish this simplification we combine the definition of $X_t$ as $I_D \otimes \mathbf{x}_t'$ with two properties of the Kronecker Product, namely:
\begin{equation*}
  \left( A \otimes B \right)\left( C \otimes D \right) =  AC \otimes BD\\
\end{equation*}
which holds provided that the respective matrices are conformable and
\begin{equation*}
  \mbox{vec}\left( AB \right) = \left( B' \otimes I_k \right) \mbox{vec}(A)
\end{equation*}
where $A$ is $k\times \ell$ and $B$ is $\ell \times m$.
Applying the first property twice in succession followed by the second property, we find that
\begin{eqnarray*}
  X_t' \Omega^{-1} \mathbf{y}_t &=&  \left( I_D \otimes \mathbf{x}_t' \right)' \Omega^{-1} \mathbf{y}_t  = \left( I_D \otimes \mathbf{x}_t \right)\Omega^{-1}\mathbf{y}_t \\
  &=& \left( I_D \otimes \mathbf{x}_t \right)\left( \Omega^{-1} \mathbf{y}_t \otimes 1 \right) = I_D \Omega^{-1} \mathbf{y}_t \otimes \mathbf{x}_t 1\\
  &=&  \Omega^{-1}\mathbf{y}_t \otimes \mathbf{x}_t = \left[\left( \Omega^{-1}\mathbf{y}_t \right)1\right] \otimes \left[I_{K+1} \mathbf{x}_t\right] \\
  &=& \left( \Omega^{-1} \mathbf{y}_t \otimes I_{K+1} \right)\left( 1 \otimes \mathbf{x}_t \right)\\
  &=& \left( \left[\Omega^{-1} \mathbf{y}_t\right] \otimes I_{K+1}\right) \mathbf{x}_t = \left( \left[\mathbf{y}_t'\Omega^{-1} \right]' \otimes I_{K+1}\right) \mbox{vec}(\mathbf{x}_t)\\
  &=& \mbox{vec}\left( \mathbf{x}_t \mathbf{y}_t' \Omega^{-1} \right)
\end{eqnarray*}
where we have used the fact that $\mbox{vec}\left( \mathbf{x}_t \right) = \mathbf{x}_t$.
Finally, since we can interchange the $\mbox{vec}$ summation operations,
\begin{eqnarray*}
  \sum_{t=1}^{T} \mbox{vec}\left( \mathbf{x}_t \mathbf{y}_t' \Omega^{-1} \right) &=&  \mbox{vec}\left[ \sum_{t=1}^{T} \mathbf{x}_t \mathbf{y}_t' \Omega^{-1} \right] =  \mbox{vec}\left[ \left(\sum_{t=1}^{T} \mathbf{x}_t \mathbf{y}_t' \right)\Omega^{-1} \right] \\
  &=& \mbox{vec}\left( \widetilde{X}' \widetilde{Y} \Omega^{-1} \right)
\end{eqnarray*}
where, as above, 
\begin{equation*}
  \widetilde{Y} = \left[
  \begin{array}{c}
    \mathbf{y}_1'\\
    \vdots \\
    \mathbf{y}_T'
  \end{array}
\right], \quad \widetilde{X} = \left[
\begin{array}{c}
  \mathbf{x}_{1}' \\
  \vdots \\
  \mathbf{x}_{T}' 
\end{array}
\right]
\end{equation*}
Thus we see that
\begin{equation*}
  \bar{\boldsymbol{\gamma}} = G_T \left[ G_0^{-1}\boldsymbol{\gamma}_0 +\mbox{vec}\left( \widetilde{X}' \widetilde{Y} \Omega^{-1} \right) \right]
\end{equation*}
Because it does not change between iterations, we can pre-compute the product $\widetilde{X}' \widetilde{Y}$.
The only term that remains to be addressed is $G_T$.
Because our normal sampler is parameterized in terms of the precision matrix rather than the covariance matrix we calculated $G_T^{-1}$ rather than $G_T$ above.
Rather than inverting it in this step, which is a very bad idea given its size, we notice that our expression for $\bar{\boldsymbol{\gamma}}$ takes the form $\mathbf{v}=A^{-1}\mathbf{b}$.
Therefore,
\begin{equation*}
\bar{\boldsymbol{\gamma}} = \mbox{solve}\left[ G_T^{-1}, \;  G_0^{-1}\boldsymbol{\gamma}_0 +\mbox{vec}\left( \widetilde{X}' \widetilde{Y} \Omega^{-1} \right)  \right]
\end{equation*}

\section{Calculating the Marginal likelihood}
We calculate the marginal likelihood using the method of Chib (1995).
Let $\theta$ denote the full collection of parameters.
By Bayes' Rule
\begin{equation*}
  \pi\left( \theta | Y_T \right) = \frac{\pi(\theta)f(Y_T|\theta)}{f(Y_T)}  
\end{equation*}
where $f(Y_T)$ is the marginal likelihood, aka the marginal data density, aka the evidence.
This identity holds true for \emph{any} value of $\theta$.
In particular it holds at the posterior mean $\theta^*$.
Solving for $f(Y_T)$ and evaluating the result at $\theta^*$, we have 
\begin{equation*}
  f\left(Y_T \right) = \frac{\pi(\theta^*)f(Y_T|\theta^*)}{\pi(\theta^*|Y_T)}  
\end{equation*}
Thus, we can express the \emph{log} marginal likelihood as
\begin{equation*}
  \log f(Y_T) = \log \pi(\theta^*) + \log f\left( Y_T|\theta^* \right) - \log \pi\left( \theta^*|Y_T \right)
\end{equation*}
Specializing this to the SUR model considered above,
\begin{equation*}
  \log f(Y_T) = \log \pi(\boldsymbol{\gamma}^*) + \log \pi\left( \Omega^{-1*} \right) + \log f\left( Y_T|\boldsymbol{\gamma}^*, \Omega^{-1*} \right) - \log \pi\left( \boldsymbol{\gamma}^*, \Omega^{-1*}|Y_T \right)
\end{equation*}
since our priors over $\boldsymbol{\gamma}$ and $\Omega^{-1}$ are independent.
The Chib (1995) method approximates $\log f(Y_T)$ by evaluating each of the terms on the right-hand-side of the preceding expression using the output of the Gibbs sampler.

\paragraph{The Contribution of the Prior}
Evaluating the first two terms, $\log \pi(\boldsymbol{\gamma}^*)$ and $\log \pi\left( \Omega^{-1*} \right)$, is easy: these are simply the priors for $\boldsymbol{\gamma}$ and $\Omega^{-1}$ evaluated at the posterior means.
We take the sample average of the Gibbs draws to approximate $\boldsymbol{\gamma}^*$ and $\Omega^{-1*}$ and evaluate the Normal and Wishart distributions at these points, with parameters given by the prior:
\begin{eqnarray*}
  \pi\left( \boldsymbol{\gamma}^* \right) &=& \mathcal{N}_p\left( \boldsymbol{\gamma}^* | \boldsymbol{\gamma}_0, G_0 \right)\\
  \pi\left( \Omega^{-1*} \right) &=& \mathcal{W}_D\left( \Omega^{-1*}|\rho_0, R_0 \right)
\end{eqnarray*}

\paragraph{The Contribution of the Likelihood}
Above we assumed a normal distribution for the regression errors, specifically, $\boldsymbol{\varepsilon}_t|\mathbf{x}_t \sim \mbox{ iid } \mathcal{N}_D(0,\Omega)$.
From the regression specification it follows that $\mathbf{y}_t \sim \mbox{ iid } \mathcal{N}_D\left(X_t \boldsymbol{\gamma}, \Omega\right)$ and thus the log likelihood evaluated at the posterior mean is
\begin{equation*}
  \log f(Y_T| \boldsymbol{\gamma}^*,\Omega^{-1*}) = \sum_{t=1}^{T} \log{ \mathcal{N}_D \left(\mathbf{y}_t|X_t \boldsymbol{\gamma}^*, \Omega^{-1*}\right)}
\end{equation*}
parameterized in terms of the precision matrix rather than the covariance matrix.
Equivalently, but more conveniently, we may write
\begin{equation*}
  \log f(Y_T| \boldsymbol{\gamma}^*,\Omega^{-1*}) = \sum_{t=1}^{T} \log{ \mathcal{N}_D \left(\mathbf{y}_t -X_t \boldsymbol{\gamma}^*|\mathbf{0}, \Omega^{-1*}\right)}
\end{equation*}
The advantage of this version of the likelihood is that the parameters of the normal density are constant over $t$, allowing us to exploit the efficient algorithm for repeatedly evaluating a MV normal density with fixed parameters, described above.
Note that we can simultaneously calculate all of the arguments for the normal density as follows:
\begin{equation*}
  (\widetilde{Y} - \widetilde{X} \Gamma^*)'= (\boldsymbol{\varepsilon}^{*})' = \left[
\begin{array}{ccc}
  \boldsymbol{\varepsilon}_1^* &
  \hdots &
  \boldsymbol{\varepsilon}_T^*
\end{array}\right]
\end{equation*}
where $\boldsymbol{\varepsilon}_t^* = \mathbf{y}_t - X_t \boldsymbol{\gamma}^*$ and $\Gamma^* = \left( \boldsymbol{\gamma}_1^*, \hdots,  \boldsymbol{\gamma}_D^*\right)$.

\paragraph{The Contribution of the Posterior}
To evaluate the third term, we factorize the joint posterior as the product of a conditional and marginal, namely:
\begin{equation*}
  \pi\left( \boldsymbol{\gamma}^*, \Omega^{-1*}|Y_T \right) = \pi\left( \boldsymbol{\gamma}^*|\Omega^{-1*},Y_T \right) \times \pi\left( \Omega^{-1*}|Y_T \right)
\end{equation*}
so that we have
\begin{equation*}
  \log \pi\left( \boldsymbol{\gamma}^*, \Omega^{-1*}|Y_T \right) = \log \pi\left( \boldsymbol{\gamma}^*|\Omega^{-1*},Y_T \right) + \log \pi\left( \Omega^{-1*}|Y_T \right)
\end{equation*}
Because we have \emph{analytical expressions} for the conditional posteriors in this model we can evaluate the first term in the product immediately.
We have $\boldsymbol{\gamma}|\Omega^{-1} \sim \mathcal{N}_p\left( \boldsymbol{\gamma}|\bar{\boldsymbol{\gamma}}, G_T \right)$ where $G_T$ and $\bar{\gamma}$ depend only on the prior, the data, and $\Omega^{-1}$.
To perform the required calculation, we simply evaluate the normal density at $\boldsymbol{\gamma}^*$ and evaluate $G_T$ and $\bar{\gamma}$ at $\Omega^{-1*}$, that is:
\begin{equation*}
  \pi\left( \boldsymbol{\gamma}^*|\Omega^{-1*}, Y_T \right) = \mathcal{N}_p\left( \boldsymbol{\gamma}^*|\bar{\gamma}^*, G_T^{-1*}\right)
\end{equation*}
where
\begin{eqnarray*}
  G_T^{-1*} &=& \left[ G_0^{-1} + \sum_{t=1}^T X_t' \Omega^{-1*} X_t \right] = \left[ G_0^{-1} + \Omega^{-1*} \otimes \widetilde{X}'\widetilde{X} \right]\\
  \bar{\boldsymbol{\gamma}}^* &=& G_T^* \left[ G_0^{-1}\boldsymbol{\gamma}_0 + \sum_{t=1}^{T} X_t'\Omega^{-1*}\mathbf{y}_t \right] = \mbox{solve}\left[G_T^{-1*},\;  G_0^{-1}\gamma_0 + \mbox{vec}\left( \widetilde{X}'\widetilde{Y}\Omega^{-1*} \right) \right]
\end{eqnarray*}
The evaluation of the second term in the product that gives the contribution of the posterior to the marginal likelihood is a bit more involved.
We write
\begin{eqnarray*}
  \pi\left( \Omega^{-1*}|Y_T \right) &=&  \int \pi\left( \boldsymbol{\gamma},\Omega^{-1*}|Y_T \right) \; d\boldsymbol{\gamma}\\
  &=& \int \pi\left( \Omega^{-1*}|\boldsymbol{\gamma}, Y_T \right)\pi\left( \boldsymbol{\gamma}|Y_T \right)\; d\boldsymbol{\gamma}
\end{eqnarray*}
and approximate the second integral using the draws from the Gibbs sampler:
\begin{eqnarray*}
  \pi\left( \Omega^{-1*}|Y_T \right) &\approx& \frac{1}{G}\sum_{g=1}^G \pi\left( \Omega^{-1*}|\boldsymbol{\gamma}^{(g)},Y_T \right)\\
  &=& \frac{1}{G}\sum_{g=1}^G \mathcal{W}_D\left(\Omega^{-1*}\left|\rho_0 + T, R_T^{\left( g \right)}\right. \right) 
\end{eqnarray*}
where
\begin{eqnarray*}
  R_T^{(g)} &=&  \left[ R_0^{-1} + \sum_{t=1}^{T} \left( \mathbf{y}_t - X_t \boldsymbol{\gamma}^{(g)} \right)\left( \mathbf{y}_t - X_t \boldsymbol{\gamma}^{(g)} \right)' \right]^{-1}\\
  &=& \left[  R_0^{-1} + \left( \widetilde{Y} - \widetilde{X} \Gamma^{(g)} \right)'\left( \widetilde{Y} - \widetilde{X} \Gamma^{(g)} \right)\right]^{-1}
\end{eqnarray*}



\section{Prediction}

Suppose we are interested in predicting the cross-section of returns $y_{n+1}
$ at time $(n+1)$. The Bayes prediction density of these returns,
conditioned on the data $Y_{n+1}$ and the factors $f_{n+1}$, is given by%
\begin{equation*}
p(y_{n+1}|Y_{n},f_{n+1})=\int_{\gamma ,\Omega ^{-1}}\mathcal{N}_{d}\left(
y_{n+1}|X_{n+1}\gamma ,\Omega \right) d\pi \left( \gamma ,\Omega
^{-1}|Y_{n}\right)
\end{equation*}%
which is estimated by the ergodic Monte Carlo average
\begin{equation*}
p(y_{n+1}|Y_{n},f_{n+1})=\frac{1}{G}\sum_{g=1}^{G}\mathcal{N}_{d}\left(
y_{n+1}|X_{n+1}\gamma ^{(g)},\Omega ^{(g)}\right)
\end{equation*}%
with the MCMC draws $\left\{ \gamma ^{(g)},\Omega ^{(g)}\right\} $ from the
posterior distribution.


\section{Gibbs Sample with Student-t Errors}

Suppose now that the errors follow a multivariate Student-t distribution rather than a normal distribution: 
\begin{equation*}
  \boldsymbol{\varepsilon}_{t}\sim t_{D,\nu }\left( 0,\Omega \right)
\end{equation*}
where $\nu$ denotes the degrees of freedom of the distribution, the location parameter is zero and the scale matrix is $\Omega$.
If $\nu >1$ then $E(\boldsymbol{\varepsilon}) = 0$. 
If $\nu>2$ then $Var\left( \boldsymbol{\varepsilon} \right) = \nu \sigma/(n-2)$.
Replacing the normal likelihood from above with the Student-t likelihood, however, breaks the conditional conjugacy that we exploited above to construct an MCMC algorithm based on the Gibbs sampler.
The solution to this problem is to work with a hierarchical representation in which the Student-t likelihood is introduced as a scale mixtrure of normal distributions, in particular
\begin{align*}
  \boldsymbol{\varepsilon}_{t}|\lambda _{t} &\sim N\left( 0,\lambda _{t}^{-1}\Omega \right)
\\
\lambda _{t} &\sim G\left( \frac{\nu }{2},\frac{\nu }{2}\right)
\end{align*}
where $G(\alpha,\beta)$ denotes the Gamma distribution with shape parameter $\alpha$ and rate parameter $\beta$.
(See below for more discussion on the parameterization of the gamma distribution.)
Using this representation, after conditioning on $\left( \nu, \boldsymbol{\lambda} \right)$, where $\boldsymbol{\lambda} = (\lambda_1, \dots \lambda_T)'$, we are essentially back in the familiar normal case from above.
In any inference that we carry out, as well as in the calculation of the marginal likelihood, we will marginalize over $\boldsymbol{\lambda}$ by simply ignoring these draws.
To begin, suppose that $\nu$ is fixed.
Later we will consider putting a prior over $\nu$ as well.

\subsection{Fixed $\nu$}
To begin, suppose for simplicity that the degrees of freedom parameter $\nu$ is fixed, i.e.\ specified in advance.
In this case, the full set of conditional posteriors is as follows:
\paragraph{Regression Coefficients:} 
$\boldsymbol{\gamma}|\Omega^{-1},Y_T \sim \mathcal{N}_p\left( \bar{\boldsymbol{\gamma}}_{\lambda},G_{T,\lambda} \right)$
\begin{eqnarray*}
  G_{T,\lambda} &=& \left[ G_0^{-1} + \sum_{t=1}^T \lambda_t X_t' \Omega^{-1} X_t \right]^{-1}\\
  \bar{\boldsymbol{\gamma}}_{\lambda} &=& G_{T,\lambda} \left[ G_0^{-1}\boldsymbol{\gamma}_0 + \sum_{t=1}^{T}\lambda_t X_t'\Omega^{-1}\mathbf{y}_t \right]
\end{eqnarray*}
\paragraph{Inverse Scale Matrix:}
$\Omega^{-1}|Y_T \sim \mathcal{W}_D\left(\rho_0 + T, R_{T,\lambda}\right)$
\begin{equation*}
  R_{T,\lambda} = \left[ R_0^{-1} + \sum_{t=1}^{T} \lambda_t\left( \mathbf{y}_t - X_t \boldsymbol{\gamma} \right)\left( \mathbf{y}_t - X_t \boldsymbol{\gamma} \right)' \right]^{-1}
\end{equation*}

\paragraph{Auxiliary Parameter:} $\lambda_t|\boldsymbol{\gamma}, \nu, Y_T \sim G\left(\displaystyle \frac{\nu + D}{2}, \frac{\nu + \boldsymbol{\varepsilon}_t' \boldsymbol{\varepsilon}_t}{2} \right)$, $\boldsymbol{\varepsilon}_t = \mathbf{y}_t - X_t \boldsymbol{\gamma}$

To implement the Gibbs sampler, we simply need to draw sequentially from these distributions, in the order given above. 
We will require, however, starting values for both $\Omega^{-1}$ and each of the $\lambda_t$ parameters. 
A reasonable starting value for $\lambda_t$ is one, which makes the initial draws for the regression coefficients and the inverse scale matrix the same as if were work withing with the normal model.

\section{Computational Details for Student-t Model}

\subsection{Parameterizing the Gamma Distribution}
The Gamma distribution is can be parameterized in two different ways. 
The parameterization upon which the algorithms described above are based uses $G(\alpha,\beta)$ to denote the density 
\begin{equation*}
  f(x|\alpha,\beta) = \frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}
\end{equation*}
where $\alpha$ is the shape parameter and $\beta$ is the rate parameter.
The base R function for drawing from this distribution is parameterized as follows:
\begin{equation*}
  \texttt{rgamma(n, shape, rate = 1, scale = 1/rate)}
\end{equation*}
so that we have a choice of specifying \emph{either} the rate parameter \emph{or} its reciprocal, which is called the \emph{scale parameter}.
If we let $s = 1/\beta$ denote the scale parameter, an alternative parameterization of the density is given by
\begin{equation*}
  f(x|\alpha,s)= \frac{1}{s^{\alpha} \Gamma(\alpha)} x^{\alpha-1} e^{-x/s}
\end{equation*}
The C function that underlies \texttt{rgamma} (see \texttt{Rmath.h}) is parameterized according to
\begin{equation*}
  \texttt{double rgamma(double a, double scl)}
\end{equation*}
so we must specify the \emph{scale} parameter if we want to call this from C++.

\subsection{Evaluating the Multivariate Student-t Density}
As one of the steps in the calculation of the marginal likelihood we need to repeatedly evaluate the log of a multivariate Student-t density at a fixed set of parameter values.
Let $Z$ be a $p\times n$ matrix, each of whose columns is a point $\mathbf{z}$ at which we wish to evaluate the log density.
The expression for the density itself is
\begin{equation*}
  t_p\left( \mathbf{z}|\nu,\mu, \Sigma \right) = \frac{\Gamma\left[ (\nu + p)/2 \right]}{|\Sigma|^{1/2}\left( \nu\pi \right)^{p/2}\Gamma\left( \nu/2 \right)}\left[ 1 + \frac{1}{\nu}\left( \mathbf{z} - \mu \right)' \Sigma^{-1} \left( \mathbf{z} - \mu \right)\right]^{-\left( \nu + p \right)/2}
\end{equation*}
where scalar parameter $\nu$ is the degrees of freedom of the distribution while the $p\times 1$ vector $\mu$ is the location parameter and the positive definite $p\times p$ matrix $\Sigma$ is the scale matrix.
If $\nu > 1$ then $E(\mathbf{z}) = \mu$. 
If $\nu >2$ then $Var(\mathbf{z}) = \nu\Sigma/(\nu-2)$.
For our problem, it makes sense to work in terms of the \emph{inverse} of the scale matrix, $\Sigma^{-1}$. 
Parameterized in this way, the log of the multivariate Student-t density is given by
  \begin{align*}
    \log {t_p\left(\mathbf{z}|\nu,\mu, \Sigma^{-1} \right)} &= \log{\Gamma}\left[ \left( \nu + p \right)/2 \right] - \log{\Gamma}\left( \nu/2 \right) - \frac{p}{2}\log{(\nu \pi)}\\
    &+ \frac{1}{2}\log{|\Sigma^{-1}|} - \frac{1}{2}(\nu + p) \log{ \left[ 1 + \frac{1}{\nu}\left(\mathbf{z} - \mu \right)' \Sigma^{-1} \left( \mathbf{z} - \mu \right)\right]}
\end{align*}
where we have used the fact that $|\Sigma|^{-1} = |\Sigma|^{-1}$ to write a positive $\frac{1}{2} \log |\Sigma^{-1}|$ term in place of a negative $\frac{1}{2}\log |\Sigma|$ term.
Now, let $R$ be the Cholesky factor of $\Sigma^{-1}$ so that $\Sigma^{-1} = R'R$ and define $\tilde{\mathbf{z}} = \mathbf{z} - \mu$ and $\mathbf{v} = R\tilde{\mathbf{z}}$.
Using these definitions,
\begin{equation*}
  \left( \mathbf{z} - \mu \right)' \Sigma^{-1} \left( \mathbf{z} - \mu \right) = (R\tilde{\mathbf{z}})'(R\tilde{\mathbf{z}}) = \mathbf{v}' \mathbf{v}
\end{equation*}
and, letting $R_{ii}$ denote the $i$th diagonal element of $R$,
\begin{eqnarray*}
  \frac{1}{2}\log |\Sigma^{-1}| &=&  \frac{1}{2}\log |R'R| = \frac{1}{2}\log\left(|R'| \cdot |R|\right) = \frac{1}{2}\left(\log |R'| + \log |R| \right)\\
  &=&\frac{1}{2}\left( 2 \log|R|\right)=  \sum_{i=1}^{p} \log R_{ii}
\end{eqnarray*} 
since $|A| = |A'|$, $|AB| = |A| \cdot |B|$ and the determinant of a triangular matrix equals the product of its diagonal elements.
Thus, we have
  \begin{align*}
    \log {t_p\left(\mathbf{z}|\nu,\mu, \Sigma^{-1} \right)} &= \log{\Gamma}\left[ \left( \nu + p \right)/2 \right] - \log{\Gamma}\left( \nu/2 \right) - \frac{p}{2}\log{(\nu \pi)}\\
    &+ \mbox{trace}\left[\log \left(\mbox{diag}\left\{R  \right\}  \right)\right]  - \frac{1}{2}(\nu + p) \log{ \left[ 1 + \frac{1}{\nu} \mathbf{v}'\mathbf{v}\right]}
\end{align*}
Note that, since $\mathbf{v}'\mathbf{v}/\nu$ could be very close to zero, it is best to implement $\texttt{log1p}(\mathbf{v}'\mathbf{v}/\nu)$.
An explanation of \texttt{log1p} is given 
\href{http://www.johndcook.com/blog/2010/06/07/math-library-functions-that-seem-unnecessary/}{here}.

Note that the only term in the log density expression that depends on $\mathbf{z}$ is $\mathbf{v}'\mathbf{v}$.
We can calculate this term simultaneously for \emph{all columns} of $Z$ as follows.
First let $\widetilde{Z}$ denote the result subtracting of subtracting the vector $\mu$ from each column of $Z$, i.e.\ $\widetilde{Z} = Z - \mu \mathbf{1}_n'$.
To calculate $\mathbf{v}'\mathbf{v}$ for each column of $Z$ we simply square the elements of $R\widetilde{Z}$ and take the column sums of the resulting matrix.

\subsection{Efficient Calculation of $R_{T,\lambda}$}
\subsection{Efficient Calculation of $G_{T,\lambda}$}
\subsection{Efficient Calculation of $\bar{\boldsymbol{\gamma}}$}
\subsection{Drawing $\lambda_t$}
We want to vectorize this step.

\section{A Prior over $\nu$}
Following Albert and Chib (1993), let us assume that the support of $\nu $
is the set of values $\left\{ \nu _{j}\right\} _{j=1}^{J}$, for example, $%
\left\{ 4,6,8,10,12,14,16\right\} $ and that a priori
\begin{equation*}
\Pr \left( \nu =\nu _{j}\right) =q_{j}
\end{equation*}%
Then, simple calculations show that
\begin{equation*}
\gamma |Y_{n},\Omega ^{-1},\nu ,\left\{ \lambda _{t}\right\} \sim \mathcal{N}%
_{d+p}\left( \hat{\gamma}_{\lambda },G_{n,\lambda }\right)
\end{equation*}%
where
\begin{align*}
\hat{\gamma}_{\lambda }& =G_{n,\lambda }\left( G_{0}^{-1}\gamma
_{0}+\sum_{t=1}^{n}\lambda _{t}X_{t}^{\prime }\Omega ^{-1}y_{t}\right)  \\
G_{n,\lambda }& =\left( G_{0}^{-1}+\sum_{t=1}^{n}\lambda _{t}X_{t}^{\prime
}\Omega ^{-1}X_{t}\right) ^{-1}
\end{align*}%
and
\begin{equation*}
\Omega ^{-1}|Y_{n},\gamma ,\nu ,\left\{ \lambda _{t}\right\} \sim \mathcal{W}%
_{d}\left( \rho _{0}+n,\left( R_{0}^{-1}+\sum_{t=1}^{n}\lambda _{t}\left(
y_{t}-X_{t}\gamma \right) \left( y_{t}-X_{t}\gamma \right) ^{\prime }\right)
^{-1}\right)
\end{equation*}%
Moreover,%
\begin{equation*}
\Pr \left( \nu =\nu _{j}|Y_{n},\gamma ,\Omega ^{-1}\right) \propto
q_{j}\prod\limits_{t=1}^{n}t_{d,\nu _{j}}\left( y_{t}|X_{t}\gamma ,\Omega
\right)
\end{equation*}%
and%
\begin{equation*}
\lambda _{t}|Y_{n},\gamma ,\nu \sim G\left( \frac{\nu +d}{2},\frac{\nu
+\left( y_{t}-X_{t}\gamma \right) ^{\prime }\left( y_{t}-X_{t}\gamma \right)
}{2}\right)
\end{equation*}%
One sweep of the MCMC\ sampling is completed by sampling these four
distributions in this order.

\section{Marginal Likelihood for Student-t Model}

The Chib (1995) method can again be applied to find the log marginal
likelihood as
\begin{equation*}
\log \Pr \left( \nu ^{\ast }\right) +\log \pi \left( \gamma ^{\ast }\right)
+\log \pi \left( \Omega ^{-1\ast }\right) +\sum_{t=1}^{n}\log t_{d,\nu
^{\ast }}\left( y_{t}|X_{t}\gamma ^{\ast },\Omega ^{\ast }\right) -\log \pi
\left( \nu ^{\ast },\gamma ^{\ast },\Omega ^{-1\ast }|Y_{n}\right)
\end{equation*}%
where $\nu ^{\ast }$ is the posterior mode (which is easily computed from
the sampled values), the last term is calculated as%
\begin{equation*}
\Pr \left( \nu ^{\ast }|Y_{n}\right) \times \pi \left( \Omega ^{-1\ast
}|Y_{n},\nu ^{\ast }\right) \times \pi \left( \gamma ^{\ast }|Y_{n},\Omega
^{-1\ast },\nu ^{\ast }\right)
\end{equation*}%
in which the first term is obtained from the posterior frequency
distribution of $\nu $, the second term is obtained from a reduced run in
which $\nu $ is fixed at $\nu ^{\ast }$ and the remaining three
distributions are sampled and the draws
\begin{equation*}
\left\{ \gamma ^{(g)},\lambda _{t}^{(g)}\right\} _{g=1}^{G}
\end{equation*}%
from this reduced MCMC run are used to calculate $\pi \left( \Omega ^{-1\ast
}|Y_{n},\nu ^{\ast }\right) $ as
\begin{equation*}
\frac{1}{G}\sum_{g=1}^{G}\mathcal{W}_{d}\left( \Omega ^{-1\ast }|\rho
_{0}+n,\left( R_{0}^{-1}+\sum_{t=1}^{n}\lambda _{t}^{(g)}\left(
y_{t}-X_{t}\gamma ^{(g)}\right) \left( y_{t}-X_{t}\gamma ^{(g)}\right)
^{\prime }\right) ^{-1}\right)
\end{equation*}%
and the final term $\pi \left( \gamma ^{\ast }|Y_{n},\Omega ^{-1\ast
}\right) $ is obtained from a second reduced run in which $\nu $ is fixed at
$\nu ^{\ast }$ and $\Omega ^{-1}$ is fixed at $\Omega ^{-1\ast }$ and the
draws
\begin{equation*}
\left\{ \lambda _{t}^{(g)}\right\}
\end{equation*}%
from this reduced run are used to give
\begin{equation*}
\pi (\gamma ^{\ast }|Y_{n},\Omega ^{-1\ast },\nu ^{\ast })=\frac{1}{G}%
\sum_{g=1}^{G}\mathcal{N}_{d+p}\left( \gamma ^{\ast }|\hat{\gamma}_{\lambda
^{(g)}}^{\ast },G_{n,\lambda ^{(g)}}^{\ast }\right)
\end{equation*}%
where%
\begin{align*}
\hat{\gamma}_{\lambda ^{(g)}}^{\ast }& =G_{n,\lambda ^{(g)}}^{\ast }\left(
G_{0}^{-1}\gamma _{0}+\sum_{t=1}^{n}\lambda _{t}^{(g)}X_{t}^{\prime }\Omega
^{-1\ast }y_{t}\right)  \\
G_{n,\lambda ^{(g)}}^{\ast }& =\left( G_{0}^{-1}+\sum_{t=1}^{n}\lambda
_{t}^{(g)}X_{t}^{\prime }\Omega ^{-1\ast }X_{t}\right) ^{-1}
\end{align*}

\end{document}
