\documentclass[12pt]{article}
\usepackage{enumerate}
\usepackage{amsmath, amssymb}
\linespread{1.2}



\begin{document}


\section{Model and Likelihood}

Consider a linear $K$-factor model for $D$ assets of the form
\begin{equation*}
  y_{it}=\alpha _{d} + \mathbf{f}_{t}' \boldsymbol{\beta}_d + \varepsilon_{it}
\end{equation*}
where $d = 1, \hdots, D$ and $t = 1, \hdots, T$ and $\mathbf{f}_{t}'=\left(f_{t1}, \hdots,f_{tK}\right)$ is a $K\times1$ vector.
This is a special case of the seemingly unrelated regression (SUR) model in which the regressors are \emph{identical} across equations.
Stacking observations for a given time period across assets, define $\mathbf{y}_t' = (y_{1t}, \hdots, y_{Dt})$ and analogously $\boldsymbol{\varepsilon}_t' = \left( \varepsilon_{t1}, \hdots \varepsilon_{tD} \right)$.
Now let $\mathbf{x}_t' = (1, \mathbf{f}_t')$ and $\boldsymbol{\gamma}_d' = (\alpha_d, \boldsymbol{\beta}_d')$ so we have
\begin{equation*}
  \mathbf{y}_t = X_t \boldsymbol{\gamma} + \boldsymbol{\varepsilon}_t
\end{equation*}
where $X_t = I_D \otimes \mathbf{x}_t'$ and $\boldsymbol{\gamma}'= \left( \boldsymbol{\gamma}_1', \hdots, \boldsymbol{\gamma}_D' \right)$. 
Now, suppose that 
\begin{equation*}
  \boldsymbol{\varepsilon}_t|\mathbf{x}_t \sim \mbox{ iid } \mathcal{N}_D(0, \Omega)
\end{equation*}
Let $Y_T$ denote the full data sample, i.e.\ $\left\{ \mathbf{y}_t, \mathbf{x}_t \right\}_{t=1}^T$.
Then the likelihood is 
\begin{equation*}
  \pi(Y_T|\boldsymbol{\gamma},\Omega^{-1}) \propto |\Omega^{-1}|^{T/2} \exp\left[ -\frac{1}{2} \sum_{t=1}^T \left(\mathbf{y}_t - X_t \boldsymbol{\gamma} \right)' \Omega^{-1} \left(\mathbf{y}_t - X_t \boldsymbol{\gamma} \right)\right]
\end{equation*}
where we parameterize this problem in terms of the $D\times D$ \emph{precision} matrix $\Omega^{-1}$ and the $p\times1$ vector of regression coefficients $\boldsymbol{\gamma}$, where $p = D(K+1)$.

\section{Prior and Posterior Distribution}
To complete the model we specify the following prior distribution
\begin{equation*}
  \pi (\boldsymbol{\gamma},\Omega^{-1})=\mathcal{N}_{p}\left( \boldsymbol{\gamma} |\boldsymbol{\gamma_{0}},G_{0}\right)
\mathcal{W}_{D}\left(\Omega ^{-1}|\rho_{0}, R_{0}\right)
\end{equation*}
This prior is conditionally conjugate with the normal likelihood.
In particular, we have 
$\boldsymbol{\gamma}|\Omega^{-1},Y_T \sim \mathcal{N}_p\left( \bar{\boldsymbol{\gamma}},G_T \right)$
where
\begin{eqnarray*}
  G_T &=& \left[ G_0^{-1} + \sum_{t=1}^T X_t' \Omega^{-1} X_t \right]^{-1}\\
  \bar{\boldsymbol{\gamma}} &=& G_T \left[ G_0^{-1}\boldsymbol{\gamma}_0 + \sum_{t=1}^{T} X_t'\Omega^{-1}\mathbf{y}_t \right]
\end{eqnarray*}
and
$\Omega^{-1}|Y_T \sim \mathcal{W}_D\left(\rho_0 + T, R_T \right)$
where
\begin{equation*}
  R_T = \left[ R_0^{-1} + \sum_{t=1}^{T} \left( \mathbf{y}_t - X_t \boldsymbol{\gamma} \right)\left( \mathbf{y}_t - X_t \boldsymbol{\gamma} \right)' \right]^{-1}
\end{equation*}

\section{MCMC}
Using the full set of conditional posteriors, given in the preceding section, we can simulate from the joint posterior for this model using a Gibbs sampler:
\begin{enumerate}
  \item Select a starting value $\Omega^{-1(0)}$ for the precision matrix.
  \item Draw $\boldsymbol{\gamma}^{(1)} \sim \mathcal{N}\left( \bar{\boldsymbol{\gamma}}^{(1)}, G_T^{(1)}\right)$ where
    \begin{eqnarray*}
      G_T^{(1)} &=& \left[ G_0^{-1} + \sum_{t=1}^T X_t' \Omega^{-1(0)} X_t \right]^{-1}\\
      \bar{\boldsymbol{\gamma}}^{(1)} &=& G_T^{(1)} \left[ G_0^{-1}\boldsymbol{\gamma}_0 + \sum_{t=1}^{T} X_t'\Omega^{-1(0)}\mathbf{y}_t \right]
    \end{eqnarray*}
  \item Draw $\Omega^{-1(1)} \sim \mathcal{W}_D\left(\rho_T, R_T^{(1)}\right)$ where
    \begin{equation*}
      R_T^{(1)} = \left[ R_0^{-1} + \sum_{t=1}^{T} \left( \mathbf{y}_t - X_t \boldsymbol{\gamma}^{(1)} \right)\left( \mathbf{y}_t - X_t \boldsymbol{\gamma}^{(1)} \right)' \right]^{-1}
    \end{equation*}
  \item Repeat the preceding two steps a total of $G$ times.
    In the $g$th iteration:
    \begin{enumerate}[(i)]
       \item Draw $\boldsymbol{\gamma}^{(g)} \sim \mathcal{N}\left( \bar{\boldsymbol{\gamma}}^{(g)}, G_T^{(g)}\right)$ where
    \begin{eqnarray*}
      G_T^{(g)} &=& \left[ G_0^{-1} + \sum_{t=1}^T X_t' \Omega^{-1(g-1)} X_t \right]^{-1}\\
      \bar{\boldsymbol{\gamma}}^{(g)} &=& G_T^{(g)} \left[ G_0^{-1}\boldsymbol{\gamma}_0 + \sum_{t=1}^{T} X_t'\Omega^{-1(g-1)}\mathbf{y}_t \right]
    \end{eqnarray*}
       \item Draw $\Omega^{-1(g)} \sim \mathcal{W}_D\left(\rho_T, R_T^{(g)}\right)$ where
    \begin{equation*}
      R_T^{(g)} = \left[ R_0^{-1} + \sum_{t=1}^{T} \left( \mathbf{y}_t - X_t \boldsymbol{\gamma}^{(g)} \right)\left( \mathbf{y}_t - X_t \boldsymbol{\gamma}^{(g)} \right)' \right]^{-1}
    \end{equation*}
    \end{enumerate}
  \item Discard the first $B$ draws.
\end{enumerate}
Note that in iteration $g$, $G_T^{(g)}$ and $\bar{\boldsymbol{\gamma}}^{(g)}$ are calculated using $\Omega^{-1(g-1)}$ while $R_T^{(g)}$ is calculated using $\boldsymbol{\gamma}^{(0)}$.
This is because we choose to initialize the sample with a starting value $\Omega^{-1(0)}$ for the precision matrix rather than for the vector of regression coefficients.

\section{Numerical Details}

\subsection{Evaluating the MV Normal Density}
As one of the steps in the calculation of the marginal likelihood (see below) we will need to repeatedly evaluate the log of a multivariate normal density at a fixed set of parameter values.
Let $Z$ be a $p\times n$ matrix, each of whose columns is a point $\mathbf{z}$ at which we wish to evaluate 
$\log {\mathcal{N}_p\left(\mathbf{z}|\mu, \Sigma \right)}$
where $\mu$ is the mean vector and $\Sigma$ the covariance matrix of a multivariate normal.
Because our problem is parameterizes in terms of the \emph{precision} matrix rather than the covariance matrix, the calculations given here assume that we are given $\Sigma^{-1}$ rather than $\Sigma$.
In terms of the precision matrix, the log of the MV normal density is given by
\begin{equation*}
  \log {\mathcal{N}_p\left(\mathbf{z}|\mu, \Sigma^{-1} \right)} = -\frac{p}{2}\log\left( 2\pi \right) + \frac{1}{2} \log \left| \Sigma^{-1} \right| - \frac{1}{2} \left(\mathbf{z} - \mu \right)' \Sigma^{-1} \left( \mathbf{z} - \mu \right)
\end{equation*}
Now let $R$ be the Cholesky factor of $\Sigma^{-1}$ so that $\Sigma^{-1} = R'R$ and define $\tilde{\mathbf{z}} = \mathbf{z} - \mu$ and $\mathbf{v} = R\tilde{\mathbf{z}}$.
Using these definitions,
\begin{equation*}
  \left( \mathbf{z} - \mu \right)' \Sigma^{-1} \left( \mathbf{z} - \mu \right) = (R\tilde{\mathbf{z}})'(R\tilde{\mathbf{z}}) = \mathbf{v}' \mathbf{v}
\end{equation*}
and, letting $R_{ii}$ denote the $i$th diagonal element of $R$,
\begin{eqnarray*}
  \log |\Sigma^{-1}| &=&  \log |R'R| = \log\left(|R'| \cdot |R|\right) = \log |R'| + \log |R| \\
  &=& 2 \log|R| = 2 \cdot \sum_{i=1}^{p} \log R_{ii}
\end{eqnarray*} 
since $|A| = |A'|$, $|AB| = |A| \cdot |B|$ and the determinant of a triangular matrix equals the product of its diagonal elements.
Thus, we have
\begin{equation*}
  \log {\mathcal{N}_p\left(\mathbf{z}|\mu, \Sigma^{-1} \right)} = -\frac{p}{2}\log\left( 2\pi \right) + \mbox{trace}\left[\log \left(\mbox{diag}\left\{R  \right\}  \right)\right] - \frac{1}{2} \mathbf{v}'\mathbf{v}
\end{equation*}
The only term in the preceding expression that depends on $\mathbf{z}$ is $\mathbf{v}'\mathbf{v}$.
We can calculate this term simultaneously for all columns of $Z$ as follows.
First let $\widetilde{Z}$ denote the result of subtracting of subtracting the vector $\mu$ from each column of $Z$, i.e.\ $\widetilde{Z} = Z - \mu \mathbf{1}_n'$.
To calculate $\mathbf{v}'\mathbf{v}$ for each column of $Z$ we simply square the elements of $R\widetilde{Z}$ and take the column sums of the resulting matrix.


\subsection{Efficient Calculation of $R_T$}
In the second step of each iteration we compute 
$\left( R_0^{-1} + \sum_{t=1}^{T} \hat{\boldsymbol{\varepsilon}}_t\hat{\boldsymbol{\varepsilon}}_t'\right)^{-1}$ 
where 
$\hat{\boldsymbol{\varepsilon}}_t=\mathbf{y}_t - X_t \boldsymbol{\gamma}$.
Since $R_0$ is simply the prior scale matrix for $\Omega^{-1}$ and hence remains unchanged during the iterations, we can pre-compute it and store the result before starting the sampler.
Since $X_t$ is a sparse matrix, there is a much more efficient and compact way to compute the sum of outer products of residuals.
Define:
\begin{equation*}
  \widetilde{Y} = \left[
  \begin{array}{c}
    \mathbf{y}_1'\\
    \vdots \\
    \mathbf{y}_T'
  \end{array}
\right], \quad \widetilde{X} = \left[
\begin{array}{c}
  \mathbf{x}_{1}' \\
  \vdots \\
  \mathbf{x}_{T}' 
\end{array}
\right], \quad \Gamma = \left[
\begin{array}{ccc}
  \boldsymbol{\gamma}_1 & \cdots & \boldsymbol{\gamma}_D
\end{array}
\right], \quad \hat{\boldsymbol{\varepsilon}} = \left[
\begin{array}{c}
  \hat{\boldsymbol{\varepsilon}}_1' \\
  \vdots \\
  \hat{\boldsymbol{\varepsilon}}_T'
\end{array}
\right]
\end{equation*}
so that $\hat{\boldsymbol{\varepsilon}} = \widetilde{Y} - \widetilde{X} \Gamma$.
Note that the vector of regression coefficients $\boldsymbol{\gamma}$ is the vec of the \emph{matrix} of regression coefficients $\Gamma$. 
Thus, expressed in terms of dense matrix operations
\begin{equation*}
  R_T^{-1} =  R_0^{-1} + \left( \widetilde{Y} - \widetilde{X} \Gamma \right)'\left( \widetilde{Y} - \widetilde{X} \Gamma \right)
\end{equation*}
The final step is to invert this sum (which is positive definite) to calculate $R_T$.
Note that the Matrix Inversion Lemma (Sherman-Morrison-Woodbury Formula) does \emph{not} simplify this calculation unless $D > T$.

\subsection{Efficient Calculation of $G_T$}
Because we parameterize our multivariate normal sampler in terms of the \emph{precision} matrix rather than the covariance matrix, we work with the \emph{inverse} of $G_T$, namely
\begin{equation*}
  G_T^{-1} =  G_0^{-1} + \sum_{t=1}^T X_t' \Omega^{-1} X_t 
\end{equation*}
Since it is simply the prior precision matrix for the vector $\boldsymbol{\gamma}$ of regression coefficients we can pre-compute $G_0$ (assuming that we elicit a prior in terms of the covariance matrix).
Now, the sum over $X_t' \Omega X_t$ can in fact be simplified using the properties of the Kronecker product.\footnote{See, e.g., Horn and Johnson (1994) Chapter 4.2.}
Recall that $X_t = I_D \otimes \mathbf{x}_t'$.
Since $\left( A\otimes B \right)' = A' \otimes B'$, 
\begin{equation*}
  X_t' \Omega^{-1} X_t = \left( I_D \otimes \mathbf{x}_t \right) \Omega^{-1}X_t
\end{equation*}
Since $\Omega^{-1}X_t = \left( \Omega^{-1}X_t \right) \otimes 1$, $\Omega^{-1} = \Omega^{-1}\otimes 1$, and $(A \otimes B)(C \otimes D) = AC \otimes BD$, provided that everything is conformable, we have
\begin{eqnarray*}
  \left( I_D \otimes \mathbf{x}_t \right) \Omega^{-1}X_t &=&  \left( I_D \otimes \mathbf{x}_t \right) (\Omega^{-1}X_t \otimes 1)\\
  &=&  \Omega^{-1} X_t \otimes \mathbf{x}_t = \left[\Omega^{-1} (I_D \otimes \mathbf{x}_t') \right]\otimes \mathbf{x}_t \\
  &=& \left[(\Omega^{-1} \otimes 1)(I_D \otimes \mathbf{x}_t') \right]\otimes \mathbf{x}_t \\
  &=& \Omega^{-1} \otimes \mathbf{x}_t' \otimes \mathbf{x}_t
\end{eqnarray*}
Finally, since  $A \otimes (B + C) = A\otimes B + A \otimes C$,
\begin{equation*}
  \sum_{t=1}^{T} X_t' \Omega^{-1} X_t = \sum_{t=1}^{T} \Omega^{-1} \otimes \mathbf{x}_t' \otimes \mathbf{x}_t = \Omega^{-1} \otimes \left(\sum_{t=1}^T  \mathbf{x}_t' \otimes \mathbf{x}_t \right)
\end{equation*}
This is an extremely useful simplification: because $\sum_{t=1}^T  \mathbf{x}_t' \otimes \mathbf{x}_t$ involves neither $\Omega^{-1}$ nor $\boldsymbol{\gamma}$, only the data, we can pre-compute this quantity.
In fact, there is one final simplification that makes this quantity even simpler. 
By writing out the definition of the Kronecker Product, we see that $\mathbf{x}_t' \otimes \mathbf{x}_t = \mathbf{x}_t \mathbf{x}_t'$
and hence
\begin{equation*}
  \sum_{t=1}^{T} X_t' \Omega^{-1} X_t = \Omega^{-1} \otimes \left( \sum_{t=1}^T \mathbf{x}_t \mathbf{x}_t' \right) = \Omega^{-1} \otimes \widetilde{X}' \widetilde{X}
\end{equation*}
where $\widetilde{X}' = \left[
\begin{array}{ccc}
  \mathbf{x}_1 & \cdots & \mathbf{x}_T
\end{array}
\right]$.
Thus, we have $G_T^{-1} = G_{0}^{-1} + \Omega^{-1} \otimes \widetilde{X}'\widetilde{X}$.

\subsection{Efficient Calculation of $\bar{\gamma}$}
The vector $\bar{\boldsymbol{\gamma}}$ is constructed from several pieces.
The first is $G_0^{-1} \gamma_0$, the solution to the linear system $G_0
\mathbf{v} = \gamma_0$.
Since this piece depends only on the prior, we can pre-compute it.
The next piece is the sum $\sum_{t=1}^{T} X_t' \Omega^{-1} \mathbf{y}_t$.
We we noted above, $X_t$ is sparse so there is a more efficient way to compute this quantity.
Indeed, while this is far from obvious at first glance, it is possible to \emph{factor} $\Omega^{-1}$ outside of the sum using some clever matrix operations, allowing us to drastically reduce the computational complexity of the sampler.
To accomplish this simplification we combine the definition of $X_t$ as $I_D \otimes \mathbf{x}_t'$ with two properties of the Kronecker Product, namely:
\begin{equation*}
  \left( A \otimes B \right)\left( C \otimes D \right) =  AC \otimes BD\\
\end{equation*}
which holds provided that the respective matrices are conformable and
\begin{equation*}
  \mbox{vec}\left( AB \right) = \left( B' \otimes I_k \right) \mbox{vec}(A)
\end{equation*}
where $A$ is $k\times \ell$ and $B$ is $\ell \times m$.
Applying the first property twice in succession followed by the second property, we find that
\begin{eqnarray*}
  X_t' \Omega^{-1} \mathbf{y}_t &=&  \left( I_D \otimes \mathbf{x}_t' \right)' \Omega^{-1} \mathbf{y}_t  = \left( I_D \otimes \mathbf{x}_t \right)\Omega^{-1}\mathbf{y}_t \\
  &=& \left( I_D \otimes \mathbf{x}_t \right)\left( \Omega^{-1} \mathbf{y}_t \otimes 1 \right) = I_D \Omega^{-1} \mathbf{y}_t \otimes \mathbf{x}_t 1\\
  &=&  \Omega^{-1}\mathbf{y}_t \otimes \mathbf{x}_t = \left[\left( \Omega^{-1}\mathbf{y}_t \right)1\right] \otimes \left[I_{K+1} \mathbf{x}_t\right] \\
  &=& \left( \Omega^{-1} \mathbf{y}_t \otimes I_{K+1} \right)\left( 1 \otimes \mathbf{x}_t \right)\\
  &=& \left( \left[\Omega^{-1} \mathbf{y}_t\right] \otimes I_{K+1}\right) \mathbf{x}_t = \left( \left[\mathbf{y}_t'\Omega^{-1} \right]' \otimes I_{K+1}\right) \mbox{vec}(\mathbf{x}_t)\\
  &=& \mbox{vec}\left( \mathbf{x}_t \mathbf{y}_t' \Omega^{-1} \right)
\end{eqnarray*}
where we have used the fact that $\mbox{vec}\left( \mathbf{x}_t \right) = \mathbf{x}_t$.
Finally, since we can interchange the $\mbox{vec}$ summation operations,
\begin{eqnarray*}
  \sum_{t=1}^{T} \mbox{vec}\left( \mathbf{x}_t \mathbf{y}_t' \Omega^{-1} \right) &=&  \mbox{vec}\left[ \sum_{t=1}^{T} \mathbf{x}_t \mathbf{y}_t' \Omega^{-1} \right] =  \mbox{vec}\left[ \left(\sum_{t=1}^{T} \mathbf{x}_t \mathbf{y}_t' \right)\Omega^{-1} \right] \\
  &=& \mbox{vec}\left( \widetilde{X}' \widetilde{Y} \Omega^{-1} \right)
\end{eqnarray*}
where, as above, 
\begin{equation*}
  \widetilde{Y} = \left[
  \begin{array}{c}
    \mathbf{y}_1'\\
    \vdots \\
    \mathbf{y}_T'
  \end{array}
\right], \quad \widetilde{X} = \left[
\begin{array}{c}
  \mathbf{x}_{1}' \\
  \vdots \\
  \mathbf{x}_{T}' 
\end{array}
\right]
\end{equation*}
Thus we see that
\begin{equation*}
  \bar{\boldsymbol{\gamma}} = G_T \left[ G_0^{-1}\boldsymbol{\gamma}_0 +\mbox{vec}\left( \widetilde{X}' \widetilde{Y} \Omega^{-1} \right) \right]
\end{equation*}
Because it does not change between iterations, we can pre-compute the product $\widetilde{X}' \widetilde{Y}$.
The only term that remains to be addressed is $G_T$.
Because our normal sampler is parameterized in terms of the precision matrix rather than the covariance matrix we calculated $G_T^{-1}$ rather than $G_T$ above.
Rather than inverting it in this step, which is a very bad idea given its size, we notice that our expression for $\bar{\boldsymbol{\gamma}}$ takes the form $\mathbf{v}=A^{-1}\mathbf{b}$.
Therefore,
\begin{equation*}
\bar{\boldsymbol{\gamma}} = \mbox{solve}\left[ G_T^{-1}, \;  G_0^{-1}\boldsymbol{\gamma}_0 +\mbox{vec}\left( \widetilde{X}' \widetilde{Y} \Omega^{-1} \right)  \right]
\end{equation*}

\section{Calculating the Marginal likelihood}
We calculate the marginal likelihood using the method of Chib (1995).
Let $\theta$ denote the full collection of parameters.
By Bayes' Rule
\begin{equation*}
  \pi\left( \theta | Y_T \right) = \frac{\pi(\theta)f(Y_T|\theta)}{f(Y_T)}  
\end{equation*}
where $f(Y_T)$ is the marginal likelihood, aka the marginal data density, aka the evidence.
This identity holds true for \emph{any} value of $\theta$.
In particular it holds at the posterior mean $\theta^*$.
Solving for $f(Y_T)$ and evaluating the result at $\theta^*$, we have 
\begin{equation*}
  f\left(Y_T \right) = \frac{\pi(\theta^*)f(Y_T|\theta^*)}{\pi(\theta^*|Y_T)}  
\end{equation*}
Thus, we can express the \emph{log} marginal likelihood as
\begin{equation*}
  \log f(Y_T) = \log \pi(\theta^*) + \log f\left( Y_T|\theta^* \right) - \pi\left( \theta^*|Y_T \right)
\end{equation*}
Specializing this to the SUR model considered above,
\begin{equation*}
  \log f(Y_T) = \log \pi(\boldsymbol{\gamma}^*) + \log \pi\left( \Omega^{-1*} \right) + \log f\left( Y_T|\boldsymbol{\gamma}^*, \Omega^{-1*} \right) - \pi\left( \boldsymbol{\gamma}^*, \Omega^{-1*}|Y_T \right)
\end{equation*}
since our priors over $\boldsymbol{\gamma}$ and $\Omega^{-1}$ are independent.
The Chib (1995) method approximates $\log f(Y_T)$ by evaluating each of the terms on the right-hand-side of the preceding expression using the output of the Gibbs sampler.

\paragraph{The Contribution of the Prior}
Evaluating the first two terms, $\log \pi(\boldsymbol{\gamma}^*)$ and $\log \pi\left( \Omega^{-1*} \right)$, is easy: these are simply the priors for $\boldsymbol{\gamma}$ and $\Omega^{-1}$ evaluated at the posterior means.
We take the sample average of the Gibbs draws to approximate $\boldsymbol{\gamma}^*$ and $\Omega^{-1*}$ and evaluate the Normal and Wishart distributions at these points, with parameters given by the prior:
\begin{eqnarray*}
  \pi\left( \boldsymbol{\gamma}^* \right) &=& \mathcal{N}_p\left( \boldsymbol{\gamma}^* | \boldsymbol{\gamma}_0, G_0 \right)\\
  \pi\left( \Omega^{-1*} \right) &=& \mathcal{W}_D\left( \Omega^{-1*}|\rho_0, R_0 \right)
\end{eqnarray*}

\paragraph{The Contribution of the Likelihood}
Above we assumed that $\boldsymbol{\varepsilon}_t|\mathbf{x}_t \sim \mbox{ iid } \mathcal{N}_D(0,\Omega)$


\paragraph{The Contribution of the Posterior}
To evaluate the third term, we factorize the joint posterior as the product of a conditional and marginal, namely:
\begin{equation*}
  \pi\left( \boldsymbol{\gamma}^*, \Omega^{-1*}|Y_T \right) = \pi\left( \boldsymbol{\gamma}^*|\Omega^{-1*},Y_T \right) \times \pi\left( \Omega^{-1*}|Y_T \right)
\end{equation*}
Because we have \emph{analytical expressions} for the conditional posteriors in this model we can evaluate the first term in the product immediately.
We have $\boldsymbol{\gamma}|\Omega^{-1} \sim \mathcal{N}_p\left( \boldsymbol{\gamma}|\bar{\boldsymbol{\gamma}}, G_T \right)$ where $G_T$ and $\bar{\gamma}$ depend only on the prior, the data, and $\Omega^{-1}$.
To perform the required calculation, we simply evaluate the normal density at $\boldsymbol{\gamma}^*$ and evaluate $G_T$ and $\bar{\gamma}$ at $\Omega^{-1*}$, that is:
\begin{equation*}
  \pi\left( \boldsymbol{\gamma}^*|\Omega^{-1*}, Y_T \right) = \mathcal{N}_p\left( \boldsymbol{\gamma}^*|\bar{\gamma}^*, G_T^*\right)
\end{equation*}
where
\begin{eqnarray*}
  G_T^* &=& \left[ G_0^{-1} + \sum_{t=1}^T X_t' \Omega^{-1*} X_t \right]^{-1} = \left[ G_0^{-1} + \Omega^{-1*} \otimes \widetilde{X}'\widetilde{X} \right]^{-1}\\
  \bar{\boldsymbol{\gamma}}^* &=& G_T^* \left[ G_0^{-1}\boldsymbol{\gamma}_0 + \sum_{t=1}^{T} X_t'\Omega^{-1*}\mathbf{y}_t \right] = G_T^* \left[ G_0^{-1}\gamma_0 + \mbox{vec}\left( \widetilde{X}'\widetilde{Y}\Omega^{-1*} \right) \right]
\end{eqnarray*}
The evaluation of the second term in the product that gives the contribution of the posterior to the marginal likelihood is a bit more involved.
We write
\begin{eqnarray*}
  \pi\left( \Omega^{-1*}|Y_T \right) &=&  \int \pi\left( \boldsymbol{\gamma},\Omega^{-1*}|Y_T \right) \; d\boldsymbol{\gamma}\\
  &=& \int \pi\left( \Omega^{-1*}|\boldsymbol{\gamma}, Y_T \right)\pi\left( \boldsymbol{\gamma}|Y_T \right)\; d\boldsymbol{\gamma}
\end{eqnarray*}
and approximate the second integral using the draws from the Gibbs sampler:
\begin{eqnarray*}
  \pi\left( \Omega^{-1*}|Y_T \right) &\approx& \frac{1}{G}\sum_{g=1}^G \pi\left( \Omega^{-1*}|\boldsymbol{\gamma}^{(g)},Y_T \right)\\
  &=& \frac{1}{G}\sum_{g=1}^G \mathcal{W}_D\left(\Omega^{-1*}\left|\rho_T, R_T^{\left( g \right)}\right. \right) 
\end{eqnarray*}
where
\begin{eqnarray*}
  R_T^{(g)} &=&  \left[ R_0^{-1} + \sum_{t=1}^{T} \left( \mathbf{y}_t - X_t \boldsymbol{\gamma}^{(g)} \right)\left( \mathbf{y}_t - X_t \boldsymbol{\gamma}^{(g)} \right)' \right]^{-1}\\
  &=& \left[  R_0^{-1} + \left( \widetilde{Y} - \widetilde{X} \Gamma^{(g)} \right)'\left( \widetilde{Y} - \widetilde{X} \Gamma^{(g)} \right)\right]^{-1}
\end{eqnarray*}



\section{Prediction}

Suppose we are interested in predicting the cross-section of returns $y_{n+1}
$ at time $(n+1)$. The Bayes prediction density of these returns,
conditioned on the data $Y_{n+1}$ and the factors $f_{n+1}$, is given by%
\begin{equation*}
p(y_{n+1}|Y_{n},f_{n+1})=\int_{\gamma ,\Omega ^{-1}}\mathcal{N}_{d}\left(
y_{n+1}|X_{n+1}\gamma ,\Omega \right) d\pi \left( \gamma ,\Omega
^{-1}|Y_{n}\right)
\end{equation*}%
which is estimated by the ergodic Monte Carlo average
\begin{equation*}
p(y_{n+1}|Y_{n},f_{n+1})=\frac{1}{G}\sum_{g=1}^{G}\mathcal{N}_{d}\left(
y_{n+1}|X_{n+1}\gamma ^{(g)},\Omega ^{(g)}\right)
\end{equation*}%
with the MCMC draws $\left\{ \gamma ^{(g)},\Omega ^{(g)}\right\} $ from the
posterior distribution.


\section{Student-t errors}

Suppose now that the errors are distributed as multivariate-t%
\begin{equation*}
\varepsilon _{t}\sim t_{d,\nu }\left( 0,\Omega \right)
\end{equation*}%
so that%
\begin{equation*}
E(\varepsilon _{t})=0\,,\;\nu >1
\end{equation*}%
\begin{equation*}
Var\left( \varepsilon _{t}\right) =\frac{\nu }{\nu -2}\Omega ,\;\nu >2
\end{equation*}%
The analysis of this model utilizes the hierarchical reprentation
\begin{align*}
\varepsilon _{t}|\lambda _{t} &\sim N\left( 0,\lambda _{t}^{-1}\Omega \right)
\\
\lambda _{t} &\sim G\left( \frac{\nu }{2},\frac{\nu }{2}\right)
\end{align*}%
which means that conditioned on $\left( \nu ,\left\{ \lambda _{t}\right\}
\right) $, the results presented for the Gaussian model can be applied with
minor modifications. The MCMC\ sampling is completed with the sampling of $%
\left( \nu ,\left\{ \lambda _{t}\right\} \right) $.

Following Albert and Chib (1993), let us assume that the support of $\nu $
is the set of values $\left\{ \nu _{j}\right\} _{j=1}^{J}$, for example, $%
\left\{ 4,6,8,10,12,14,16\right\} $ and that a priori
\begin{equation*}
\Pr \left( \nu =\nu _{j}\right) =q_{j}
\end{equation*}%
Then, simple calculations show that
\begin{equation*}
\gamma |Y_{n},\Omega ^{-1},\nu ,\left\{ \lambda _{t}\right\} \sim \mathcal{N}%
_{d+p}\left( \hat{\gamma}_{\lambda },G_{n,\lambda }\right)
\end{equation*}%
where
\begin{align*}
\hat{\gamma}_{\lambda }& =G_{n,\lambda }\left( G_{0}^{-1}\gamma
_{0}+\sum_{t=1}^{n}\lambda _{t}X_{t}^{\prime }\Omega ^{-1}y_{t}\right)  \\
G_{n,\lambda }& =\left( G_{0}^{-1}+\sum_{t=1}^{n}\lambda _{t}X_{t}^{\prime
}\Omega ^{-1}X_{t}\right) ^{-1}
\end{align*}%
and
\begin{equation*}
\Omega ^{-1}|Y_{n},\gamma ,\nu ,\left\{ \lambda _{t}\right\} \sim \mathcal{W}%
_{d}\left( \rho _{0}+n,\left( R_{0}^{-1}+\sum_{t=1}^{n}\lambda _{t}\left(
y_{t}-X_{t}\gamma \right) \left( y_{t}-X_{t}\gamma \right) ^{\prime }\right)
^{-1}\right)
\end{equation*}%
Moreover,%
\begin{equation*}
\Pr \left( \nu =\nu _{j}|Y_{n},\gamma ,\Omega ^{-1}\right) \propto
q_{j}\prod\limits_{t=1}^{n}t_{d,\nu _{j}}\left( y_{t}|X_{t}\gamma ,\Omega
\right)
\end{equation*}%
and%
\begin{equation*}
\lambda _{t}|Y_{n},\gamma ,\nu \sim G\left( \frac{\nu +d}{2},\frac{\nu
+\left( y_{t}-X_{t}\gamma \right) ^{\prime }\left( y_{t}-X_{t}\gamma \right)
}{2}\right)
\end{equation*}%
One sweep of the MCMC\ sampling is completed by sampling these four
distributions in this order.

\subsection{Marginal likelihood}

The Chib (1995) method can again be applied to find the log marginal
likelihood as
\begin{equation*}
\log \Pr \left( \nu ^{\ast }\right) +\log \pi \left( \gamma ^{\ast }\right)
+\log \pi \left( \Omega ^{-1\ast }\right) +\sum_{t=1}^{n}\log t_{d,\nu
^{\ast }}\left( y_{t}|X_{t}\gamma ^{\ast },\Omega ^{\ast }\right) -\log \pi
\left( \nu ^{\ast },\gamma ^{\ast },\Omega ^{-1\ast }|Y_{n}\right)
\end{equation*}%
where $\nu ^{\ast }$ is the posterior mode (which is easily computed from
the sampled values), the last term is calculated as%
\begin{equation*}
\Pr \left( \nu ^{\ast }|Y_{n}\right) \times \pi \left( \Omega ^{-1\ast
}|Y_{n},\nu ^{\ast }\right) \times \pi \left( \gamma ^{\ast }|Y_{n},\Omega
^{-1\ast },\nu ^{\ast }\right)
\end{equation*}%
in which the first term is obtained from the posterior frequency
distribution of $\nu $, the second term is obtained from a reduced run in
which $\nu $ is fixed at $\nu ^{\ast }$ and the remaining three
distributions are sampled and the draws
\begin{equation*}
\left\{ \gamma ^{(g)},\lambda _{t}^{(g)}\right\} _{g=1}^{G}
\end{equation*}%
from this reduced MCMC run are used to calculate $\pi \left( \Omega ^{-1\ast
}|Y_{n},\nu ^{\ast }\right) $ as
\begin{equation*}
\frac{1}{G}\sum_{g=1}^{G}\mathcal{W}_{d}\left( \Omega ^{-1\ast }|\rho
_{0}+n,\left( R_{0}^{-1}+\sum_{t=1}^{n}\lambda _{t}^{(g)}\left(
y_{t}-X_{t}\gamma ^{(g)}\right) \left( y_{t}-X_{t}\gamma ^{(g)}\right)
^{\prime }\right) ^{-1}\right)
\end{equation*}%
and the final term $\pi \left( \gamma ^{\ast }|Y_{n},\Omega ^{-1\ast
}\right) $ is obtained from a second reduced run in which $\nu $ is fixed at
$\nu ^{\ast }$ and $\Omega ^{-1}$ is fixed at $\Omega ^{-1\ast }$ and the
draws
\begin{equation*}
\left\{ \lambda _{t}^{(g)}\right\}
\end{equation*}%
from this reduced run are used to give
\begin{equation*}
\pi (\gamma ^{\ast }|Y_{n},\Omega ^{-1\ast },\nu ^{\ast })=\frac{1}{G}%
\sum_{g=1}^{G}\mathcal{N}_{d+p}\left( \gamma ^{\ast }|\hat{\gamma}_{\lambda
^{(g)}}^{\ast },G_{n,\lambda ^{(g)}}^{\ast }\right)
\end{equation*}%
where%
\begin{align*}
\hat{\gamma}_{\lambda ^{(g)}}^{\ast }& =G_{n,\lambda ^{(g)}}^{\ast }\left(
G_{0}^{-1}\gamma _{0}+\sum_{t=1}^{n}\lambda _{t}^{(g)}X_{t}^{\prime }\Omega
^{-1\ast }y_{t}\right)  \\
G_{n,\lambda ^{(g)}}^{\ast }& =\left( G_{0}^{-1}+\sum_{t=1}^{n}\lambda
_{t}^{(g)}X_{t}^{\prime }\Omega ^{-1\ast }X_{t}\right) ^{-1}
\end{align*}

\end{document}
