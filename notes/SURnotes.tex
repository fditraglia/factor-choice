\documentclass[12pt]{article}
\usepackage{enumerate, hyperref}
\usepackage{amsmath, amssymb}
\usepackage[margin=1.2in]{geometry}
\linespread{1.2}



\begin{document}


\section{Model and Likelihood}

Consider a linear $K$-factor model for $D$ assets of the form
\begin{equation*}
  y_{it}=\alpha _{d} + \mathbf{f}_{t}' \boldsymbol{\beta}_d + \varepsilon_{it}
\end{equation*}
where $d = 1, \hdots, D$ and $t = 1, \hdots, T$ and $\mathbf{f}_{t}'=\left(f_{t1}, \hdots,f_{tK}\right)$ is a $K\times1$ vector.
This is a special case of the seemingly unrelated regression (SUR) model in which the regressors are \emph{identical} across equations.
Stacking observations for a given time period across assets, define $\mathbf{y}_t' = (y_{1t}, \hdots, y_{Dt})$ and analogously $\boldsymbol{\varepsilon}_t' = \left( \varepsilon_{t1}, \hdots \varepsilon_{tD} \right)$.
Now let $\mathbf{x}_t' = (1, \mathbf{f}_t')$ and $\boldsymbol{\gamma}_d' = (\alpha_d, \boldsymbol{\beta}_d')$ so we have
\begin{equation*}
  \mathbf{y}_t = X_t \boldsymbol{\gamma} + \boldsymbol{\varepsilon}_t
\end{equation*}
where $X_t = I_D \otimes \mathbf{x}_t'$ and $\boldsymbol{\gamma}'= \left( \boldsymbol{\gamma}_1', \hdots, \boldsymbol{\gamma}_D' \right)$. 
Now, suppose that 
\begin{equation*}
  \boldsymbol{\varepsilon}_t|\mathbf{x}_t \sim \mbox{ iid } \mathcal{N}_D(0, \Omega)
\end{equation*}
Let $Y_T$ denote the full data sample, i.e.\ $\left\{ \mathbf{y}_t, \mathbf{x}_t \right\}_{t=1}^T$.
Then the likelihood is 
\begin{equation*}
  \pi(Y_T|\boldsymbol{\gamma},\Omega^{-1}) \propto |\Omega^{-1}|^{T/2} \exp\left[ -\frac{1}{2} \sum_{t=1}^T \left(\mathbf{y}_t - X_t \boldsymbol{\gamma} \right)' \Omega^{-1} \left(\mathbf{y}_t - X_t \boldsymbol{\gamma} \right)\right]
\end{equation*}
where we parameterize this problem in terms of the $D\times D$ \emph{precision} matrix $\Omega^{-1}$ and the $p\times1$ vector of regression coefficients $\boldsymbol{\gamma}$, where $p = D(K+1)$.

\section{Prior and Posterior Distribution}
To complete the model we specify the following prior distribution
\begin{equation*}
  \pi (\boldsymbol{\gamma},\Omega^{-1})=\mathcal{N}_{p}\left( \boldsymbol{\gamma} |\boldsymbol{\gamma_{0}},G_{0}\right)
\mathcal{W}_{D}\left(\Omega ^{-1}|\rho_{0}, R_{0}\right)
\end{equation*}
This prior is conditionally conjugate with the normal likelihood.
In particular, we have 
$\boldsymbol{\gamma}|\Omega^{-1},Y_T \sim \mathcal{N}_p\left( \bar{\boldsymbol{\gamma}},G_T \right)$
where
\begin{eqnarray*}
  G_T &=& \left[ G_0^{-1} + \sum_{t=1}^T X_t' \Omega^{-1} X_t \right]^{-1}\\
  \bar{\boldsymbol{\gamma}} &=& G_T \left[ G_0^{-1}\boldsymbol{\gamma}_0 + \sum_{t=1}^{T} X_t'\Omega^{-1}\mathbf{y}_t \right]
\end{eqnarray*}
and
$\Omega^{-1}|Y_T \sim \mathcal{W}_D\left(\rho_0 + T, R_T \right)$
where
\begin{equation*}
  R_T = \left[ R_0^{-1} + \sum_{t=1}^{T} \left( \mathbf{y}_t - X_t \boldsymbol{\gamma} \right)\left( \mathbf{y}_t - X_t \boldsymbol{\gamma} \right)' \right]^{-1}
\end{equation*}

\section{MCMC}
Using the full set of conditional posteriors, given in the preceding section, we can simulate from the joint posterior for this model using a Gibbs sampler:
\begin{enumerate}
  \item Select a starting value $\Omega^{-1(0)}$ for the precision matrix.
  \item Draw $\boldsymbol{\gamma}^{(1)} \sim \mathcal{N}\left( \bar{\boldsymbol{\gamma}}^{(1)}, G_T^{(1)}\right)$ where
    \begin{eqnarray*}
      G_T^{(1)} &=& \left[ G_0^{-1} + \sum_{t=1}^T X_t' \Omega^{-1(0)} X_t \right]^{-1}\\
      \bar{\boldsymbol{\gamma}}^{(1)} &=& G_T^{(1)} \left[ G_0^{-1}\boldsymbol{\gamma}_0 + \sum_{t=1}^{T} X_t'\Omega^{-1(0)}\mathbf{y}_t \right]
    \end{eqnarray*}
  \item Draw $\Omega^{-1(1)} \sim \mathcal{W}_D\left(\rho_T, R_T^{(1)}\right)$ where
    \begin{equation*}
      R_T^{(1)} = \left[ R_0^{-1} + \sum_{t=1}^{T} \left( \mathbf{y}_t - X_t \boldsymbol{\gamma}^{(1)} \right)\left( \mathbf{y}_t - X_t \boldsymbol{\gamma}^{(1)} \right)' \right]^{-1}
    \end{equation*}
  \item Repeat the preceding two steps a total of $G$ times.
    In the $g$th iteration:
    \begin{enumerate}[(i)]
       \item Draw $\boldsymbol{\gamma}^{(g)} \sim \mathcal{N}\left( \bar{\boldsymbol{\gamma}}^{(g)}, G_T^{(g)}\right)$ where
    \begin{eqnarray*}
      G_T^{(g)} &=& \left[ G_0^{-1} + \sum_{t=1}^T X_t' \Omega^{-1(g-1)} X_t \right]^{-1}\\
      \bar{\boldsymbol{\gamma}}^{(g)} &=& G_T^{(g)} \left[ G_0^{-1}\boldsymbol{\gamma}_0 + \sum_{t=1}^{T} X_t'\Omega^{-1(g-1)}\mathbf{y}_t \right]
    \end{eqnarray*}
       \item Draw $\Omega^{-1(g)} \sim \mathcal{W}_D\left(\rho_T, R_T^{(g)}\right)$ where
    \begin{equation*}
      R_T^{(g)} = \left[ R_0^{-1} + \sum_{t=1}^{T} \left( \mathbf{y}_t - X_t \boldsymbol{\gamma}^{(g)} \right)\left( \mathbf{y}_t - X_t \boldsymbol{\gamma}^{(g)} \right)' \right]^{-1}
    \end{equation*}
    \end{enumerate}
  \item Discard the first $B$ draws.
\end{enumerate}
Note that in iteration $g$, $G_T^{(g)}$ and $\bar{\boldsymbol{\gamma}}^{(g)}$ are calculated using $\Omega^{-1(g-1)}$ while $R_T^{(g)}$ is calculated using $\boldsymbol{\gamma}^{(0)}$.
This is because we choose to initialize the sample with a starting value $\Omega^{-1(0)}$ for the precision matrix rather than for the vector of regression coefficients.

\section{Computational Details}

\subsection{Evaluating the MV Normal Density}
As one of the steps in the calculation of the marginal likelihood (see below) we will need to repeatedly evaluate the log of a multivariate normal density at a fixed set of parameter values.
Let $Z$ be a $p\times n$ matrix, each of whose columns is a point $\mathbf{z}$ at which we wish to evaluate 
$\log {\mathcal{N}_p\left(\mathbf{z}|\mu, \Sigma \right)}$
where $\mu$ is the mean vector and $\Sigma$ the covariance matrix of a multivariate normal.
Because our problem is parameterized in terms of the \emph{precision} matrix rather than the covariance matrix, the calculations given here assume that we are given $\Sigma^{-1}$ rather than $\Sigma$.
In terms of the precision matrix, the log of the MV normal density is given by
\begin{equation*}
  \log {\mathcal{N}_p\left(\mathbf{z}|\mu, \Sigma^{-1} \right)} = -\frac{p}{2}\log\left( 2\pi \right) + \frac{1}{2} \log \left| \Sigma^{-1} \right| - \frac{1}{2} \left(\mathbf{z} - \mu \right)' \Sigma^{-1} \left( \mathbf{z} - \mu \right)
\end{equation*}
Now let $R$ be the Cholesky factor of $\Sigma^{-1}$ so that $\Sigma^{-1} = R'R$ and define $\tilde{\mathbf{z}} = \mathbf{z} - \mu$ and $\mathbf{v} = R\tilde{\mathbf{z}}$.
Using these definitions,
\begin{equation*}
  \left( \mathbf{z} - \mu \right)' \Sigma^{-1} \left( \mathbf{z} - \mu \right) = (R\tilde{\mathbf{z}})'(R\tilde{\mathbf{z}}) = \mathbf{v}' \mathbf{v}
\end{equation*}
and, letting $R_{ii}$ denote the $i$th diagonal element of $R$,
\begin{eqnarray*}
  \frac{1}{2}\log |\Sigma^{-1}| &=&  \frac{1}{2}\log |R'R| = \frac{1}{2}\log\left(|R'| \cdot |R|\right) = \frac{1}{2}\left(\log |R'| + \log |R| \right)\\
  &=&\frac{1}{2}\left( 2 \log|R|\right)=  \sum_{i=1}^{p} \log R_{ii}
\end{eqnarray*} 
since $|A| = |A'|$, $|AB| = |A| \cdot |B|$ and the determinant of a triangular matrix equals the product of its diagonal elements.
Thus, we have
\begin{equation*}
  \log {\mathcal{N}_p\left(\mathbf{z}|\mu, \Sigma^{-1} \right)} = -\frac{p}{2}\log\left( 2\pi \right) + \mbox{trace}\left[\log \left(\mbox{diag}\left\{R  \right\}  \right)\right] - \frac{1}{2} \mathbf{v}'\mathbf{v}
\end{equation*}
The only term in the preceding expression that depends on $\mathbf{z}$ is $\mathbf{v}'\mathbf{v}$.
We can calculate this term simultaneously for all columns of $Z$ as follows.
First let $\widetilde{Z}$ denote the result subtracting of subtracting the vector $\mu$ from each column of $Z$, i.e.\ $\widetilde{Z} = Z - \mu \mathbf{1}_n'$.
To calculate $\mathbf{v}'\mathbf{v}$ for each column of $Z$ we simply square the elements of $R\widetilde{Z}$ and take the column sums of the resulting matrix.


\subsection{Efficient Calculation of $R_T$}
In the second step of each iteration we compute 
$\left( R_0^{-1} + \sum_{t=1}^{T} \boldsymbol{\varepsilon}_t \boldsymbol{\varepsilon}_t'\right)^{-1}$ 
where 
$\boldsymbol{\varepsilon}_t=\mathbf{y}_t - X_t \boldsymbol{\gamma}$.
Since $R_0$ is simply the prior scale matrix for $\Omega^{-1}$ and hence remains unchanged during the iterations, we can pre-compute it and store the result before starting the sampler.
Since $X_t$ is a sparse matrix, there is a much more efficient and compact way to compute the sum of outer products of residuals.
Define:
\begin{equation*}
  \widetilde{Y} = \left[
  \begin{array}{c}
    \mathbf{y}_1'\\
    \vdots \\
    \mathbf{y}_T'
  \end{array}
\right], \quad \widetilde{X} = \left[
\begin{array}{c}
  \mathbf{x}_{1}' \\
  \vdots \\
  \mathbf{x}_{T}' 
\end{array}
\right], \quad \Gamma = \left[
\begin{array}{ccc}
  \boldsymbol{\gamma}_1 & \cdots & \boldsymbol{\gamma}_D
\end{array}
\right], \quad \boldsymbol{\varepsilon} = \left[
\begin{array}{c}
  \boldsymbol{\varepsilon}_1' \\
  \vdots \\
  \boldsymbol{\varepsilon}_T'
\end{array}
\right]
\end{equation*}
so that $\boldsymbol{\varepsilon} = \widetilde{Y} - \widetilde{X} \Gamma$.
Note that the vector of regression coefficients $\boldsymbol{\gamma}$ is the vec of the \emph{matrix} of regression coefficients $\Gamma$. 
Thus, expressed in terms of dense matrix operations
\begin{equation*}
  R_T^{-1} =  R_0^{-1} + \left( \widetilde{Y} - \widetilde{X} \Gamma \right)'\left( \widetilde{Y} - \widetilde{X} \Gamma \right)
\end{equation*}
The final step is to invert this sum (which is positive definite) to calculate $R_T$.
Note that the Matrix Inversion Lemma (Sherman-Morrison-Woodbury Formula) does \emph{not} simplify this calculation unless $D > T$.

\subsection{Efficient Calculation of $G_T$}
Because we parameterize our multivariate normal sampler in terms of the \emph{precision} matrix rather than the covariance matrix, we work with the \emph{inverse} of $G_T$, namely
\begin{equation*}
  G_T^{-1} =  G_0^{-1} + \sum_{t=1}^T X_t' \Omega^{-1} X_t 
\end{equation*}
Since it is simply the prior precision matrix for the vector $\boldsymbol{\gamma}$ of regression coefficients we can pre-compute $G_0^{-1}$ (assuming that we elicit a prior in terms of the covariance matrix).
Now, the sum over $X_t' \Omega X_t$ can in fact be simplified using the properties of the Kronecker product.\footnote{See, e.g., Horn and Johnson (1994) Chapter 4.2.}
Recall that $X_t = I_D \otimes \mathbf{x}_t'$.
Since $\left( A\otimes B \right)' = A' \otimes B'$, 
\begin{equation*}
  X_t' \Omega^{-1} X_t = \left( I_D \otimes \mathbf{x}_t \right) \Omega^{-1}X_t
\end{equation*}
Since $\Omega^{-1}X_t = \left( \Omega^{-1}X_t \right) \otimes 1$, $\Omega^{-1} = \Omega^{-1}\otimes 1$, and $(A \otimes B)(C \otimes D) = AC \otimes BD$, provided that everything is conformable, we have
\begin{eqnarray*}
  \left( I_D \otimes \mathbf{x}_t \right) \Omega^{-1}X_t &=&  \left( I_D \otimes \mathbf{x}_t \right) (\Omega^{-1}X_t \otimes 1)\\
  &=&  \Omega^{-1} X_t \otimes \mathbf{x}_t = \left[\Omega^{-1} (I_D \otimes \mathbf{x}_t') \right]\otimes \mathbf{x}_t \\
  &=& \left[(\Omega^{-1} \otimes 1)(I_D \otimes \mathbf{x}_t') \right]\otimes \mathbf{x}_t \\
  &=& \Omega^{-1} \otimes \mathbf{x}_t' \otimes \mathbf{x}_t
\end{eqnarray*}
Finally, since  $A \otimes (B + C) = A\otimes B + A \otimes C$,
\begin{equation*}
  \sum_{t=1}^{T} X_t' \Omega^{-1} X_t = \sum_{t=1}^{T} \Omega^{-1} \otimes \mathbf{x}_t' \otimes \mathbf{x}_t = \Omega^{-1} \otimes \left(\sum_{t=1}^T  \mathbf{x}_t' \otimes \mathbf{x}_t \right)
\end{equation*}
This is an extremely useful simplification: because $\sum_{t=1}^T  \mathbf{x}_t' \otimes \mathbf{x}_t$ involves neither $\Omega^{-1}$ nor $\boldsymbol{\gamma}$, only the data, we can pre-compute this quantity.
In fact, there is one final simplification that makes this quantity even simpler. 
By writing out the definition of the Kronecker Product, we see that $\mathbf{x}_t' \otimes \mathbf{x}_t = \mathbf{x}_t \mathbf{x}_t'$
and hence
\begin{equation*}
  \sum_{t=1}^{T} X_t' \Omega^{-1} X_t = \Omega^{-1} \otimes \left( \sum_{t=1}^T \mathbf{x}_t \mathbf{x}_t' \right) = \Omega^{-1} \otimes \widetilde{X}' \widetilde{X}
\end{equation*}
where $\widetilde{X}' = \left[
\begin{array}{ccc}
  \mathbf{x}_1 & \cdots & \mathbf{x}_T
\end{array}
\right]$.
Thus, we have $G_T^{-1} = G_{0}^{-1} + \Omega^{-1} \otimes \widetilde{X}'\widetilde{X}$.

\subsection{Efficient Calculation of $\bar{\gamma}$}
The vector $\bar{\boldsymbol{\gamma}}$ is constructed from several pieces.
The first is $G_0^{-1} \gamma_0$, the solution to the linear system $G_0
\mathbf{v} = \gamma_0$.
Since this piece depends only on the prior, we can pre-compute it.
The next piece is the sum $\sum_{t=1}^{T} X_t' \Omega^{-1} \mathbf{y}_t$.
We we noted above, $X_t$ is sparse so there is a more efficient way to compute this quantity.
Indeed, while this is far from obvious at first glance, it is possible to \emph{factor} $\Omega^{-1}$ outside of the sum using some clever matrix operations, allowing us to drastically reduce the computational complexity of the sampler.
To accomplish this simplification we combine the definition of $X_t$ as $I_D \otimes \mathbf{x}_t'$ with two properties of the Kronecker Product, namely:
\begin{equation*}
  \left( A \otimes B \right)\left( C \otimes D \right) =  AC \otimes BD\\
\end{equation*}
which holds provided that the respective matrices are conformable and
\begin{equation*}
  \mbox{vec}\left( AB \right) = \left( B' \otimes I_k \right) \mbox{vec}(A)
\end{equation*}
where $A$ is $k\times \ell$ and $B$ is $\ell \times m$.
Applying the first property twice in succession followed by the second property, we find that
\begin{eqnarray*}
  X_t' \Omega^{-1} \mathbf{y}_t &=&  \left( I_D \otimes \mathbf{x}_t' \right)' \Omega^{-1} \mathbf{y}_t  = \left( I_D \otimes \mathbf{x}_t \right)\Omega^{-1}\mathbf{y}_t \\
  &=& \left( I_D \otimes \mathbf{x}_t \right)\left( \Omega^{-1} \mathbf{y}_t \otimes 1 \right) = I_D \Omega^{-1} \mathbf{y}_t \otimes \mathbf{x}_t 1\\
  &=&  \Omega^{-1}\mathbf{y}_t \otimes \mathbf{x}_t = \left[\left( \Omega^{-1}\mathbf{y}_t \right)1\right] \otimes \left[I_{K+1} \mathbf{x}_t\right] \\
  &=& \left( \Omega^{-1} \mathbf{y}_t \otimes I_{K+1} \right)\left( 1 \otimes \mathbf{x}_t \right)\\
  &=& \left( \left[\Omega^{-1} \mathbf{y}_t\right] \otimes I_{K+1}\right) \mathbf{x}_t = \left( \left[\mathbf{y}_t'\Omega^{-1} \right]' \otimes I_{K+1}\right) \mbox{vec}(\mathbf{x}_t)\\
  &=& \mbox{vec}\left( \mathbf{x}_t \mathbf{y}_t' \Omega^{-1} \right)
\end{eqnarray*}
where we have used the fact that $\mbox{vec}\left( \mathbf{x}_t \right) = \mathbf{x}_t$.
Finally, since we can interchange the $\mbox{vec}$ summation operations,
\begin{eqnarray*}
  \sum_{t=1}^{T} \mbox{vec}\left( \mathbf{x}_t \mathbf{y}_t' \Omega^{-1} \right) &=&  \mbox{vec}\left[ \sum_{t=1}^{T} \mathbf{x}_t \mathbf{y}_t' \Omega^{-1} \right] =  \mbox{vec}\left[ \left(\sum_{t=1}^{T} \mathbf{x}_t \mathbf{y}_t' \right)\Omega^{-1} \right] \\
  &=& \mbox{vec}\left( \widetilde{X}' \widetilde{Y} \Omega^{-1} \right)
\end{eqnarray*}
where, as above, 
\begin{equation*}
  \widetilde{Y} = \left[
  \begin{array}{c}
    \mathbf{y}_1'\\
    \vdots \\
    \mathbf{y}_T'
  \end{array}
\right], \quad \widetilde{X} = \left[
\begin{array}{c}
  \mathbf{x}_{1}' \\
  \vdots \\
  \mathbf{x}_{T}' 
\end{array}
\right]
\end{equation*}
Thus we see that
\begin{equation*}
  \bar{\boldsymbol{\gamma}} = G_T \left[ G_0^{-1}\boldsymbol{\gamma}_0 +\mbox{vec}\left( \widetilde{X}' \widetilde{Y} \Omega^{-1} \right) \right]
\end{equation*}
Because it does not change between iterations, we can pre-compute the product $\widetilde{X}' \widetilde{Y}$.
The only term that remains to be addressed is $G_T$.
Because our normal sampler is parameterized in terms of the precision matrix rather than the covariance matrix we calculated $G_T^{-1}$ rather than $G_T$ above.
Rather than inverting it in this step, which is a very bad idea given its size, we notice that our expression for $\bar{\boldsymbol{\gamma}}$ takes the form $\mathbf{v}=A^{-1}\mathbf{b}$.
Therefore,
\begin{equation*}
\bar{\boldsymbol{\gamma}} = \mbox{solve}\left[ G_T^{-1}, \;  G_0^{-1}\boldsymbol{\gamma}_0 +\mbox{vec}\left( \widetilde{X}' \widetilde{Y} \Omega^{-1} \right)  \right]
\end{equation*}

\section{Calculating the Marginal likelihood}
We calculate the marginal likelihood using the method of Chib (1995).
Let $\theta$ denote the full collection of parameters.
By Bayes' Rule
\begin{equation*}
  \pi\left( \theta | Y_T \right) = \frac{\pi(\theta)f(Y_T|\theta)}{f(Y_T)}  
\end{equation*}
where $f(Y_T)$ is the marginal likelihood, aka the marginal data density, aka the evidence.
This identity holds true for \emph{any} value of $\theta$.
In particular it holds at the posterior mean $\theta^*$.
Solving for $f(Y_T)$ and evaluating the result at $\theta^*$, we have 
\begin{equation*}
  f\left(Y_T \right) = \frac{\pi(\theta^*)f(Y_T|\theta^*)}{\pi(\theta^*|Y_T)}  
\end{equation*}
Thus, we can express the \emph{log} marginal likelihood as
\begin{equation*}
  \log f(Y_T) = \log \pi(\theta^*) + \log f\left( Y_T|\theta^* \right) - \log \pi\left( \theta^*|Y_T \right)
\end{equation*}
Specializing this to the SUR model considered above,
\begin{equation*}
  \log f(Y_T) = \log \pi(\boldsymbol{\gamma}^*) + \log \pi\left( \Omega^{-1*} \right) + \log f\left( Y_T|\boldsymbol{\gamma}^*, \Omega^{-1*} \right) - \log \pi\left( \boldsymbol{\gamma}^*, \Omega^{-1*}|Y_T \right)
\end{equation*}
since our priors over $\boldsymbol{\gamma}$ and $\Omega^{-1}$ are independent.
The Chib (1995) method approximates $\log f(Y_T)$ by evaluating each of the terms on the right-hand-side of the preceding expression using the output of the Gibbs sampler.

\paragraph{The Contribution of the Prior}
Evaluating the first two terms, $\log \pi(\boldsymbol{\gamma}^*)$ and $\log \pi\left( \Omega^{-1*} \right)$, is easy: these are simply the priors for $\boldsymbol{\gamma}$ and $\Omega^{-1}$ evaluated at the posterior means.
We take the sample average of the Gibbs draws to approximate $\boldsymbol{\gamma}^*$ and $\Omega^{-1*}$ and evaluate the Normal and Wishart distributions at these points, with parameters given by the prior:
\begin{eqnarray*}
  \pi\left( \boldsymbol{\gamma}^* \right) &=& \mathcal{N}_p\left( \boldsymbol{\gamma}^* | \boldsymbol{\gamma}_0, G_0 \right)\\
  \pi\left( \Omega^{-1*} \right) &=& \mathcal{W}_D\left( \Omega^{-1*}|\rho_0, R_0 \right)
\end{eqnarray*}

\paragraph{The Contribution of the Likelihood}
Above we assumed a normal distribution for the regression errors, specifically, $\boldsymbol{\varepsilon}_t|\mathbf{x}_t \sim \mbox{ iid } \mathcal{N}_D(0,\Omega)$.
From the regression specification it follows that $\mathbf{y}_t \sim \mbox{ iid } \mathcal{N}_D\left(X_t \boldsymbol{\gamma}, \Omega\right)$ and thus the log likelihood evaluated at the posterior mean is
\begin{equation*}
  \log f(Y_T| \boldsymbol{\gamma}^*,\Omega^{-1*}) = \sum_{t=1}^{T} \log{ \mathcal{N}_D \left(\mathbf{y}_t|X_t \boldsymbol{\gamma}^*, \Omega^{-1*}\right)}
\end{equation*}
parameterized in terms of the precision matrix rather than the covariance matrix.
Equivalently, but more conveniently, we may write
\begin{equation*}
  \log f(Y_T| \boldsymbol{\gamma}^*,\Omega^{-1*}) = \sum_{t=1}^{T} \log{ \mathcal{N}_D \left(\mathbf{y}_t -X_t \boldsymbol{\gamma}^*|\mathbf{0}, \Omega^{-1*}\right)}
\end{equation*}
The advantage of this version of the likelihood is that the parameters of the normal density are constant over $t$, allowing us to exploit the efficient algorithm for repeatedly evaluating a MV normal density with fixed parameters, described above.
Note that we can simultaneously calculate all of the arguments for the normal density as follows:
\begin{equation*}
  (\widetilde{Y} - \widetilde{X} \Gamma^*)'= (\boldsymbol{\varepsilon}^{*})' = \left[
\begin{array}{ccc}
  \boldsymbol{\varepsilon}_1^* &
  \hdots &
  \boldsymbol{\varepsilon}_T^*
\end{array}\right]
\end{equation*}
where $\boldsymbol{\varepsilon}_t^* = \mathbf{y}_t - X_t \boldsymbol{\gamma}^*$ and $\Gamma^* = \left( \boldsymbol{\gamma}_1^*, \hdots,  \boldsymbol{\gamma}_D^*\right)$.

\paragraph{The Contribution of the Posterior}
To evaluate the third term, we factorize the joint posterior as the product of a conditional and marginal, namely:
\begin{equation*}
  \pi\left( \boldsymbol{\gamma}^*, \Omega^{-1*}|Y_T \right) = \pi\left( \boldsymbol{\gamma}^*|\Omega^{-1*},Y_T \right) \times \pi\left( \Omega^{-1*}|Y_T \right)
\end{equation*}
so that we have
\begin{equation*}
  \log \pi\left( \boldsymbol{\gamma}^*, \Omega^{-1*}|Y_T \right) = \log \pi\left( \boldsymbol{\gamma}^*|\Omega^{-1*},Y_T \right) + \log \pi\left( \Omega^{-1*}|Y_T \right)
\end{equation*}
Because we have \emph{analytical expressions} for the conditional posteriors in this model we can evaluate the first term in the product immediately.
We have $\boldsymbol{\gamma}|\Omega^{-1} \sim \mathcal{N}_p\left( \boldsymbol{\gamma}|\bar{\boldsymbol{\gamma}}, G_T \right)$ where $G_T$ and $\bar{\gamma}$ depend only on the prior, the data, and $\Omega^{-1}$.
To perform the required calculation, we simply evaluate the normal density at $\boldsymbol{\gamma}^*$ and evaluate $G_T$ and $\bar{\gamma}$ at $\Omega^{-1*}$, that is:
\begin{equation*}
  \pi\left( \boldsymbol{\gamma}^*|\Omega^{-1*}, Y_T \right) = \mathcal{N}_p\left( \boldsymbol{\gamma}^*|\bar{\gamma}^*, G_T^{-1*}\right)
\end{equation*}
where
\begin{eqnarray*}
  G_T^{-1*} &=& \left[ G_0^{-1} + \sum_{t=1}^T X_t' \Omega^{-1*} X_t \right] = \left[ G_0^{-1} + \Omega^{-1*} \otimes \widetilde{X}'\widetilde{X} \right]\\
  \bar{\boldsymbol{\gamma}}^* &=& G_T^* \left[ G_0^{-1}\boldsymbol{\gamma}_0 + \sum_{t=1}^{T} X_t'\Omega^{-1*}\mathbf{y}_t \right] = \mbox{solve}\left[G_T^{-1*},\;  G_0^{-1}\gamma_0 + \mbox{vec}\left( \widetilde{X}'\widetilde{Y}\Omega^{-1*} \right) \right]
\end{eqnarray*}
The evaluation of the second term in the product that gives the contribution of the posterior to the marginal likelihood is a bit more involved.
We write
\begin{eqnarray*}
  \pi\left( \Omega^{-1*}|Y_T \right) &=&  \int \pi\left( \boldsymbol{\gamma},\Omega^{-1*}|Y_T \right) \; d\boldsymbol{\gamma}\\
  &=& \int \pi\left( \Omega^{-1*}|\boldsymbol{\gamma}, Y_T \right)\pi\left( \boldsymbol{\gamma}|Y_T \right)\; d\boldsymbol{\gamma}
\end{eqnarray*}
and approximate the second integral using the draws from the Gibbs sampler:
\begin{eqnarray*}
  \pi\left( \Omega^{-1*}|Y_T \right) &\approx& \frac{1}{G}\sum_{g=1}^G \pi\left( \Omega^{-1*}|\boldsymbol{\gamma}^{(g)},Y_T \right)\\
  &=& \frac{1}{G}\sum_{g=1}^G \mathcal{W}_D\left(\Omega^{-1*}\left|\rho_0 + T, R_T^{\left( g \right)}\right. \right) 
\end{eqnarray*}
where
\begin{eqnarray*}
  R_T^{(g)} &=&  \left[ R_0^{-1} + \sum_{t=1}^{T} \left( \mathbf{y}_t - X_t \boldsymbol{\gamma}^{(g)} \right)\left( \mathbf{y}_t - X_t \boldsymbol{\gamma}^{(g)} \right)' \right]^{-1}\\
  &=& \left[  R_0^{-1} + \left( \widetilde{Y} - \widetilde{X} \Gamma^{(g)} \right)'\left( \widetilde{Y} - \widetilde{X} \Gamma^{(g)} \right)\right]^{-1}
\end{eqnarray*}



\section{Prediction}

Suppose we are interested in predicting the cross-section of returns $y_{n+1}
$ at time $(n+1)$. The Bayes prediction density of these returns,
conditioned on the data $Y_{n+1}$ and the factors $f_{n+1}$, is given by%
\begin{equation*}
p(y_{n+1}|Y_{n},f_{n+1})=\int_{\gamma ,\Omega ^{-1}}\mathcal{N}_{d}\left(
y_{n+1}|X_{n+1}\gamma ,\Omega \right) d\pi \left( \gamma ,\Omega
^{-1}|Y_{n}\right)
\end{equation*}%
which is estimated by the ergodic Monte Carlo average
\begin{equation*}
p(y_{n+1}|Y_{n},f_{n+1})=\frac{1}{G}\sum_{g=1}^{G}\mathcal{N}_{d}\left(
y_{n+1}|X_{n+1}\gamma ^{(g)},\Omega ^{(g)}\right)
\end{equation*}%
with the MCMC draws $\left\{ \gamma ^{(g)},\Omega ^{(g)}\right\} $ from the
posterior distribution.


\section{Gibbs Sampler with Student-t Errors}

\subsection{A Hierarchical Representation}
Suppose now that the errors follow a multivariate Student-t distribution rather than a normal distribution: 
\begin{equation*}
  \boldsymbol{\varepsilon}_{t}\sim t_{D,\nu }\left( 0,\Omega \right)
\end{equation*}
where $\nu$ denotes the degrees of freedom of the distribution, the location parameter is zero and the scale matrix is $\Omega$.
If $\nu >1$ then $E(\boldsymbol{\varepsilon}) = 0$. 
If $\nu>2$ then $Var\left( \boldsymbol{\varepsilon} \right) = \nu \sigma/(n-2)$.
Replacing the normal likelihood from above with the Student-t likelihood, however, breaks the conditional conjugacy that we exploited above to construct an MCMC algorithm based on the Gibbs sampler.
The solution to this problem is to work with a hierarchical representation in which the Student-t likelihood is introduced as a scale mixtrure of normal distributions, in particular
\begin{align*}
  \boldsymbol{\varepsilon}_{t}|\lambda _{t} &\sim N\left( 0,\lambda _{t}^{-1}\Omega \right)
\\
\lambda _{t} &\sim G\left( \frac{\nu }{2},\frac{\nu }{2}\right)
\end{align*}
where $G(\alpha,\beta)$ denotes the Gamma distribution with shape parameter $\alpha$ and rate parameter $\beta$.
(See below for more discussion on the parameterization of the gamma distribution.)
Using this representation, after conditioning on $\left( \nu, \boldsymbol{\lambda} \right)$, where $\boldsymbol{\lambda} = (\lambda_1, \dots \lambda_T)'$, we are essentially back in the familiar normal case from above.
In any inference that we carry out, as well as in the calculation of the marginal likelihood, we will marginalize over $\boldsymbol{\lambda}$ by simply ignoring these draws.
We will continue to place a normal prior on $\boldsymbol{\gamma}$ and a Wishart prior on $\Omega^{-1}$, exactly as we did when working with the normal likelihood above.

\subsection{The Sampler}
The sampler proceeds by fixing the degrees of freedom parameter $\nu$.
If $\nu$ is to be chosen from the data, this can be accomplished using the marginal likelihood, as described below.
Holding $\nu$ fixed, the full set of conditional posteriors is as follows:

\paragraph{Regression Coefficients:} 
$\boldsymbol{\gamma}|\Omega^{-1},Y_T \sim \mathcal{N}_p\left( \bar{\boldsymbol{\gamma}}_{\lambda},G_{T,\lambda} \right)$
\begin{eqnarray*}
  G_{T,\lambda} &=& \left[ G_0^{-1} + \sum_{t=1}^T \lambda_t X_t' \Omega^{-1} X_t \right]^{-1}\\
  \bar{\boldsymbol{\gamma}}_{\lambda} &=& G_{T,\lambda} \left[ G_0^{-1}\boldsymbol{\gamma}_0 + \sum_{t=1}^{T}\lambda_t X_t'\Omega^{-1}\mathbf{y}_t \right]
\end{eqnarray*}
\paragraph{Inverse Scale Matrix:}
$\Omega^{-1}|Y_T \sim \mathcal{W}_D\left(\rho_0 + T, R_{T,\lambda}\right)$
\begin{equation*}
  R_{T,\lambda} = \left[ R_0^{-1} + \sum_{t=1}^{T} \lambda_t\left( \mathbf{y}_t - X_t \boldsymbol{\gamma} \right)\left( \mathbf{y}_t - X_t \boldsymbol{\gamma} \right)' \right]^{-1}
\end{equation*}

\paragraph{Auxiliary Parameter:} $\lambda_t|\boldsymbol{\gamma}, \nu, Y_T \sim G\left(\displaystyle \frac{\nu + D}{2}, \frac{\nu + \boldsymbol{\varepsilon}_t' \boldsymbol{\varepsilon}_t}{2} \right)$, $\boldsymbol{\varepsilon}_t = \mathbf{y}_t - X_t \boldsymbol{\gamma}$

To implement the Gibbs sampler, we simply need to draw sequentially from these distributions, in the order given above. 
We will require, however, starting values for both $\Omega^{-1}$ and each of the $\lambda_t$ parameters. 
A reasonable starting value for $\lambda_t$ is one, which makes the initial draws for the regression coefficients and the inverse scale matrix the same as if were work withing with the normal model.

\section{Computational Details for Student-t Model}

\subsection{Parameterizing the Gamma Distribution}
The Gamma distribution is can be parameterized in two different ways. 
The parameterization upon which the algorithms described above are based uses $G(\alpha,\beta)$ to denote the density 
\begin{equation*}
  f(x|\alpha,\beta) = \frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}
\end{equation*}
where $\alpha$ is the shape parameter and $\beta$ is the rate parameter.
The base R function for drawing from this distribution is parameterized as follows:
\begin{equation*}
  \texttt{rgamma(n, shape, rate = 1, scale = 1/rate)}
\end{equation*}
so that we have a choice of specifying \emph{either} the rate parameter \emph{or} its reciprocal, which is called the \emph{scale parameter}.
If we let $s = 1/\beta$ denote the scale parameter, an alternative parameterization of the density is given by
\begin{equation*}
  f(x|\alpha,s)= \frac{1}{s^{\alpha} \Gamma(\alpha)} x^{\alpha-1} e^{-x/s}
\end{equation*}
The C function that underlies \texttt{rgamma} (see \texttt{Rmath.h}) is parameterized according to
\begin{equation*}
  \texttt{double rgamma(double a, double scl)}
\end{equation*}
so we must specify the \emph{scale} parameter if we want to call this from C++.

\subsection{Evaluating the Multivariate Student-t Density}
As one of the steps in the calculation of the marginal likelihood we need to repeatedly evaluate the log of a multivariate Student-t density at a fixed set of parameter values.
Let $Z$ be a $p\times n$ matrix, each of whose columns is a point $\mathbf{z}$ at which we wish to evaluate the log density.
The expression for the density itself is
\begin{equation*}
  t_p\left( \mathbf{z}|\nu,\mu, \Sigma \right) = \frac{\Gamma\left[ (\nu + p)/2 \right]}{|\Sigma|^{1/2}\left( \nu\pi \right)^{p/2}\Gamma\left( \nu/2 \right)}\left[ 1 + \frac{1}{\nu}\left( \mathbf{z} - \mu \right)' \Sigma^{-1} \left( \mathbf{z} - \mu \right)\right]^{-\left( \nu + p \right)/2}
\end{equation*}
where scalar parameter $\nu$ is the degrees of freedom of the distribution while the $p\times 1$ vector $\mu$ is the location parameter and the positive definite $p\times p$ matrix $\Sigma$ is the scale matrix.
If $\nu > 1$ then $E(\mathbf{z}) = \mu$. 
If $\nu >2$ then $Var(\mathbf{z}) = \nu\Sigma/(\nu-2)$.
For our problem, it makes sense to work in terms of the \emph{inverse} of the scale matrix, $\Sigma^{-1}$. 
Parameterized in this way, the log of the multivariate Student-t density is given by
  \begin{align*}
    \log {t_p\left(\mathbf{z}|\nu,\mu, \Sigma^{-1} \right)} &= \log{\Gamma}\left[ \left( \nu + p \right)/2 \right] - \log{\Gamma}\left( \nu/2 \right) - \frac{p}{2}\log{(\nu \pi)}\\
    &+ \frac{1}{2}\log{|\Sigma^{-1}|} - \frac{1}{2}(\nu + p) \log{ \left[ 1 + \frac{1}{\nu}\left(\mathbf{z} - \mu \right)' \Sigma^{-1} \left( \mathbf{z} - \mu \right)\right]}
\end{align*}
where we have used the fact that $|\Sigma|^{-1} = |\Sigma|^{-1}$ to write a positive $\frac{1}{2} \log |\Sigma^{-1}|$ term in place of a negative $\frac{1}{2}\log |\Sigma|$ term.
Now, let $R$ be the Cholesky factor of $\Sigma^{-1}$ so that $\Sigma^{-1} = R'R$ and define $\tilde{\mathbf{z}} = \mathbf{z} - \mu$ and $\mathbf{v} = R\tilde{\mathbf{z}}$.
Using these definitions,
\begin{equation*}
  \left( \mathbf{z} - \mu \right)' \Sigma^{-1} \left( \mathbf{z} - \mu \right) = (R\tilde{\mathbf{z}})'(R\tilde{\mathbf{z}}) = \mathbf{v}' \mathbf{v}
\end{equation*}
and, letting $R_{ii}$ denote the $i$th diagonal element of $R$,
\begin{eqnarray*}
  \frac{1}{2}\log |\Sigma^{-1}| &=&  \frac{1}{2}\log |R'R| = \frac{1}{2}\log\left(|R'| \cdot |R|\right) = \frac{1}{2}\left(\log |R'| + \log |R| \right)\\
  &=&\frac{1}{2}\left( 2 \log|R|\right)=  \sum_{i=1}^{p} \log R_{ii}
\end{eqnarray*} 
since $|A| = |A'|$, $|AB| = |A| \cdot |B|$ and the determinant of a triangular matrix equals the product of its diagonal elements.
Thus, we have
  \begin{align*}
    \log {t_p\left(\mathbf{z}|\nu,\mu, \Sigma^{-1} \right)} &= \log{\Gamma}\left[ \left( \nu + p \right)/2 \right] - \log{\Gamma}\left( \nu/2 \right) - \frac{p}{2}\log{(\nu \pi)}\\
    &+ \mbox{trace}\left[\log \left(\mbox{diag}\left\{R  \right\}  \right)\right]  - \frac{1}{2}(\nu + p) \log{ \left[ 1 + \frac{1}{\nu} \mathbf{v}'\mathbf{v}\right]}
\end{align*}
Note that, since $\mathbf{v}'\mathbf{v}/\nu$ could be very close to zero, it is best to implement $\texttt{log1p}(\mathbf{v}'\mathbf{v}/\nu)$.
An explanation of \texttt{log1p} is given 
\href{http://www.johndcook.com/blog/2010/06/07/math-library-functions-that-seem-unnecessary/}{here}.

Note that the only term in the log density expression that depends on $\mathbf{z}$ is $\mathbf{v}'\mathbf{v}$.
We can calculate this term simultaneously for \emph{all columns} of $Z$ as follows.
First let $\widetilde{Z}$ denote the result subtracting of subtracting the vector $\mu$ from each column of $Z$, i.e.\ $\widetilde{Z} = Z - \mu \mathbf{1}_n'$.
To calculate $\mathbf{v}'\mathbf{v}$ for each column of $Z$ we simply square the elements of $R\widetilde{Z}$ and take the column sums of the resulting matrix.

\subsection{Efficient Calculation of $R_{T,\lambda}$}
In the normal model from above we described the efficient calculation of
\begin{equation*}
  R_T = \left[ R_0^{-1} + \sum_{t=1}^T \boldsymbol{\varepsilon}_t \boldsymbol{\varepsilon}_t' \right]^{-1}
\end{equation*}
where $\boldsymbol{\varepsilon}_t = \mathbf{y}_t - X_t \boldsymbol{\gamma}$.
In the Student-t model we instead need to calculate
\begin{equation*}
  R_{T,\lambda} = \left[ R_0^{-1} + \sum_{t=1}^T \lambda_t \boldsymbol{\varepsilon}_t \boldsymbol{\varepsilon}_t' \right]^{-1}
\end{equation*}
which differs only in that each term of the sum is multiplied by the scalar $\lambda_t$.
Recall that we defined
\begin{equation*}
  \widetilde{Y} = \left[
  \begin{array}{c}
    \mathbf{y}_1'\\
    \vdots \\
    \mathbf{y}_T'
  \end{array}
\right], \quad \widetilde{X} = \left[
\begin{array}{c}
  \mathbf{x}_{1}' \\
  \vdots \\
  \mathbf{x}_{T}' 
\end{array}
\right], \quad \Gamma = \left[
\begin{array}{ccc}
  \boldsymbol{\gamma}_1 & \cdots & \boldsymbol{\gamma}_D
\end{array}
\right], \quad \boldsymbol{\varepsilon} = \left[
\begin{array}{c}
  \boldsymbol{\varepsilon}_1' \\
  \vdots \\
  \boldsymbol{\varepsilon}_T'
\end{array}
\right]
\end{equation*}
so that $\boldsymbol{\varepsilon} = \widetilde{Y} - \widetilde{X} \Gamma$ allowing us to express the sum in the expression for $R_T$ more compactly in matrix notation as $\left( \widetilde{Y} - \widetilde{X}\Gamma \right)'\left( \widetilde{Y} - \widetilde{X}\Gamma \right)$.
To calculate the sum in the expression for $R_{T,\lambda}$, however, we need to take into account $\lambda_t$.
To this end, define $\boldsymbol{\lambda} = \left( \lambda_1, \dots, \lambda_T \right)'$ and $\Lambda = \mbox{diag}(\boldsymbol{\lambda})$.
Then we see that
\begin{equation*}
  R_{T,\lambda} = \left[ R_0^{-1} +  \left( \widetilde{Y} - \widetilde{X}\Gamma \right)' \Lambda \left( \widetilde{Y} - \widetilde{X}\Gamma \right)\right]^{-1}
\end{equation*}
Everything else is the same as above, for example can pre-compute $R_{0}^{-1}$ and there is no gain from the Sherman-Morris-Woodbury formula unless $D>T$.

\subsection{Efficient Calculation of $G_{T,\lambda}$}
In the case of the normal model, described above, we used clever matrix algebra to pre-compute quantities involved in the expression\footnote{Since the MV normal sample we have implement is in terms of the precision matrix rather than covariance matrix we work with $G_T^{-1}$ rather than its inverse $G_T$.}
\begin{equation*}
  G_T^{-1} = G_0^{-1} + \sum_{t=1}^{T} X_t' \Omega^{-1}X_t 
\end{equation*}
We won't end up with such nice results as in the normal case here, but we can still express the calculations in a more convenient form by exploiting some of the same properties of the Kronecker product.
Recall that
\begin{equation*}
  X_t'\Omega^{-1}X_t = \Omega^{-1} \otimes (\mathbf{x}_t' \otimes \mathbf{x}_t') = \Omega^{-1} \otimes (\mathbf{x}_t \mathbf{x}_t')
\end{equation*}
as we showed above in the discussion for the normal model.
One further property of the Kronecker product that we did not use in our earlier calculations is the following:
\begin{equation*}
  (k A) \otimes B = A \otimes (kB) = k (A \otimes B)
\end{equation*}
if $k$ is a scalar and $A,B$ are matrices.
Thus, since $\lambda_t$ is a scalar, 
\begin{equation*}
  \lambda_t X_t'\Omega^{-1}X_t =\lambda_t\left[ \Omega^{-1} \otimes (\mathbf{x}_t \mathbf{x}_t')\right] = \Omega^{-1} \otimes \left( \lambda_t \mathbf{x}_t \mathbf{x}_t'\right)
\end{equation*}
and thus, since the Kronecker product is bilinear, 
\begin{equation*}
  \sum_{t=1}^{T}\lambda_t X_t' \Omega^{-1} X_t = \Omega^{-1}\otimes \left(\sum_{t=1}^T  \lambda_t \mathbf{x}_t \mathbf{x}_t'\right) = \Omega^{-1} \otimes \left( \widetilde{X}' \Lambda \widetilde{X} \right)
\end{equation*}
where $\widetilde{X}$ and $\Lambda$ are defined in the preceding subsection on calculating $R_{T,\lambda}$.
Thus, we have
\begin{equation*}
  G_{T,\lambda}^{-1} = G_0^{-1} + \Omega^{-1} \otimes \left( \widetilde{X}' \Lambda \widetilde{X} \right)
\end{equation*}
Unlike the case of the normal model, we cannot pre-compute the second term in the Kronecker product since it depends on $\Lambda$, which is updated in each iteration of the sampler.
We could, however, pre-compute the matrices $\mathbf{x}_t \mathbf{x}_t'$ and then take the $\lambda$-weighted sum in each step of the sampler.
It is unclear whether this efficiency gain is worth the trouble, but we could try it if things are running too slowly.

\subsection{Efficient Calculation of $\bar{\boldsymbol{\gamma}}_{\lambda}$}
Recall from our calculations for the normal model given above that, using the properties of the Kronecker product and the vec operator,
\begin{equation*}
  X_t' \Omega^{-1} \mathbf{y}_t = \mbox{vec}(\mathbf{x}_t \mathbf{y}_t' \Omega^{-1})
\end{equation*}
Replacing $\Omega^{-1}$ with $\lambda_t \Omega^{-1}$ and using the fact that $\lambda_t$ is a scalar, and hence commutes, along with the fact that  $\mbox{vec}(kA)=k \mbox{vec}(A)$ for any scalar $k$ and matrix $A$, it follows that
\begin{equation*}
  \lambda_t X_t' \Omega^{-1} \mathbf{y}_t = \mbox{vec}( \lambda_t\mathbf{x}_t \mathbf{y}_t' \Omega^{-1}) = \lambda_t\mbox{vec}(\mathbf{x}_t \mathbf{y}_t' \Omega^{-1})
\end{equation*}
From here it follows that
\begin{equation*}
  \sum_{t=1}^T  \lambda_t X_t' \Omega^{-1} \mathbf{y}_t = \sum_{t=1}^T\mbox{vec}(\lambda_t \mathbf{x}_t \mathbf{y}_t' \Omega^{-1}) =\mbox{vec}\left[ \left(\sum_{t=1}^T \lambda_t\mathbf{x}_t \mathbf{y}_t' \right)\Omega^{-1}\right] = \mbox{vec}\left( \widetilde{X}
  ' \Lambda \widetilde{Y} \Omega^{-1}\right)
\end{equation*}
where $\widetilde{X}, \widetilde{Y}$ and $\Lambda$ are as defined in the subsection on efficient calculation of $R_{T,\lambda}$ from above.
The rest of the calculation is essentially the same as that for $\bar{\boldsymbol{\gamma}}$ in the case of the normal model, described above.


\subsection{Drawing $\lambda_t$}
This step in the sampler has no counterpart in the normal model:
\begin{equation*}
\lambda_t|\boldsymbol{\gamma}, \nu, Y_T \sim G\left(\displaystyle \frac{\nu + D}{2}, \frac{\nu + \boldsymbol{\varepsilon}_t' \boldsymbol{\varepsilon}_t}{2} \right)
\end{equation*}
where $\boldsymbol{\varepsilon}_t = \mathbf{y}_t - X_t \boldsymbol{\gamma}$.
Notice that we have one auxiliary parameter, $\lambda_t$, per observation.
Each of these comes from a \emph{different} Gamma distribution: while the shape parameter is common, the rate parameter differs according to the residual $\boldsymbol{\varepsilon}$ associated with each observation.
Because we have a C++ function for drawing gamma variates that is vectorized with respect to its second argument, the rate parameter, we need an expression for the while \emph{vector} of rate parameters.
I don't think there's any straightforward way to write this out using matrix operations, so we'll just have to use a loop to construct the desired vector.

%\section{A Prior over $\nu$}
%Following Albert and Chib (1993), let us assume that the support of $\nu $
%is the set of values $\left\{ \nu _{j}\right\} _{j=1}^{J}$, for example, $%
%\left\{ 4,6,8,10,12,14,16\right\} $ and that a priori
%\begin{equation*}
%\Pr \left( \nu =\nu _{j}\right) =q_{j}
%\end{equation*}%
%Then, simple calculations show that
%\begin{equation*}
%\gamma |Y_{n},\Omega ^{-1},\nu ,\left\{ \lambda _{t}\right\} \sim \mathcal{N}%
%_{d+p}\left( \hat{\gamma}_{\lambda },G_{n,\lambda }\right)
%\end{equation*}%
%where
%\begin{align*}
%\hat{\gamma}_{\lambda }& =G_{n,\lambda }\left( G_{0}^{-1}\gamma
%_{0}+\sum_{t=1}^{n}\lambda _{t}X_{t}^{\prime }\Omega ^{-1}y_{t}\right)  \\
%G_{n,\lambda }& =\left( G_{0}^{-1}+\sum_{t=1}^{n}\lambda _{t}X_{t}^{\prime
%}\Omega ^{-1}X_{t}\right) ^{-1}
%\end{align*}%
%and
%\begin{equation*}
%\Omega ^{-1}|Y_{n},\gamma ,\nu ,\left\{ \lambda _{t}\right\} \sim \mathcal{W}%
%_{d}\left( \rho _{0}+n,\left( R_{0}^{-1}+\sum_{t=1}^{n}\lambda _{t}\left(
%y_{t}-X_{t}\gamma \right) \left( y_{t}-X_{t}\gamma \right) ^{\prime }\right)
%^{-1}\right)
%\end{equation*}%
%Moreover,%
%\begin{equation*}
%\Pr \left( \nu =\nu _{j}|Y_{n},\gamma ,\Omega ^{-1}\right) \propto
%q_{j}\prod\limits_{t=1}^{n}t_{d,\nu _{j}}\left( y_{t}|X_{t}\gamma ,\Omega
%\right)
%\end{equation*}%
%and%
%\begin{equation*}
%\lambda _{t}|Y_{n},\gamma ,\nu \sim G\left( \frac{\nu +d}{2},\frac{\nu
%+\left( y_{t}-X_{t}\gamma \right) ^{\prime }\left( y_{t}-X_{t}\gamma \right)
%}{2}\right)
%\end{equation*}%
%One sweep of the MCMC\ sampling is completed by sampling these four distributions in this order.

\section{Marginal Likelihood for Student-t Model}
Because the Gibbs sampler for the Student-t model has three blocks rather than two, we need to use a more complicated version of the Chib (1995) method to calculate the marginal likelihood for this model.

\paragraph{General Three-Block Algorithm} First we describe the method for an arbitrary three-block Gibbs sampler. 
By re-arranging Bayes' Rule we have the identity
\begin{equation*}
  f(y) = \frac{f(y|\theta_1^*, \theta^*_2, \theta_3^*)\pi\left( \theta^*_1, \theta^*_2, \theta^*_3 \right)}{\pi\left( \theta_1^*, \theta_2^*, \theta_3^*|y \right)}
\end{equation*}
for any specified values $(\theta_1^*, \theta_2^*, \theta_3^*)$ of the parameters.
In particular this holds at the \emph{posterior mean} which is where we will evaluate the expression.
Hence, the \emph{log} marginal likelihood is given by
\begin{equation*}
  \log{f(y)} =  \log \pi\left( \theta^*_1, \theta^*_2, \theta^*_3 \right) + \log{f(y|\theta^*_1, \theta^*_2, \theta^*_3}) - \pi\left( \theta^*_1, \theta^*_2, \theta^*_3|y \right)
\end{equation*}
The first two terms are easy: we simply evaluate the log of the prior and likelihood at the posterior mean for the three parameters.
The third one, however, is more complicated.
To calculate it we use the factorization:
\begin{equation*}
\pi\left( \theta^*_1, \theta^*_2, \theta^*_3|y \right) = \pi(\theta^*_1|y) \pi\left( \theta^*_2|\theta^*_1,y \right)\pi\left( \theta^*_3|\theta^*_2, \theta^*_1,y \right)
\end{equation*}
This leaves us with three a product of new terms that we need to calculate.
The last of the terms in the product, $\pi(\theta^*_3|\theta^*_2, \theta^*_1, y)$ is immediately available: this conditional density is known since we used it as a step in the Gibbs sampler.
All we need to do is substitute in the appropriate values for $\theta^*_2,\theta^*_1$ and evaluate the density at $\theta^*_3$.
To calculate the first term in the product we need to marginalize over $\theta_2, \theta_3$.
A Monte-Carlo approximation to the appropriate integral can be computed directly from the Gibbs sampler output:
\begin{equation*}
  \widehat{\pi}\left( \theta^*_1|y \right) = \frac{1}{G}\sum_{g=1}^{G} \pi\left( \theta^*_1|\theta_2^{(g)}, \theta_3^{(g)}, y \right)
\end{equation*}
To do this we rely on the fact that $\pi(\theta_1|\theta_2, \theta_3, y)$ is a known density -- we use it in the Gibbs sampler.
The middle term in the product is the most difficult one to calculate.
To begin, notice that
\begin{equation*}
  \pi\left( \theta^*_2|\theta^*_1, y \right) = \int \pi(\theta^*_2, \theta_3|\theta_1^*,y) \; d\theta_3 = \int \pi(\theta_2^*|\theta^*_1, \theta^*_3,y)\pi(\theta_3|\theta^*_1,y) \; d\theta_3
\end{equation*}
The idea is to construct a Monte-Carlo approximation of the integral on the right-hand-side of the preceding expression.
The approximation we use is
\begin{equation*}
  \hat{\pi}\left( \theta_2^*|\theta^*_1,y \right) = \frac{1}{G} \sum_{g = 1}^G \pi(\theta_2^*|\theta_1^*, \theta_3^{(g)},y)
\end{equation*}
but the draws $\left\{ \theta_3^{(g)} \right\}$ come \emph{not} from the original run of the Gibbs sampler but from a so-called ``reduced run'' in which we sample $\theta_2^{(g)}$ and $\theta_3^{(g)}$ from $\pi(\theta_2|\theta_1^*,\theta_3, y)$ and $\pi(\theta_3|\theta_1^*,\theta_2,y)$.
In other words, the reduced run holds $\theta_1$ \emph{fixed} at $\theta^*_1$, the posterior mean calculated from the draws of the \emph{usual} Gibbs sampler.
We can carry out the reduced run using the exact same algorithm as we use for the full Gibbs sampler: we just need to keep $\theta^*_1$ fixed and make sure that we store the draws $\theta_3^{(g)}$ that we will need to calculate $\hat{\pi}(\theta^*_2|\theta_1^*,y)$.


\paragraph{Specializing to the Student-t Model}
For the SUR model we have
\begin{equation*}
  \log f(Y_T) = \log \pi(\boldsymbol{\gamma}^*) + \log \pi(\Omega^{-1*}) + \log f(Y_T|\boldsymbol{\gamma}^*, \Omega^{-1*}) - \log \pi(\boldsymbol{\gamma}^*, \Omega^{-1*}|Y_T)
\end{equation*}
exactly as in the case with normal errors.
This is because we still use the \emph{same prior}, under which $\boldsymbol{\gamma}$ and $\Omega^{-1}$ are independent.
Moreover, note that $\lambda_t$ does not make a direct appearance.
This is because we use it only as a way of obtaining conditional conjugacy: it is not a parameter over which we place a prior.
The first two terms of this expression give the contribution of the prior to the marginal likelihood.
These are computed in exactly the same way as above in the normal case.
The third term gives the contribution of the likelihood.
In the Student-t model we have $\varepsilon_{t}|\mathbf{x}_t \sim \mbox{iid } t_{D,\nu}(0, \Omega)$ and hence, from the regression specification, it follows that $\mathbf{y}_t \sim \mbox{iid } t_{D,\nu}(X_t \boldsymbol{\gamma},\Omega)$.
Hence, evaluating the log-likelihood at the posterior mean gives
\begin{equation*}
  \log f(Y_T| \boldsymbol{\gamma}^*,\Omega^{-1*}) = \sum_{t=1}^{T} \log{ t_{D,\nu} \left(\mathbf{y}_t|X_t \boldsymbol{\gamma}^*, \Omega^{-1*}\right)}
\end{equation*}
where we parameterize in terms of $\Omega^{-1}$ instead of $\Omega$ since this is how our C++ function for the Student-t density is specified.
From here, the calculation is identical to the case for the normal model except with a Student-t density in place of a normal.\footnote{In particular, it's still convenient to work with the likelihood in terms of $\varepsilon_t$ rather than $\mathbf{y}_t$ and we have a way to simultaneously evaluate the likelihood contribution for each observation using our C++ routine for the Student-t density. See above for more details.}

It is the \emph{final} term in the log marginal likelihood expression, the contribution of the posterior, whose computation is substantially different for the case of the Student-t model.
We factorize the contribution of the prior according to:
\begin{equation*}
  \log \pi(\boldsymbol{\gamma}^*, \Omega^{-1*}|Y_T) =  \log \pi\left( \Omega^{-1*}|Y_T \right) + \log \pi\left( \boldsymbol{\gamma}^*|\Omega^{-1*},Y_T \right) 
\end{equation*}
To approximate the $\log \pi(\Omega^{-1*}|Y_T)$ term we evaluate the density $\pi(\Omega^{-1}|\boldsymbol{\gamma},\boldsymbol{\lambda},Y_T)$ at $\Omega^{-1*}$ and marginalize over the original Gibbs sampler draws $\left\{ \boldsymbol{\gamma}^{(g)}, \boldsymbol{\lambda}^{(g)} \right\}$, that is
\begin{eqnarray*}
  \widehat{\pi}\left( \Omega^{-1*}|Y_T \right) &=& \frac{1}{G}\sum_{g=1}^G \pi\left( \Omega^{-1*}|\boldsymbol{\gamma}^{(g)},\boldsymbol{\lambda}^{(g)},Y_T \right)\\
  &=& \frac{1}{G}\sum_{g=1}^G \mathcal{W}_D\left(\Omega^{-1*}\left|\rho_0 + T, R_{T,\lambda^{(g)}}^{\left( g \right)}\right. \right) 
\end{eqnarray*}
where
\begin{eqnarray*}
  R^{(g)}_{T,\lambda^{(g)}} &=&  \left[ R_0^{-1} + \sum_{t=1}^{T} \lambda_t \left( \mathbf{y}_t - X_t \boldsymbol{\gamma}^{(g)} \right)\left( \mathbf{y}_t - X_t \boldsymbol{\gamma}^{(g)} \right)' \right]^{-1}\\
  &=& \left[  R_0^{-1} + \left( \widetilde{Y} - \widetilde{X} \Gamma^{(g)} \right)' \Lambda^{(g)}\left( \widetilde{Y} - \widetilde{X} \Gamma^{(g)} \right)\right]^{-1}
\end{eqnarray*}

All that remains is to approximate $\pi\left( \boldsymbol{\gamma}^*|\Omega^{-1*}, Y_T \right)$.
To do this, we will need to use a reduced run similar to the one used in the calculations for the general three-block Gibbs-sampler described above.
We know $\pi\left( \boldsymbol{\gamma}|\Omega^{-1}, \boldsymbol{\lambda},Y_T \right)$ so we will evaluate this expression at $\boldsymbol{\gamma}^*$ and $\Omega^{-1*}$ and integrate out $\boldsymbol{\lambda}$ using a set of draws $\left\{ \boldsymbol{\lambda}^{(g)} \right\}$ that were generated holding $\Omega^{-1}$ \emph{fixed} at $\Omega^{-1*}$.
Specifically,

from this reduced run are used to give
\begin{equation*}
\pi (\gamma ^{\ast }|Y_{n},\Omega ^{-1\ast },\nu ^{\ast })=\frac{1}{G}%
\sum_{g=1}^{G}\mathcal{N}_{d+p}\left( \gamma ^{\ast }|\hat{\gamma}_{\lambda
^{(g)}}^{\ast },G_{n,\lambda ^{(g)}}^{\ast }\right)
\end{equation*}%
where%
\begin{align*}
\hat{\gamma}_{\lambda ^{(g)}}^{\ast }& =G_{n,\lambda ^{(g)}}^{\ast }\left(
G_{0}^{-1}\gamma _{0}+\sum_{t=1}^{n}\lambda _{t}^{(g)}X_{t}^{\prime }\Omega
^{-1\ast }y_{t}\right)  \\
G_{n,\lambda ^{(g)}}^{\ast }& =\left( G_{0}^{-1}+\sum_{t=1}^{n}\lambda
_{t}^{(g)}X_{t}^{\prime }\Omega ^{-1\ast }X_{t}\right) ^{-1}
\end{align*}
\section{OLD STUFF!}

\begin{equation*}
\log \Pr \left( \nu ^{\ast }\right) +\log \pi \left( \gamma ^{\ast }\right)
+\log \pi \left( \Omega ^{-1\ast }\right) +\sum_{t=1}^{n}\log t_{d,\nu
^{\ast }}\left( y_{t}|X_{t}\gamma ^{\ast },\Omega ^{\ast }\right) -\log \pi
\left( \nu ^{\ast },\gamma ^{\ast },\Omega ^{-1\ast }|Y_{n}\right)
\end{equation*}%
where $\nu ^{\ast }$ is the posterior mode (which is easily computed from
the sampled values), the last term is calculated as%
\begin{equation*}
\Pr \left( \nu ^{\ast }|Y_{n}\right) \times \pi \left( \Omega ^{-1\ast
}|Y_{n},\nu ^{\ast }\right) \times \pi \left( \gamma ^{\ast }|Y_{n},\Omega
^{-1\ast },\nu ^{\ast }\right)
\end{equation*}%
in which the first term is obtained from the posterior frequency
distribution of $\nu $, the second term is obtained from a reduced run in
which $\nu $ is fixed at $\nu ^{\ast }$ and the remaining three
distributions are sampled and the draws
\begin{equation*}
\left\{ \gamma ^{(g)},\lambda _{t}^{(g)}\right\} _{g=1}^{G}
\end{equation*}%
from this reduced MCMC run are used to calculate $\pi \left( \Omega ^{-1\ast
}|Y_{n},\nu ^{\ast }\right) $ as
\begin{equation*}
\frac{1}{G}\sum_{g=1}^{G}\mathcal{W}_{d}\left( \Omega ^{-1\ast }|\rho
_{0}+n,\left( R_{0}^{-1}+\sum_{t=1}^{n}\lambda _{t}^{(g)}\left(
y_{t}-X_{t}\gamma ^{(g)}\right) \left( y_{t}-X_{t}\gamma ^{(g)}\right)
^{\prime }\right) ^{-1}\right)
\end{equation*}%
and the final term $\pi \left( \gamma ^{\ast }|Y_{n},\Omega ^{-1\ast
}\right) $ is obtained from a second reduced run in which $\nu $ is fixed at
$\nu ^{\ast }$ and $\Omega ^{-1}$ is fixed at $\Omega ^{-1\ast }$ and the
draws
\begin{equation*}
\left\{ \lambda _{t}^{(g)}\right\}
\end{equation*}%
from this reduced run are used to give
\begin{equation*}
\pi (\gamma ^{\ast }|Y_{n},\Omega ^{-1\ast },\nu ^{\ast })=\frac{1}{G}%
\sum_{g=1}^{G}\mathcal{N}_{d+p}\left( \gamma ^{\ast }|\hat{\gamma}_{\lambda
^{(g)}}^{\ast },G_{n,\lambda ^{(g)}}^{\ast }\right)
\end{equation*}%
where%
\begin{align*}
\hat{\gamma}_{\lambda ^{(g)}}^{\ast }& =G_{n,\lambda ^{(g)}}^{\ast }\left(
G_{0}^{-1}\gamma _{0}+\sum_{t=1}^{n}\lambda _{t}^{(g)}X_{t}^{\prime }\Omega
^{-1\ast }y_{t}\right)  \\
G_{n,\lambda ^{(g)}}^{\ast }& =\left( G_{0}^{-1}+\sum_{t=1}^{n}\lambda
_{t}^{(g)}X_{t}^{\prime }\Omega ^{-1\ast }X_{t}\right) ^{-1}
\end{align*}

\end{document}
