%!TEX root = ../main.tex
\section{Model and Framework}
Consider a linear $K$-factor model for $D$ assets of the form
\begin{equation*}
y_{it}=\alpha _{d} + \mathbf{f}_{t}' \boldsymbol{\beta}_d + \varepsilon_{it}
\end{equation*}
where $d = 1, \hdots, D$ and $t = 1, \hdots, T$ and $\mathbf{f}_{t}'=\left(f_{t1}, \hdots,f_{tK}\right)$ is a $K\times1$ vector.

We consider a cross-section of $D$ returns.
$y_{it}$ denotes a return for an asset $i$ at period $t$. 
The cross-section of returns on right hand side is the same for all models. 
The returns are assumed to follow a common factor structure with $K$ factors denoted as $\mathbf{f}_{t}$. 
Note that the factors included into the regression $\mathbf{f}$, the number of factors $K$ as well as the presence of the intercept depend on the model under consideration.  
In order to simplify the notation, we do not introduce the model index, however, it is important to keep in mind that the techniques discussed below are applied to each possible combination of factors.
This is a special case of the seemingly unrelated regression (SUR) model in which the regressors are \emph{identical} across equations.

Stacking observations for a given time period across assets, define $\mathbf{y}_t' = (y_{1t}, \hdots, y_{Dt})$ and analogously $\boldsymbol{\varepsilon}_t' = \left( \varepsilon_{t1}, \hdots \varepsilon_{tD} \right)$. 
Both $\mathbf{y}_t$ and $\boldsymbol{\varepsilon}_t$ are $D \times 1$ vectors.
Now let $\mathbf{x}_t' = (1, \mathbf{f}_t')$ be a $1 \times (K+1)$ vector and $\boldsymbol{\gamma}_d' = (\alpha_d, \boldsymbol{\beta}_d')$ to be a $1 \times (K+1)$ so we have
\begin{equation*}
\mathbf{y}_t = X_t \boldsymbol{\gamma} + \boldsymbol{\varepsilon}_t
\end{equation*}
where $X_t = I_D \otimes \mathbf{x}_t'$, a $D \times D(K+1)$ matrix, and $\boldsymbol{\gamma}'= \left( \boldsymbol{\gamma}_1', \hdots, \boldsymbol{\gamma}_D' \right)$, a $1 \times (K+1)D$ vector. 
Let $Y_T$ denote the full data sample, i.e.\ $\left\{ \mathbf{y}_t, \mathbf{x}_t \right\}_{t=1}^T$.

In order to apply Bayesian inference, we should make an assumption about the joint distribution of the errors $\varepsilon_t$.  
Different specifications of the errors distribution can lead to the selection of different models. 
The normality assumption, which is often exploited due to the conjugacy properties, should be treated with caution because the tail behavior of the financial returns is different from the one imposed by Gaussian distribution.
For this reason we fit a multivariate Student-t distribution that accounts for fat tails specific to the returns. 
As we want to remain agnostic about the shape of the distribution, we explore multiple distributions from the same family with different degrees of freedom and let the data determined the best one.

Suppose now that the errors follow a multivariate Student-t distribution: 
\begin{equation*}
\boldsymbol{\varepsilon}_{t}\sim t_{D,\nu }\left( 0,\Omega \right)
\end{equation*}
where $\nu$ denotes the degrees of freedom of the distribution, the location parameter is zero and the scale matrix is $\Omega$.
If $\nu >1$ then $E(\boldsymbol{\varepsilon}) = 0$. 
If $\nu>2$ then $Var\left( \boldsymbol{\varepsilon} \right) = \nu \Omega/(\nu-2)$. \\
The SUR model lets us explicitly explore the cross-sectional dependence of errors by jointly fitting the model for all assets. 
The possible variance linkages between different assets are captured by the scale matrix $\Omega$. 
While acknowledging the importance of these linkages and the possible influence they may have on the model selection, we have little understanding about their nature and thus should avoid imposing rigid assumptions. 
Careful treatment of the covariance matrix $\Omega$ and its inverse $\Omega^{-1}$ allows us to infer the covariance structure of errors from the data.

\subsection{A Hierarchical Representation}
Replacing the normal likelihood with the Student-t likelihood, however, breaks the conditional conjugacy that is usually exploited to construct an MCMC algorithm based on the Gibbs sampler.
The solution to this problem is to work with a hierarchical representation in which the Student-t likelihood is introduced as a scale mixture of normal distributions (\cite{chib1995hierarchical}), in particular
\begin{align*}
\boldsymbol{\varepsilon}_{t}|\lambda _{t} &\sim N\left( 0,\lambda _{t}^{-1}\Omega \right)
\\
\lambda _{t} &\sim G\left( \frac{\nu }{2},\frac{\nu }{2}\right)
\end{align*}
where $G(\alpha,\beta)$ denotes the Gamma distribution with shape parameter $\alpha$ and rate parameter $\beta$. 
We parameterize this problem in terms of the $D\times D$ \emph{precision} matrix $\Omega^{-1}$ and the $p\times1$ vector of regression coefficients $\boldsymbol{\gamma}$, where $p = D(K+1)$. 
Using this representation, after conditioning on $\left( \nu, \boldsymbol{\lambda} \right)$, where $\boldsymbol{\lambda} = (\lambda_1, \dots \lambda_T)'$, we are essentially back in the familiar normal case.
In any inference that we carry out, as well as in the calculation of the marginal likelihood, we will marginalize over $\boldsymbol{\lambda}$ by simply ignoring these draws.
We will place a normal prior on $\boldsymbol{\gamma}$ and a Wishart prior on $\Omega^{-1}$. 

\subsection{The Gibbs Sampler}
The sampler proceeds by fixing the degrees of freedom parameter $\nu$.
If $\nu$ is to be chosen from the data, this can be accomplished using the marginal likelihood, as described below.
Holding $\nu$ fixed, the full set of conditional posteriors is as follows:

\paragraph{Regression Coefficients:} 
$\boldsymbol{\gamma}|\Omega^{-1},Y_T \sim \mathcal{N}_p\left( \bar{\boldsymbol{\gamma}}_{\lambda},G_{T,\lambda} \right)$
\begin{eqnarray*}
	G_{T,\lambda} &=& \left[ G_0^{-1} + \sum_{t=1}^T \lambda_t X_t' \Omega^{-1} X_t \right]^{-1}\\
	\bar{\boldsymbol{\gamma}}_{\lambda} &=& G_{T,\lambda} \left[ G_0^{-1}\boldsymbol{\gamma}_0 + \sum_{t=1}^{T}\lambda_t X_t'\Omega^{-1}\mathbf{y}_t \right]
\end{eqnarray*}
\paragraph{Inverse Scale Matrix:}
$\Omega^{-1}|Y_T \sim \mathcal{W}_D\left(\rho_0 + T, R_{T,\lambda}\right)$
\begin{equation*}
R_{T,\lambda} = \left[ R_0^{-1} + \sum_{t=1}^{T} \lambda_t\left( \mathbf{y}_t - X_t \boldsymbol{\gamma} \right)\left( \mathbf{y}_t - X_t \boldsymbol{\gamma} \right)' \right]^{-1}
\end{equation*}

\paragraph{Auxiliary Parameter:} $\lambda_t|\boldsymbol{\gamma}, \nu, Y_T \sim G\left(\displaystyle \frac{\nu + D}{2}, \frac{\nu + \boldsymbol{\varepsilon}_t' \boldsymbol{\varepsilon}_t}{2} \right)$, $\boldsymbol{\varepsilon}_t = \mathbf{y}_t - X_t \boldsymbol{\gamma}$

To implement the Gibbs sampler, we simply need to draw sequentially from these distributions, in the order given above. 
We will require, however, starting values for both $\Omega^{-1}$ and each of the $\lambda_t$ parameters. 
A reasonable starting value for $\lambda_t$ is one, which makes the initial draws for the regression coefficients and the inverse scale matrix the same as if were work withing with the normal model.

\subsection{Marginal Likelihood for Student-t Model}
 We calculate the marginal likelihood using the method of \cite{chib1995marginal}.
 \begin{equation*}
 f(y) = \frac{(y|\boldsymbol{\gamma}^*,\Omega^{-1*})\pi\left(\boldsymbol{\gamma}^*, \Omega^{-1*}\right)}{\pi\left(\boldsymbol{\gamma}^*, \Omega^{-1*}|y \right)}
 \end{equation*} 
Re-arranging Bayes' Rule we have the identity
\begin{equation*}
f(y) = \frac{f(y|\boldsymbol{\gamma}^*,\Omega^{-1*})\pi\left(\boldsymbol{\gamma}^*, \Omega^{-1*}\right)}{\pi\left(\boldsymbol{\gamma}^*, \Omega^{-1*}|y \right)}
\end{equation*}
for any specified values $(\boldsymbol{\gamma}^*,\Omega^{-1*})$ of the parameters.
In particular this holds at the \emph{posterior mean} which is where we will evaluate the expression.
Hence, the \emph{log} marginal likelihood is given by
\begin{equation*}
\log{f(y)} =  \log \left(\boldsymbol{\gamma}^*, \Omega^{-1*}\right) + \log{f(y|\boldsymbol{\gamma}^*,\Omega^{-1*})} - \pi\left(\boldsymbol{\gamma}^*, \Omega^{-1*}|y \right)
\end{equation*}
Specializing this to the SUR model considered above,
\begin{equation*}
\log f(Y_T) = \log \pi(\boldsymbol{\gamma}^*) + \log \pi\left( \Omega^{-1*} \right) + \log f\left( Y_T|\boldsymbol{\gamma}^*, \Omega^{-1*} \right) - \log \pi\left( \boldsymbol{\gamma}^*, \Omega^{-1*}|Y_T \right)
\end{equation*}
since our priors over $\boldsymbol{\gamma}$ and $\Omega^{-1}$ are independent. We can evaluate the marginal likelihood using the \cite{chib1995marginal} method that approximates $\log f(Y_T)$ by evaluating each of the terms on the right-hand-side of the preceding expression using the output of the Gibbs sampler.

\subsection{Priors}
One of the most challenging parts of the estimation procedure is to set up reasonable priors. 
In order for the marginal likelihood comparison to be valid, the marginal likelihood for each model should be compute with proper prior. 
Using generic prior may lead to undesirable consequences. 
In other words, it's important to set the priors carefully and make them specific to each model under consideration. 
In order to set individual level priors, we suggest to use a training sample to form qualified prior views for every model at hand.
The priors based on he training sample are model-specific and thus can be used for model comparison. 
We split  the sample into two parts: training sub-sample and fit sub-sample. 
The training sample is used to carefully construct priors later used to fit the model on the rest of the data. 
Training sample is not a part of the final estimation. 

Both for Student-t and normal errors we specify two prior distributions: for the coefficients $\gamma \sim \mathcal{N}_{p}\left( \boldsymbol{\gamma} |\boldsymbol{\gamma_{0}},G_{0}\right)$ and for the precision matrix $\Omega^{-1} \sim \mathcal{W}_{D}  \left(\Omega ^{-1}|\rho_{0}, R_{0}\right) $. 
Parameters for the two distributions are specified differently for each stage.


\subsubsection{Stage 1: Training Sample}
When applying the method to the training sample, we have very little information about the possible parameters values. 
We use diffused priors to reflect the uncertainty.
For each model we start with the assumption that factor loadings can be both positive and negative. 
We have very little information about how these loadings interact, so our starting point would be to use diagonal covariance matrix of factor loadings.
Parameters of the prior for the regression coefficients are set as follows:
\begin{eqnarray*}
	\gamma_{0} &=& 0 \\ 
	G_{0} &=& C_{1}^{2}I_{p} 
\end{eqnarray*}
The prior distribution of the coefficient vector is centered around zero. 
The covariance matrix is assumed to be diagonal.
The constant $C_{1}$ controls the tightness: the larger $C_{1}$, the wider is the prior.
As we have very little information about the covariance structure of the errors, a prior centered around the diagonal matrix seems to be a reasonable starting point. 
Covariance matrix is assumed to follow the inverse Wishart distribution with the following parameters:
\begin{eqnarray*}
	\rho_{0} &=& d + C_{2} \\ 
	R_{0} &=& \frac{1}{C_{3}^{2} (\rho_{0}-d-1)}I_{d}
\end{eqnarray*}
Using the properties of the inverse Wishart distribution, one can easily see that the mean value of $\Omega$ implied by the prior is a diagonal matrix $R_{0} \times(\rho_{0} - d- 1) = C_{3}^{2}I_{d}$. 
One way of widening the prior would be to decrease number of degrees of freedom $\rho_{0}$ by adjusting the value of $C_2$. 
Constant $C_{3}$ is used to set the magnitude of the elements of the covariance matrix. 
For the rest of the paper we set $C_{1} = 2$, $C_{2} = 6$ and $C_{3} = 0.05$ (we are working with return in decimals).
These priors are used to estimate the model using only the training sample data. 
The posterior draws obtained as a result of the procedure, are later used to form more informed view about the model parameters.

\subsubsection{Stage 2}
The first stage draws contain some information about the parameters and serve as a basis to construct proper priors for the estimation.
Denote first stage posterior means of $\gamma$ as $\overline{\gamma}$ and the sample covariance matrix of these draws as $\widehat{G}$. 
We also calculate a posterior mean of the precision matrix $\Omega^{-1}$: $\overline{\Omega}^{-1}$. 
The regression coefficients prior is:
\begin{eqnarray*}
	\gamma_{0} &=& \overline{\gamma} \\ 
	G_{0} &=& C_{4}^{2} \widehat{G}
\end{eqnarray*}\\
The prior of factor loadings is thus centered around the posterior mean of the first step draws. 
The standard deviation is based on the sample standard deviation of the first stage draws adjusted by the factor of $C_{4}$ to reflect uncertainty.
Note that our algorithm provides us draws of the precision matrix $\Omega^{-1}$, not of the covariance matrix $\Omega$. 
For this reason we form the prior in terms of the posterior mean of ${\Omega}^{-1}$. 
We use the duality of Wishart and inverse Wishart distribution: if $\Omega \sim \mathcal{W}_D\left(\rho_0, R_0 \right)$, $\Omega^{-1} \sim \mathcal{IW}_D\left(\rho_0, R_0 \right)$. 
The prior of the precision matrix is set:
\begin{eqnarray*}
	\rho_{0} &=& d + C_{5} \\ 
	R_{0} &=& \frac{1}{\rho} \overline{\Omega}^{-1}
\end{eqnarray*}
The prior is constructed to set the mean of the precision matrix $\Omega^{-1}$ equal to the posterior mean based on the training sample: $\rho_{0} \times R_{0} =\overline{\Omega}^{-1} $. 
Again, the tightness of the prior is controlled by degrees of freedom parameter $\rho_{0}$. 
For all the applications we use $C_{4} = 3$ and $C_5 = 6$.

\subsection{Prior Updates}
\todo[inline]{Fill in something here?}

